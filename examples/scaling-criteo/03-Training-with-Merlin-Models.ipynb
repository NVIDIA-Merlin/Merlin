{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b71acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f95be1",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Training with Merlin DLRM\n",
    "\n",
    "This notebook is created using Tensorflow 20.05 version. In this notebook we converted the Tensorflow implementation of [Scaling Criteo](https://github.com/NVIDIA-Merlin/Merlin/blob/main/examples/scaling-criteo/03-Training-with-TF.ipynb) to Merlin Models. We have used DLRM model for this experiment.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "We observed that TensorFlow training pipelines can be slow as the dataloader is a bottleneck. The native dataloader in TensorFlow randomly sample each item from the dataset, which is very slow. The window dataloader in TensorFlow is also not much faster. In our experiments, we are able to speed-up existing TensorFlow pipelines by 9x using a highly optimized dataloader.\n",
    "\n",
    "\n",
    "We have already discussed the NVTabular dataloader for TensorFlow in more detail in our [Getting Started with Movielens notebooks](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples/getting-started-movielens).\n",
    "\n",
    "\n",
    "We will use the same techniques to train a deep learning model for the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "In this notebook, we learn how to:\n",
    "\n",
    "- Use NVTabular dataloader with Merlin DLRM model using Criteo dataset\n",
    "    \n",
    "## NVTabular dataloader for TensorFlow\n",
    "We’ve identified that the dataloader is one bottleneck in deep learning recommender systems when training pipelines with TensorFlow. The dataloader cannot prepare the next batch fast enough and therefore, the GPU is not fully utilized.\n",
    "\n",
    "We developed a highly customized tabular dataloader for accelerating existing pipelines in TensorFlow. In our experiments, we see a speed-up by 9x of the same training workflow with NVTabular dataloader. NVTabular dataloader’s features are:\n",
    "\n",
    "- removing bottleneck of item-by-item dataloading\n",
    "- enabling larger than memory dataset by streaming from disk\n",
    "- reading data directly into GPU memory and remove CPU-GPU communication\n",
    "- preparing batch asynchronously in GPU to avoid CPU-GPU communication\n",
    "- supporting commonly used .parquet format\n",
    "- easy integration into existing TensorFlow pipelines by using similar API - works with tf.keras models\n",
    "\n",
    "\n",
    "More information in our [blogpost](https://medium.com/nvidia-merlin/training-deep-learning-based-recommender-systems-9x-faster-with-tensorflow-cc5a2572ea49)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68941f39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b347509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "from merlin.schema import Tags\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4923fa",
   "metadata": {},
   "source": [
    "Define the path to directories which contains the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f448ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/workspace/criteo/test_dask/output/train/part_0.parquet'],\n",
       " ['/workspace/criteo/test_dask/output/valid/part_0.parquet'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/workspace/criteo\")\n",
    "input_path = os.environ.get(\"INPUT_DATA_DIR\", os.path.join(BASE_DIR, \"test_dask/output\"))\n",
    "\n",
    "# path to processed data\n",
    "PATH_TO_TRAIN_DATA = sorted(glob.glob(os.path.join(input_path, \"train\", \"*.parquet\")))\n",
    "PATH_TO_VALID_DATA = sorted(glob.glob(os.path.join(input_path, \"valid\", \"*.parquet\")))\n",
    "\n",
    "PATH_TO_TRAIN_DATA, PATH_TO_VALID_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa21e3",
   "metadata": {},
   "source": [
    "## Define hyperparameters\n",
    "\n",
    "First, we define the data schema and differentiate between single-hot and multi-hot categorical features. Note, that we do not have any numerical input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c35685",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n",
    "CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n",
    "LABEL_COLUMNS = [\"label\"]\n",
    "\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 64 * 1024))\n",
    "EMBEDDING_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LR = 0.01\n",
    "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97375902",
   "metadata": {},
   "source": [
    "## Create the dataset\n",
    "\n",
    "In this experiment, we will use [Dataset](https://github.com/NVIDIA-Merlin/core/blob/main/merlin/io/dataset.py) which is a external-data wrapper for NVTabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aab6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(PATH_TO_TRAIN_DATA, part_mem_fraction=0.04)\n",
    "valid = Dataset(PATH_TO_VALID_DATA, part_mem_fraction=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf40eb9",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Merlin models internally adds a dense layer with correct output based on the prediction tasks. In this case, since we are trying to implement Binary Classification task the dense layer will have a output dimension of 1. \n",
    "\n",
    "To know more about Merlin models go [here](https://github.com/NVIDIA-Merlin/models/tree/3b8e90368ba610011daacf87e78fbca73dae03c8/merlin/models/tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    train.schema,                                                            # 1\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    bottom_block=mm.MLPBlock([128, EMBEDDING_SIZE]),                         # 2\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(                            # 3\n",
    "        train.schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "    )               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ada271",
   "metadata": {},
   "source": [
    "Utilize the NVTabular data loader to create the validation dataloader which will be sent as a callback during training. The NVTabular data loader are initialized as usually and we specify both single-hot and multi-hot categorical features as cat_names. The data loader will automatically recognize the single/multi-hot columns and represent them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac36e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "\n",
    "valid_dataloader = KerasSequenceLoader(\n",
    "    valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    engine=\"parquet\",\n",
    "    shuffle=False,\n",
    "    parts_per_chunk=1,\n",
    ")\n",
    "validation_callback = KerasSequenceValidater(valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a56da",
   "metadata": {},
   "source": [
    "## Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ea88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 16:09:22.676530: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-08 16:09:24.425708: I tensorflow/stream_executor/cuda/cuda_blas.cc:1804] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11799/11800 [============================>.] - ETA: 0s - label/binary_classification_task/precision: 0.0316 - label/binary_classification_task/recall: 1.0189e-04 - label/binary_classification_task/binary_accuracy: 0.9679 - label/binary_classification_task/auc: 0.6346 - loss: 0.1412 - regularization_loss: 0.0000e+00 - total_loss: 0.1412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 16:17:30.566120: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/branch_executed/_19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_label/binary_classification_task/precision': 0.097826086, 'val_label/binary_classification_task/recall': 1.7168699e-06, 'val_label/binary_classification_task/binary_accuracy': 0.9655388, 'val_label/binary_classification_task/auc': 0.66828704}\n",
      "11800/11800 [==============================] - 671s 56ms/step - label/binary_classification_task/precision: 0.0316 - label/binary_classification_task/recall: 1.0189e-04 - label/binary_classification_task/binary_accuracy: 0.9679 - label/binary_classification_task/auc: 0.6346 - loss: 0.1412 - regularization_loss: 0.0000e+00 - total_loss: 0.1412 - val_label/binary_classification_task/precision: 0.0978 - val_label/binary_classification_task/recall: 1.7169e-06 - val_label/binary_classification_task/binary_accuracy: 0.9655 - val_label/binary_classification_task/auc: 0.6683 - val_loss: 0.2844 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.2844\n",
      "CPU times: user 34min 59s, sys: 7min 11s, total: 42min 11s\n",
      "Wall time: 11min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb58ccd7b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=BATCH_SIZE, callbacks=[validation_callback], epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3cf26",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "In our experiment, we used the following configurations:\n",
    "```\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 64 * 1024))\n",
    "EMBEDDING_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 0.08\n",
    "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    train.schema,                                                            # 1\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    bottom_block=mm.MLPBlock([256, EMBEDDING_SIZE]),                         # 2\n",
    "    top_block=mm.MLPBlock([256, 128, 64]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(                            # 3\n",
    "        train.schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "    )               \n",
    ")\n",
    "```\n",
    "\n",
    "and we achieved a AUC score of **0.7247** while using only 5 parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db25ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322/2322 [==============================] - 58s 24ms/step - label/binary_classification_task/precision: 0.0978 - label/binary_classification_task/recall: 1.7169e-06 - label/binary_classification_task/binary_accuracy: 0.9655 - label/binary_classification_task/auc: 0.6683 - loss: 0.1440 - regularization_loss: 0.0000e+00 - total_loss: 0.1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label/binary_classification_task/precision': 0.09782608598470688,\n",
       " 'label/binary_classification_task/recall': 1.7168698605019017e-06,\n",
       " 'label/binary_classification_task/binary_accuracy': 0.9655378460884094,\n",
       " 'label/binary_classification_task/auc': 0.6682871580123901,\n",
       " 'loss': 0.28435835242271423,\n",
       " 'regularization_loss': 0.0,\n",
       " 'total_loss': 0.28435835242271423}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(valid, batch_size=BATCH_SIZE, return_dict=True)\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3d245",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(input_path, \"model.savedmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6124331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
