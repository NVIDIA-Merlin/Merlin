{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_scaling-criteo-04-triton-inference-with-hugectr/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Scaling Criteo: Triton Inference with HugeCTR\n",
    "\n",
    "This notebook is created using the latest stable [merlin-hugectr](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "The last step is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the deep learning model for a prediction. Therefore, we deploy the NVTabular workflow with the HugeCTR model as an ensemble model to Triton Inference. The ensemble model guarantees that the same transformation are applied to the raw inputs.\n",
    "\n",
    "<img src='./imgs/triton-hugectr.png' width=\"25%\">\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to deploy our models to production:\n",
    "\n",
    "- Use **NVTabular** to generate config and model files for Triton Inference Server\n",
    "- Deploy an ensemble of NVTabular workflow and HugeCTR model\n",
    "- Send example request to Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Ensemble to Triton Inference Server\n",
    "\n",
    "First, we need to generate the Triton Inference Server configurations and save the models in the correct format. In the previous notebooks [02-ETL-with-NVTabular](./02-ETL-with-NVTabular.ipynb) and [03-Training-with-HugeCTR](./03-Training-with-HugeCTR.ipynb) we saved the NVTabular workflow and HugeCTR model to disk. We will load them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training terminates, we can see that two `.model` files are generated. We need to move them inside a temporary folder, like `criteo_hugectr/1`. Let's create these folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "from nvtabular.inference.triton import export_hugectr_ensemble\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", BASE_DIR + \"/test_dask/output\")\n",
    "original_data_path = os.environ.get(\"INPUT_FOLDER\", BASE_DIR + \"/converted/criteo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move our saved `.model` files inside 1 folder. We use only the last snapshot after `9600` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mv *9600.model \" + os.path.join(OUTPUT_DATA_DIR, \"criteo_hugectr/1/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the NVTabular workflow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow.load(os.path.join(OUTPUT_DATA_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clear the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"rm -rf \" + os.path.join(OUTPUT_DATA_DIR, \"model_inference\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export artifacts\n",
    "\n",
    "Now, we can save our models for use later during the inference stage. To do so we use export_hugectr_ensemble method below. With this method, we can generate the `config.pbtxt` files automatically for each model.<br><br>\n",
    "The script below creates an ensemble triton server model where\n",
    "- workflow is the the nvtabular workflow used in preprocessing,\n",
    "- hugectr_model_path is the HugeCTR model that should be served. This path includes the model files.\n",
    "- name is the base name of the various triton models.\n",
    "- output_path is the path where is model will be saved to.\n",
    "- cats are the categorical column names\n",
    "- conts are the continuous column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugectr_params = dict()\n",
    "# Config File in the final directory for serving\n",
    "hugectr_params[\"config\"] = os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"criteo/1/criteo.json\")\n",
    "hugectr_params[\"slots\"] = 26\n",
    "hugectr_params[\"max_nnz\"] = 1\n",
    "hugectr_params[\"embedding_vector_size\"] = 128\n",
    "hugectr_params[\"n_outputs\"] = 1\n",
    "export_hugectr_ensemble(\n",
    "    workflow=workflow,\n",
    "    # Current directory with model weights and config file\n",
    "    hugectr_model_path=os.path.join(OUTPUT_DATA_DIR, \"criteo_hugectr/1/\"),\n",
    "    hugectr_params=hugectr_params,\n",
    "    name=\"criteo\",\n",
    "    # Base directory for serving\n",
    "    output_path=os.path.join(OUTPUT_DATA_DIR, \"model_inference\"),\n",
    "    label_columns=[\"label\"],\n",
    "    cats=[\"C\" + str(x) for x in range(1, 27)],\n",
    "    conts=[\"I\" + str(x) for x in range(1, 14)],\n",
    "    max_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/test_merlin_criteo_hugectr/output/criteo//model_inference\u001b[00m\r\n",
      "├── \u001b[01;34mcriteo\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34m0_sparse_9600.model\u001b[00m\r\n",
      "│   │   │   ├── emb_vector\r\n",
      "│   │   │   ├── key\r\n",
      "│   │   │   └── slot_id\r\n",
      "│   │   ├── _dense_9600.model\r\n",
      "│   │   ├── _opt_dense_9600.model\r\n",
      "│   │   └── criteo.json\r\n",
      "│   └── config.pbtxt\r\n",
      "├── \u001b[01;34mcriteo_ens\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   └── config.pbtxt\r\n",
      "├── \u001b[01;34mcriteo_nvt\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│   │   │   └── model.cpython-38.pyc\r\n",
      "│   │   ├── model.py\r\n",
      "│   │   └── \u001b[01;34mworkflow\u001b[00m\r\n",
      "│   │       ├── \u001b[01;34mcategories\u001b[00m\r\n",
      "│   │       │   ├── unique.C1.parquet\r\n",
      "│   │       │   ├── unique.C10.parquet\r\n",
      "│   │       │   ├── unique.C11.parquet\r\n",
      "│   │       │   ├── unique.C12.parquet\r\n",
      "│   │       │   ├── unique.C13.parquet\r\n",
      "│   │       │   ├── unique.C14.parquet\r\n",
      "│   │       │   ├── unique.C15.parquet\r\n",
      "│   │       │   ├── unique.C16.parquet\r\n",
      "│   │       │   ├── unique.C17.parquet\r\n",
      "│   │       │   ├── unique.C18.parquet\r\n",
      "│   │       │   ├── unique.C19.parquet\r\n",
      "│   │       │   ├── unique.C2.parquet\r\n",
      "│   │       │   ├── unique.C20.parquet\r\n",
      "│   │       │   ├── unique.C21.parquet\r\n",
      "│   │       │   ├── unique.C22.parquet\r\n",
      "│   │       │   ├── unique.C23.parquet\r\n",
      "│   │       │   ├── unique.C24.parquet\r\n",
      "│   │       │   ├── unique.C25.parquet\r\n",
      "│   │       │   ├── unique.C26.parquet\r\n",
      "│   │       │   ├── unique.C3.parquet\r\n",
      "│   │       │   ├── unique.C4.parquet\r\n",
      "│   │       │   ├── unique.C5.parquet\r\n",
      "│   │       │   ├── unique.C6.parquet\r\n",
      "│   │       │   ├── unique.C7.parquet\r\n",
      "│   │       │   ├── unique.C8.parquet\r\n",
      "│   │       │   └── unique.C9.parquet\r\n",
      "│   │       ├── metadata.json\r\n",
      "│   │       └── workflow.pkl\r\n",
      "│   └── config.pbtxt\r\n",
      "└── ps.json\r\n",
      "\r\n",
      "10 directories, 40 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree $OUTPUT_DATA_DIR/model_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write a configuration file with the stored model weights and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = json.dumps(\n",
    "{\n",
    "    \"supportlonglong\": \"true\",\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": \"criteo\",\n",
    "            \"sparse_files\": [os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"criteo/1/0_sparse_9600.model\")],\n",
    "            \"dense_file\": os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"criteo/1/_dense_9600.model\"),\n",
    "            \"network_file\": os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"criteo/1/criteo.json\"),\n",
    "            \"max_batch_size\": \"64\",\n",
    "            \"gpucache\": \"true\",\n",
    "            \"hit_rate_threshold\": \"0.9\",\n",
    "            \"gpucacheper\": \"0.5\",\n",
    "            \"num_of_worker_buffer_in_pool\": \"4\",\n",
    "            \"num_of_refresher_buffer_in_pool\": \"1\",\n",
    "            \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "            \"deployed_device_list\": [\"0\"],\n",
    "            \"default_value_for_each_table\": [\"0.0\", \"0.0\"],\n",
    "            \"maxnum_catfeature_query_per_table_per_sample\": [2, 26],\n",
    "            \"embedding_vecsize_per_table\": [16 for x in range(26)],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    ")\n",
    "\n",
    "config = json.loads(config)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"ps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Triton Inference Server\n",
    "\n",
    "After we export the ensemble, we are ready to start the Triton Inference Server. The server is installed in the merlin-tensorflow-container. If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server [documentation](https://github.com/triton-inference-server/server/blob/r22.03/README.md#documentation). \n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "```shell\n",
    "tritonserver --model-repository=<output_path> --backend-config=hugectr,ps=<ps.json file>\n",
    "```\n",
    "\n",
    "For the `--model-repository` argument, specify the same value as `os.path.join(OUTPUT_DATA_DIR, \"model_inference\"` that you specified previously in `export_hugectr_ensemble` for `output_path`.\n",
    "For `ps=` argument, specify the same value as `os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"ps.json)` the file for ps.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/test_merlin_criteo_hugectr/output/criteo/model_inference\n",
      "/tmp/test_merlin_criteo_hugectr/output/criteo/model_inference/ps.json\n"
     ]
    }
   ],
   "source": [
    "print(os.path.join(OUTPUT_DATA_DIR, \"model_inference\"))\n",
    "print(os.path.join(OUTPUT_DATA_DIR, \"model_inference\", \"ps.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prediction from Triton Inference Server\n",
    "\n",
    "We have saved the models for Triton Inference Server. We started Triton Inference Server and the models are loaded.  Now, we can send raw data as a request and receive the predictions.\n",
    "\n",
    "We read 3 example rows from the last parquet file from the raw data. We drop the target column, `label`, from the dataframe, as the information is not available at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>...</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>2714039</td>\n",
       "      <td>29401</td>\n",
       "      <td>11464</td>\n",
       "      <td>1122</td>\n",
       "      <td>9355</td>\n",
       "      <td>2</td>\n",
       "      <td>6370</td>\n",
       "      <td>1010</td>\n",
       "      <td>37</td>\n",
       "      <td>1865651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208215</td>\n",
       "      <td>0.952671</td>\n",
       "      <td>0.955872</td>\n",
       "      <td>0.944922</td>\n",
       "      <td>0.139380</td>\n",
       "      <td>0.994092</td>\n",
       "      <td>0.056103</td>\n",
       "      <td>0.547473</td>\n",
       "      <td>0.709442</td>\n",
       "      <td>0.930728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70001</th>\n",
       "      <td>3514299</td>\n",
       "      <td>27259</td>\n",
       "      <td>8072</td>\n",
       "      <td>395</td>\n",
       "      <td>9361</td>\n",
       "      <td>1</td>\n",
       "      <td>544</td>\n",
       "      <td>862</td>\n",
       "      <td>11</td>\n",
       "      <td>3292987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171709</td>\n",
       "      <td>0.759526</td>\n",
       "      <td>0.795019</td>\n",
       "      <td>0.716366</td>\n",
       "      <td>0.134964</td>\n",
       "      <td>0.516737</td>\n",
       "      <td>0.065577</td>\n",
       "      <td>0.129782</td>\n",
       "      <td>0.471361</td>\n",
       "      <td>0.386101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70002</th>\n",
       "      <td>1304577</td>\n",
       "      <td>5287</td>\n",
       "      <td>7367</td>\n",
       "      <td>2033</td>\n",
       "      <td>2899</td>\n",
       "      <td>2</td>\n",
       "      <td>712</td>\n",
       "      <td>640</td>\n",
       "      <td>36</td>\n",
       "      <td>6415968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880028</td>\n",
       "      <td>0.347701</td>\n",
       "      <td>0.207892</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.371013</td>\n",
       "      <td>0.759502</td>\n",
       "      <td>0.201477</td>\n",
       "      <td>0.192447</td>\n",
       "      <td>0.085893</td>\n",
       "      <td>0.957961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            C1     C2     C3    C4    C5  C6    C7    C8  C9      C10  ...  \\\n",
       "70000  2714039  29401  11464  1122  9355   2  6370  1010  37  1865651  ...   \n",
       "70001  3514299  27259   8072   395  9361   1   544   862  11  3292987  ...   \n",
       "70002  1304577   5287   7367  2033  2899   2   712   640  36  6415968  ...   \n",
       "\n",
       "             I4        I5        I6        I7        I8        I9       I10  \\\n",
       "70000  0.208215  0.952671  0.955872  0.944922  0.139380  0.994092  0.056103   \n",
       "70001  0.171709  0.759526  0.795019  0.716366  0.134964  0.516737  0.065577   \n",
       "70002  0.880028  0.347701  0.207892  0.753950  0.371013  0.759502  0.201477   \n",
       "\n",
       "            I11       I12       I13  \n",
       "70000  0.547473  0.709442  0.930728  \n",
       "70001  0.129782  0.471361  0.386101  \n",
       "70002  0.192447  0.085893  0.957961  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lib = get_lib()\n",
    "input_cols = workflow.input_schema.column_names\n",
    "# read in data for request\n",
    "data = df_lib.read_parquet(\n",
    "    os.path.join(sorted(glob.glob(original_data_path + \"/*.parquet\"))[-1]),\n",
    "    columns=input_cols\n",
    ")\n",
    "batch = data[:3]\n",
    "batch = batch[[x for x in batch.columns if x not in ['label']]]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a Triton Inference Server request object. \n",
    "\n",
    "Currently, `NA` and `None` values are not supported for `int32` columns. As a workaround, we will `NA` values with `0`. The output of the HugeCTR model is called `OUTPUT0`. For the same reason of dropping the target column, we need to remove it from the input schema, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = workflow.input_schema.remove_col('label')\n",
    "inputs = convert_df_to_triton_input(\n",
    "    input_schema, \n",
    "    batch.fillna(0), \n",
    "    grpcclient.InferInput\n",
    ")\n",
    "output_cols = ['OUTPUT0']\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in output_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send the request to Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to tritonserver\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(\"criteo_ens\", inputs, request_id=\"1\", outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the predictions. The outputs are the probability scores, predicted by our model, how likely the ad will be clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT0 [0.52164096 0.50390565 0.4957397 ] (3,)\n"
     ]
    }
   ],
   "source": [
    "for col in output_cols:\n",
    "    print(col, response[col], response[col].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this example, we deployed a recommender system pipeline as an ensemble. First, NVTabular created features and afterwards, HugeCTR predicted the processed data. This process ensures that the training and production environments use the same feature engineering.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There is more detailed information in the [API documentation](https://nvidia-merlin.github.io/HugeCTR/stable/hugectr_user_guide.html) and [more examples](https://nvidia-merlin.github.io/HugeCTR/stable/notebooks/index.html) in the [HugeCTR repository](https://github.com/NVIDIA-Merlin/HugeCTR)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-hugectr:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
