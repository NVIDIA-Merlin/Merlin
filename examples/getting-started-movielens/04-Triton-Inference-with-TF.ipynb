{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_getting-started-movielens-04-triton-inference-with-tf/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Serve Recommendations from the TensorFlow Model\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container.\n",
    "\n",
    "The last step is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as done during training ETL. We need to ensure that our data is processed in the same fashion during training as in production. Therefore, we deploy the NVTabular workflow with the Merlin Model as an ensemble model to Triton Inference Server. The ensemble model guarantees that the same transformations are applied to the raw inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/triton-tf.png\" width=\"25%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching and Starting the Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, you should start the container for Triton Inference Server with the following command. This command includes the `-v` argument that mounts your local `model-repository` folder with your saved models from the previous notebook (`03a-Training-with-TF.ipynb`) to the `/model` directory in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -it --gpus device=0 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}:/model/ nvcr.io/nvidia/merlin/merlin-tensorflow:latest\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you started the container, you can start Triton Inference Server with the following command.\n",
    "You need to provide correct path for the `models` directory.\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=path_to_models --backend-config=tensorflow,version=2 --model-control-mode=explicit \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The model-repository path is `/root/nvt-examples/models`. The models haven't been loaded, yet. Below, we will request the Triton server to load the saved ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# Get dataframe library - cudf or pandas\n",
    "from merlin.core.dispatch import get_lib\n",
    "df_lib = get_lib()\n",
    "\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to preprocessed data\n",
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"~/nvt-examples/movielens/data/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deactivate the warnings before sending requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ensemble Model with Triton Inference Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you should have launched the Triton Inference Server docker container with the instructions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's connect to the Triton Inference Server. Use Tritonâ€™s ready endpoint to verify that the server and the models are ready for inference. Replace localhost with your host ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the server is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the available models in the repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '21'}>\n",
      "bytearray(b'[{\"name\":\"ensemble\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'ensemble'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/ensemble/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=400 headers={'content-type': 'application/json', 'content-length': '75'}>\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "failed to load 'ensemble', failed to poll from model repository",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:681\u001b[0m, in \u001b[0;36mInferenceServerClient.load_model\u001b[0;34m(self, model_name, headers, query_params, config, files)\u001b[0m\n\u001b[1;32m    676\u001b[0m         load_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m][path] \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(content)\n\u001b[1;32m    677\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(request_uri\u001b[38;5;241m=\u001b[39mrequest_uri,\n\u001b[1;32m    678\u001b[0m                       request_body\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(load_request),\n\u001b[1;32m    679\u001b[0m                       headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    680\u001b[0m                       query_params\u001b[38;5;241m=\u001b[39mquery_params)\n\u001b[0;32m--> 681\u001b[0m \u001b[43m_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:65\u001b[0m, in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     63\u001b[0m error \u001b[38;5;241m=\u001b[39m _get_error(response)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: failed to load 'ensemble', failed to poll from model repository"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a Request to Triton Inference Server to Transform a Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal model repository for a TensorFlow SavedModel model is:\n",
    "```\n",
    "  <model-repository-path>/<model-name>/\n",
    "      config.pbtxt\n",
    "      1/\n",
    "        model.savedmodel/\n",
    "           <saved-model files>\n",
    "```\n",
    "Let's check out our model repository layout. You can install tree library with apt-get install tree, and then run `!tree /model/models/` to print out the model repository layout as below:\n",
    "               \n",
    "```\n",
    "/model/models/\n",
    "|-- movielens\n",
    "|   |-- 1\n",
    "|   `-- config.pbtxt\n",
    "|-- movielens_nvt\n",
    "|   |-- 1\n",
    "|   |   |-- __pycache__\n",
    "|   |   |   `-- model.cpython-38.pyc\n",
    "|   |   |-- model.py\n",
    "|   |   `-- workflow\n",
    "|   |       |-- categories\n",
    "|   |       |   |-- unique.genres.parquet\n",
    "|   |       |   |-- unique.movieId.parquet\n",
    "|   |       |   `-- unique.userId.parquet\n",
    "|   |       |-- metadata.json\n",
    "|   |       `-- workflow.pkl\n",
    "|   `-- config.pbtxt\n",
    "`-- movielens_tf\n",
    "    |-- 1\n",
    "    |   `-- model.savedmodel\n",
    "    |       |-- assets\n",
    "    |       |-- saved_model.pb\n",
    "    |       `-- variables\n",
    "    |           |-- variables.data-00000-of-00001\n",
    "    |           `-- variables.index\n",
    "    `-- config.pbtxt\n",
    "```\n",
    "You can see that we have a `config.pbtxt` file. Each model in a model repository must include a model configuration that provides required and optional information about the model. Typically, this configuration is provided in a `config.pbtxt` file specified as [ModelConfig protobuf](https://github.com/triton-inference-server/server/blob/r20.12/src/core/model_config.proto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the raw validation set, and send 3 rows of `userId` and `movieId` as input to the saved NVTabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId\n",
      "5867328    37999     8874\n",
      "13455621   87008     2710\n",
      "15095829   97836     2797\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3, columns=[\"userId\", \"movieId\"]\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId [[37999]\n",
      " [87008]\n",
      " [97836]] (3, 1)\n",
      "movieId [[8874]\n",
      " [2710]\n",
      " [2797]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = nvt_triton.convert_df_to_triton_input([\"userId\", \"movieId\"], batch, grpcclient.InferInput)\n",
    "\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in [\"userId\", \"movieId\"]\n",
    "]\n",
    "\n",
    "MODEL_NAME_NVT = os.environ.get(\"MODEL_NAME_NVT\", \"movielens_nvt\")\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs, request_id=\"1\", outputs=outputs)\n",
    "\n",
    "for col in [\"userId\", \"movieId\"]:\n",
    "    print(col, response.as_numpy(col), response.as_numpy(col).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that we don't need to send the genres column as an input. The reason for that is the nvt model will look up the genres for each movie as part of the `JoinExternal` op it applies. Also notice that when creating the request for the `movielens_nvt` model, we return 2 columns (values and nnzs) for the `genres` column rather than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the End-To-End Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same, but this time we directly read in first 3 rows of the the raw `valid.parquet` file with cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name: \"movielens_nvt\"\n",
       "model_version: \"1\"\n",
       "id: \"1\"\n",
       "outputs {\n",
       "  name: \"userId\"\n",
       "  datatype: \"INT64\"\n",
       "  shape: 3\n",
       "  shape: 1\n",
       "}\n",
       "outputs {\n",
       "  name: \"movieId\"\n",
       "  datatype: \"INT64\"\n",
       "  shape: 3\n",
       "  shape: 1\n",
       "}\n",
       "raw_output_contents: \"o\\224\\000\\000\\000\\000\\000\\000\\340S\\001\\000\\000\\000\\000\\000,~\\001\\000\\000\\000\\000\\000\"\n",
       "raw_output_contents: \"\\252\\\"\\000\\000\\000\\000\\000\\000\\226\\n\\000\\000\\000\\000\\000\\000\\355\\n\\000\\000\\000\\000\\000\\000\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens_tf/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_tf'\n",
      "POST /v2/repository/models/movielens/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens'\n",
      "raw data:\n",
      "           userId  movieId\n",
      "5867328    37999     8874\n",
      "13455621   87008     2710\n",
      "15095829   97836     2797 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "triton_client.load_model(model_name=\"movielens_tf\")\n",
    "triton_client.load_model(model_name=\"movielens\")\n",
    "\n",
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3, columns=[\"userId\", \"movieId\"]\n",
    ")\n",
    "\n",
    "print(\"raw data:\\n\", batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "inputs = nvt_triton.convert_df_to_triton_input([\"userId\", \"movieId\"], batch, grpcclient.InferInput)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(\"rating\")]\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get(\"MODEL_NAME_ENSEMBLE\", \"movielens\")\n",
    "\n",
    "# build a client to connect to our server.\n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.grpc.InferInput object\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_ENSEMBLE, inputs, request_id=\"1\", outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name: \"movielens\"\n",
       "model_version: \"1\"\n",
       "id: \"1\"\n",
       "parameters {\n",
       "  key: \"sequence_end\"\n",
       "  value {\n",
       "    bool_param: false\n",
       "  }\n",
       "}\n",
       "parameters {\n",
       "  key: \"sequence_id\"\n",
       "  value {\n",
       "    int64_param: 0\n",
       "  }\n",
       "}\n",
       "parameters {\n",
       "  key: \"sequence_start\"\n",
       "  value {\n",
       "    bool_param: false\n",
       "  }\n",
       "}\n",
       "outputs {\n",
       "  name: \"rating\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: 3\n",
       "  shape: 1\n",
       "}\n",
       "raw_output_contents: \"1\\014 ?\\250!#?^p!?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sigmoid result:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted sigmoid result:\\n\", response.as_numpy(\"outputs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send request for a larger batch size and measure the total run time and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sigmoid result:\n",
      " [[0.6152474 ]\n",
      " [0.6197106 ]\n",
      " [0.6189732 ]\n",
      " [0.61421186]\n",
      " [0.61287224]\n",
      " [0.612817  ]\n",
      " [0.61092085]\n",
      " [0.6234857 ]\n",
      " [0.61349696]\n",
      " [0.61103773]\n",
      " [0.61636466]\n",
      " [0.61942095]\n",
      " [0.6194856 ]\n",
      " [0.6162945 ]\n",
      " [0.60351753]\n",
      " [0.609695  ]\n",
      " [0.62262255]\n",
      " [0.6180354 ]\n",
      " [0.6175308 ]\n",
      " [0.61908466]\n",
      " [0.6161449 ]\n",
      " [0.6128701 ]\n",
      " [0.60643166]\n",
      " [0.616207  ]\n",
      " [0.616828  ]\n",
      " [0.6228604 ]\n",
      " [0.6162526 ]\n",
      " [0.6177362 ]\n",
      " [0.6238064 ]\n",
      " [0.6199255 ]\n",
      " [0.6200201 ]\n",
      " [0.61577153]\n",
      " [0.6227766 ]\n",
      " [0.6123207 ]\n",
      " [0.6177533 ]\n",
      " [0.6164962 ]\n",
      " [0.6176095 ]\n",
      " [0.6182202 ]\n",
      " [0.6162337 ]\n",
      " [0.615147  ]\n",
      " [0.61689186]\n",
      " [0.6206259 ]\n",
      " [0.6219693 ]\n",
      " [0.6189993 ]\n",
      " [0.60714984]\n",
      " [0.61736315]\n",
      " [0.6237706 ]\n",
      " [0.6075932 ]\n",
      " [0.6135807 ]\n",
      " [0.6114176 ]\n",
      " [0.6124701 ]\n",
      " [0.61637044]\n",
      " [0.6148614 ]\n",
      " [0.62120986]\n",
      " [0.60603315]\n",
      " [0.61048526]\n",
      " [0.61690056]\n",
      " [0.61266035]\n",
      " [0.6128772 ]\n",
      " [0.6178152 ]\n",
      " [0.61760587]\n",
      " [0.61757046]\n",
      " [0.6055966 ]\n",
      " [0.6156989 ]] \n",
      "\n",
      "run_time(sec): 0.01111745834350586 - rows: 64 - inference_thru: 5756.71147330045\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch_size = 64\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(INPUT_DATA_DIR, \"valid.parquet\"),\n",
    "    num_rows=batch_size,\n",
    "    columns=[\"userId\", \"movieId\"],\n",
    ")\n",
    "\n",
    "start = time()\n",
    "# convert the batch to a triton inputs\n",
    "inputs = nvt_triton.convert_df_to_triton_input([\"userId\", \"movieId\"], batch, grpcclient.InferInput)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(\"output\")]\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get(\"MODEL_NAME_ENSEMBLE\", \"movielens\")\n",
    "\n",
    "# build a client to connect to our server.\n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.grpc.InferInput object\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_ENSEMBLE, inputs, request_id=\"1\", outputs=outputs)\n",
    "\n",
    "t_final = time() - start\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy(\"output\"), \"\\n\")\n",
    "\n",
    "print(f\"run_time(sec): {t_final} - rows: {batch_size} - inference_thru: {batch_size / t_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unload all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens'\n",
      "POST /v2/repository/models/movielens_nvt/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_nvt'\n",
      "POST /v2/repository/models/movielens_tf/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_tf'\n"
     ]
    }
   ],
   "source": [
    "triton_client.unload_model(model_name=\"movielens\")\n",
    "triton_client.unload_model(model_name=\"movielens_nvt\")\n",
    "triton_client.unload_model(model_name=\"movielens_tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
