{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d813a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dbfff",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_getting-started-movielens-04-triton-inference-with-hugectr/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Serve Recommendations from the HugeCTR Model\n",
    "\n",
    "This notebook is created using the latest stable [merlin-hugectr](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will show how we do inference with our trained deep learning recommender model using Triton Inference Server. In this example, we deploy the NVTabular workflow and HugeCTR model with Triton Inference Server. We deploy them as an ensemble. For each request, Triton Inference Server will feed the input data through the NVTabular workflow and its output through the HugeCR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0157e1c",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304a10",
   "metadata": {},
   "source": [
    "We need to write configuration files with the stored model weights and model configuration.\n",
    "\n",
    "Let us first move all of our model files to a directory that we will be able to access from the scripts that we will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ca4537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# path to preprocessed data\n",
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"~/nvt-examples/movielens/data/\")\n",
    ")\n",
    "\n",
    "# path to saved model\n",
    "MODEL_DIR = os.path.join(INPUT_DATA_DIR, \"model/movielens_hugectr\")\n",
    "\n",
    "!mkdir /model\n",
    "!mv {MODEL_DIR} /model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a9fbb6d",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model/movielens_hugectr/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /model/movielens_hugectr/config.pbtxt\n",
    "name: \"movielens_hugectr\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size: 64\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/model/movielens_hugectr/1/movielens.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"3\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"4\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"0\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"16\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a23cb52",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /model/ps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /model/ps.json\n",
    "{\n",
    "    \"supportlonglong\":true,\n",
    "    \"models\":[\n",
    "        {\n",
    "            \"model\":\"movielens_hugectr\",\n",
    "            \"sparse_files\":[\"/model/movielens_hugectr/0_sparse_1900.model\"],\n",
    "            \"dense_file\":\"/model/movielens_hugectr/_dense_1900.model\",\n",
    "            \"network_file\":\"/model/movielens_hugectr/1/movielens.json\",\n",
    "            \"num_of_worker_buffer_in_pool\": \"1\",\n",
    "            \"num_of_refresher_buffer_in_pool\": \"1\",\n",
    "            \"cache_refresh_percentage_per_iteration\": \"0.2\",\n",
    "            \"deployed_device_list\":[\"0\"],\n",
    "            \"max_batch_size\":\"64\",\n",
    "            \"default_value_for_each_table\":[\"0.0\",\"0.0\"],\n",
    "            \"hit_rate_threshold\":\"0.9\",\n",
    "            \"gpucacheper\":\"0.5\",\n",
    "            \"maxnum_catfeature_query_per_table_per_sample\":\"12\",\n",
    "            \"gpucache\":\"true\"\n",
    "        }\n",
    "    ]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3627f",
   "metadata": {},
   "source": [
    "Let's import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b54092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.grpc as httpclient\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4592a9",
   "metadata": {},
   "source": [
    "### Load Models on Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b4754",
   "metadata": {},
   "source": [
    "At this stage, you should start the Triton Inference Server in the container with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a350fce",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -it --gpus=all -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}:/model nvcr.io/nvidia/merlin/merlin-hugectr:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f50e9e",
   "metadata": {},
   "source": [
    "After you started the container you can start triton server with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8aa849",
   "metadata": {},
   "source": [
    "```\n",
    "tritonserver --model-repository=<path_to_models> --backend-config=hugectr,ps=<path_to_models>/ps.json --model-control-mode=explicit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7de550",
   "metadata": {},
   "source": [
    "Note: The model-repository path is `/model/`. The models haven't been loaded, yet. We can request triton server to load the saved ensemble.  We initialize a triton client. The path for the json file is `/model/movielens_hugectr/1/movielens.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9d1c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f86290af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a2bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac3dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '30'}>\n",
      "bytearray(b'[{\"name\":\"movielens_hugectr\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'movielens_hugectr'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2df62",
   "metadata": {},
   "source": [
    "Let's load our model to Triton Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a1ec18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens_hugectr/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=400 headers={'content-type': 'application/json', 'content-length': '144'}>\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "load failed for model 'movielens_hugectr': version 1 is at UNAVAILABLE state: Internal: attempt to access JSON non-array as array;\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:681\u001b[0m, in \u001b[0;36mInferenceServerClient.load_model\u001b[0;34m(self, model_name, headers, query_params, config, files)\u001b[0m\n\u001b[1;32m    676\u001b[0m         load_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m][path] \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(content)\n\u001b[1;32m    677\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(request_uri\u001b[38;5;241m=\u001b[39mrequest_uri,\n\u001b[1;32m    678\u001b[0m                       request_body\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(load_request),\n\u001b[1;32m    679\u001b[0m                       headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    680\u001b[0m                       query_params\u001b[38;5;241m=\u001b[39mquery_params)\n\u001b[0;32m--> 681\u001b[0m \u001b[43m_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:65\u001b[0m, in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     63\u001b[0m error \u001b[38;5;241m=\u001b[39m _get_error(response)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: load failed for model 'movielens_hugectr': version 1 is at UNAVAILABLE state: Internal: attempt to access JSON non-array as array;\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"movielens_hugectr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2d617",
   "metadata": {},
   "source": [
    "Let's send a request to Inference Server and print out the response. Since in our example above we do not have continuous columns, below our only inputs are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aea0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/model/data/valid/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f696c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./wdl2predict.py\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'movielens_hugectr'\n",
    "CATEGORICAL_COLUMNS = [\"userId\", \"movieId\", \"genres\"]\n",
    "CONTINUOUS_COLUMNS = []\n",
    "LABEL_COLUMNS = ['label']\n",
    "emb_size_array = [162542, 29434, 20]\n",
    "shift = np.insert(np.cumsum(emb_size_array), 0, 0)[:-1]\n",
    "df = pd.read_parquet(\"/model/data/valid/part_0.parquet\")\n",
    "test_df = df.head(10)\n",
    "\n",
    "rp_lst = [0]\n",
    "cur = 0\n",
    "for i in range(1, 31):\n",
    "    if i % 3 == 0:\n",
    "        cur += 2\n",
    "        rp_lst.append(cur)\n",
    "    else:\n",
    "        cur += 1\n",
    "        rp_lst.append(cur)\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    test_df.iloc[:, :2] = test_df.iloc[:, :2] + shift[:2]\n",
    "    test_df.iloc[:, 2] = test_df.iloc[:, 2].apply(lambda x: [e + shift[2] for e in x])\n",
    "    embedding_columns = np.array([list(np.hstack(np.hstack(test_df[CATEGORICAL_COLUMNS].values)))], dtype='int64')\n",
    "    dense_features = np.array([[]], dtype='float32')\n",
    "    row_ptrs = np.array([rp_lst], dtype='int32')\n",
    "\n",
    "    inputs = [httpclient.InferInput(\"DES\", dense_features.shape, np_to_triton_dtype(dense_features.dtype)),\n",
    "              httpclient.InferInput(\"CATCOLUMN\", embedding_columns.shape, np_to_triton_dtype(embedding_columns.dtype)),\n",
    "              httpclient.InferInput(\"ROWINDEX\", row_ptrs.shape, np_to_triton_dtype(row_ptrs.dtype))]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(dense_features)\n",
    "    inputs[1].set_data_from_numpy(embedding_columns)\n",
    "    inputs[2].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\")]\n",
    "\n",
    "    response = client.infer(model_name, inputs, request_id=str(1), outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339340c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./wdl2predict.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-hugectr:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
