{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models-transformers-net-item-prediction/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformer-based architecture for next-item prediction task\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will train a Transformer-based architecture for next-item prediction task.\n",
    "\n",
    "**Note, the data for this notebook will be automatically downloaded to the folder specified in the cells below.**\n",
    "\n",
    "We will use the [booking.com dataset](https://github.com/bookingcom/ml-dataset-mdt) to train a session-based model. The dataset contains 1,166,835 of anonymized hotel reservations in the train set and 378,667 in the test set. Each reservation is a part of a customer's trip (identified by `utrip_id`) which includes consecutive reservations.\n",
    "\n",
    "We will reshape the data to organize it into 'sessions'. Each session will be a full customer itinerary in chronological order. The goal will be to predict the city_id of the final reservation of each trip.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training a Transformer-based architecture for next-item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b619b",
   "metadata": {},
   "source": [
    "We will download the dataset using a functionality provided by merlin models. The dataset can be found on GitHub [here](https://github.com/bookingcom/ml-dataset-mdt).\n",
    "\n",
    "**Read more about libraries used in the import statements below**\n",
    "\n",
    "- [get_lib](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/core/dispatch.py)\n",
    "- [get_booking](https://github.com/NVIDIA-Merlin/models/tree/stable/merlin/datasets/ecommerce)\n",
    "- [nvtabular](https://github.com/NVIDIA-Merlin/NVTabular/tree/stable/nvtabular)\n",
    "- [nvtabular ops](https://github.com/NVIDIA-Merlin/NVTabular/tree/stable/nvtabular/ops)\n",
    "- [schema tags](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/tags.py)\n",
    "- [merlin models tensorflow](https://github.com/NVIDIA-Merlin/models/tree/stable/merlin/models/tf)\n",
    "- [get_booking](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/datasets/ecommerce/booking/dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e9ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 06:06:25.697025: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "2023-05-31 06:06:26.988036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:26.988386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:26.988518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 06:06:28.519868: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 06:06:28.520815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.520999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.521129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.591345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.591534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.591665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-31 06:06:28.591770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-05-31 06:06:28.591778: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-05-31 06:06:28.591860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1621] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Resetting the TF memory allocation to not be 50% by default. \n",
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.datasets.ecommerce import get_booking\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', '/workspace/data')\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', '/workspace/data')\n",
    "NUM_EPOCHS = int(os.environ.get('NUM_EPOCHS', '5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b42076",
   "metadata": {},
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a33352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.SESSION_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.SESSION: 'session'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<merlin.io.dataset.Dataset at 0x7fe90a7fce80>,\n",
       " <merlin.io.dataset.Dataset at 0x7fe90a7f7820>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_booking(INPUT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9dd8c8",
   "metadata": {},
   "source": [
    "Each reservation has a unique utrip_id. During each trip a customer vists several destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d1b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
      "0  1000027 2016-08-13 2016-08-14     8183      desktop          7168   \n",
      "1  1000027 2016-08-14 2016-08-16    15626      desktop          7168   \n",
      "2  1000027 2016-08-16 2016-08-18    60902      desktop          7168   \n",
      "3  1000027 2016-08-18 2016-08-21    30628      desktop           253   \n",
      "4  1000033 2016-04-09 2016-04-11    38677       mobile           359   \n",
      "\n",
      "  booker_country hotel_country   utrip_id  \n",
      "0        Elbonia        Gondal  1000027_1  \n",
      "1        Elbonia        Gondal  1000027_1  \n",
      "2        Elbonia        Gondal  1000027_1  \n",
      "3        Elbonia        Gondal  1000027_1  \n",
      "4         Gondal  Cobra Island  1000033_1  \n"
     ]
    }
   ],
   "source": [
    "# When displaying cudf dataframes use print() or display(), otherwise Jupyter creates hidden copies.\n",
    "train = get_lib().read_csv(f'{INPUT_DATA_DIR}/train_set.csv', parse_dates=['checkin', 'checkout'])\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc2d94",
   "metadata": {},
   "source": [
    "We will train on sequences of `city_id` and `booker_country` and based on this information, our model will attempt to predict the next `city_id` (the next hop in the journey).\n",
    "\n",
    "We will train a transformer model that can work with sequences of variable length within a batch. This functionality is provided to us out of the box and doesn't require any changes to the architecture. Thanks to it we do not have to pad or trim our sequences to any particular length -- our model can make effective use of all of the data!\n",
    "\n",
    "*With one exception.* For a masked language model that we will be training, we need to discard sequences that are shorter than two hops. This makes sense as there is nothing our model could learn if it was only presented with an itinerary with a single destination on it!\n",
    "\n",
    "Let us begin by splitting the data into a train and validation set based on trip ID.\n",
    "\n",
    "Let's see how many unique trips there are in the dataset. Also, let us shuffle the trips along the way so that our validation set consists of a random sample of our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bef6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique trips is : 217686\n"
     ]
    }
   ],
   "source": [
    "# Unique trip ids.\n",
    "utrip_ids = train.sample(frac=1).utrip_id.unique()\n",
    "print('Number of unique trips is :', len(utrip_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eca1f6",
   "metadata": {},
   "source": [
    "Now let's assign data to our train and validation sets. Furthermore, we sort the data by `utrip_id` and `checkin`. This way we ensure our sequences of visited `city_ids` will be in proper order!\n",
    "\n",
    "Also, let's remove trips where only a single city was visited as they cannot be modeled as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7754847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_lib().from_pandas(\n",
    "    train.to_pandas().join(train.to_pandas().groupby('utrip_id').size().rename('num_examples'), on='utrip_id')\n",
    ")\n",
    "train = train[train.num_examples > 1]\n",
    "\n",
    "train.checkin = train.checkin.astype('int')\n",
    "train.checkout = train.checkout.astype('int')\n",
    "\n",
    "train_set_utrip_ids = utrip_ids[:int(0.8 * utrip_ids.shape[0])]\n",
    "validation_set_utrip_ids = utrip_ids[int(0.8 * utrip_ids.shape[0]):]\n",
    "\n",
    "train_set = train[train.utrip_id.isin(train_set_utrip_ids)].sort_values(['utrip_id', 'checkin'])\n",
    "validation_set = train[train.utrip_id.isin(validation_set_utrip_ids)].sort_values(['utrip_id', 'checkin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc3992",
   "metadata": {},
   "source": [
    "##  Preprocessing with NVTabular\n",
    "\n",
    "We can now begin with data preprocessing.\n",
    "\n",
    "We will combine trips into \"sessions\", discard trips that are too short and calculate total trip length.\n",
    "\n",
    "We will use NVTabular for this work. It offers optimized tabular data preprocessing operators that run on the GPU. If you would like to learn more about the NVTabular library, please take a look [here](https://github.com/NVIDIA-Merlin/NVTabular).\n",
    "\n",
    "Read more about the [Merlin's Dataset API](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/io/dataset.py)  \n",
    "Read more about how [parquet files are read in and processed by Merlin](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/io/parquet.py)  \n",
    "Read more about [Tags](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/tags.py)  \n",
    "- [schema_select_by_tag](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/schema.py)  \n",
    "\n",
    "Read more about [NVTabular Workflows](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/workflow/workflow.py)  \n",
    "- [fit_transform](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/workflow/workflow.py)\n",
    "- [transform](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/workflow/workflow.py)  \n",
    "\n",
    "Read more about the [NVTabular Operators]()  \n",
    "- [Categorify](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/ops/categorify.py)\n",
    "- [AddTags](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/ops/add_metadata.py)\n",
    "- [LambdaOp](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/ops/lambdaop.py)\n",
    "- [Rename](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/ops/rename.py)\n",
    "- [Filter](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/nvtabular/ops/filter.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3435af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dataset = Dataset(train_set)\n",
    "validation_set_dataset = Dataset(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bd5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_checkin = (\n",
    "    [\"checkin\"]\n",
    "    >> ops.LambdaOp(lambda col: get_lib().to_datetime(col).dt.weekday)\n",
    "    >> ops.Rename(name=\"weekday_checkin\")\n",
    ")\n",
    "\n",
    "weekday_checkout = (\n",
    "    [\"checkout\"]\n",
    "    >> ops.LambdaOp(lambda col: get_lib().to_datetime(col).dt.weekday)\n",
    "    >> ops.Rename(name=\"weekday_checkout\")\n",
    ")\n",
    "\n",
    "categorical_features = (['city_id', 'booker_country', 'hotel_country'] +\n",
    "                         weekday_checkin + weekday_checkout\n",
    "                       ) >> ops.Categorify()\n",
    "\n",
    "groupby_features = categorical_features + ['utrip_id', 'checkin'] >> ops.Groupby(\n",
    "    groupby_cols=['utrip_id'],\n",
    "    aggs={\n",
    "        'city_id': ['list', 'count'],\n",
    "        'booker_country': ['list'],\n",
    "        'hotel_country': ['list'],\n",
    "        'weekday_checkin': ['list'],\n",
    "        'weekday_checkout': ['list']\n",
    "    },\n",
    "    sort_cols=\"checkin\"\n",
    ")\n",
    "\n",
    "list_features = (\n",
    "            groupby_features['city_id_list', 'booker_country_list', 'hotel_country_list', \n",
    "                 'weekday_checkin_list', 'weekday_checkout_list'\n",
    "            ] >> ops.AddTags([Tags.SEQUENCE])\n",
    ")\n",
    "\n",
    "# Filter out sessions with less than 2 interactions \n",
    "MINIMUM_SESSION_LENGTH = 2\n",
    "features = list_features + (groupby_features['city_id_count'] >>  ops.AddTags([Tags.CONTINUOUS]))\n",
    "filtered_sessions = features >> ops.Filter(f=lambda df: df[\"city_id_count\"] >= MINIMUM_SESSION_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6105767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(filtered_sessions)\n",
    "\n",
    "wf.fit_transform(train_set_dataset).to_parquet(os.path.join(OUTPUT_DATA_DIR, 'train_processed.parquet'))\n",
    "wf.transform(validation_set_dataset).to_parquet(os.path.join(OUTPUT_DATA_DIR, 'validation_processed.parquet'))\n",
    "\n",
    "wf.save(os.path.join(OUTPUT_DATA_DIR, 'workflow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6675",
   "metadata": {},
   "source": [
    "Our data consists of a sequence of visited `city_ids`, a sequence of `booker_countries` (represented as integer categories) and a `city_id_count` column (which contains the count of visited cities in a trip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dee6b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id_list</th>\n",
       "      <th>booker_country_list</th>\n",
       "      <th>hotel_country_list</th>\n",
       "      <th>weekday_checkin_list</th>\n",
       "      <th>weekday_checkout_list</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8238, 156, 2278, 2097]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "      <td>[5, 7, 4, 3]</td>\n",
       "      <td>[7, 4, 2, 7]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[63, 1160, 87, 618, 63]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 1, 4, 3, 5]</td>\n",
       "      <td>[6, 4, 2, 5, 4]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[7, 6, 24, 1050, 65, 52, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>[2, 2, 2, 16, 16, 3, 3]</td>\n",
       "      <td>[5, 1, 2, 6, 5, 7, 4]</td>\n",
       "      <td>[6, 3, 1, 5, 7, 4, 3]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1032, 757, 140, 3]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[19, 19, 19, 3]</td>\n",
       "      <td>[1, 4, 2, 3]</td>\n",
       "      <td>[4, 3, 2, 5]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3603, 262, 662, 250, 359]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[30, 30, 30, 30, 30]</td>\n",
       "      <td>[1, 3, 6, 5, 1]</td>\n",
       "      <td>[2, 1, 5, 6, 3]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  city_id_list    booker_country_list  \\\n",
       "0      [8238, 156, 2278, 2097]           [3, 3, 3, 3]   \n",
       "1      [63, 1160, 87, 618, 63]        [1, 1, 1, 1, 1]   \n",
       "2  [7, 6, 24, 1050, 65, 52, 3]  [2, 2, 2, 2, 2, 2, 2]   \n",
       "3          [1032, 757, 140, 3]           [2, 2, 2, 2]   \n",
       "4   [3603, 262, 662, 250, 359]        [1, 1, 1, 1, 1]   \n",
       "\n",
       "        hotel_country_list   weekday_checkin_list  weekday_checkout_list  \\\n",
       "0             [3, 3, 3, 3]           [5, 7, 4, 3]           [7, 4, 2, 7]   \n",
       "1          [1, 1, 1, 1, 1]        [5, 1, 4, 3, 5]        [6, 4, 2, 5, 4]   \n",
       "2  [2, 2, 2, 16, 16, 3, 3]  [5, 1, 2, 6, 5, 7, 4]  [6, 3, 1, 5, 7, 4, 3]   \n",
       "3          [19, 19, 19, 3]           [1, 4, 2, 3]           [4, 3, 2, 5]   \n",
       "4     [30, 30, 30, 30, 30]        [1, 3, 6, 5, 1]        [2, 1, 5, 6, 3]   \n",
       "\n",
       "   city_id_count  \n",
       "0              4  \n",
       "1              5  \n",
       "2              7  \n",
       "3              4  \n",
       "4              5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset(os.path.join(OUTPUT_DATA_DIR, 'train_processed.parquet')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc3a0",
   "metadata": {},
   "source": [
    "We are now ready to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c794",
   "metadata": {},
   "source": [
    "Here is the schema of the data that our model will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4813456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city_id_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.city_id.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>37202</td>\n",
       "      <td>city_id</td>\n",
       "      <td>37203</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.booker_country.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>booker_country</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotel_country_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.hotel_country.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>hotel_country</td>\n",
       "      <td>195</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekday_checkin_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.weekday_checkin.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>weekday_checkin</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekday_checkout_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.weekday_checkout.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>weekday_checkout</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'city_id_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.city_id.parquet', 'domain': {'min': 0, 'max': 37202, 'name': 'city_id'}, 'embedding_sizes': {'cardinality': 37203, 'dimension': 512}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'booker_country_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.booker_country.parquet', 'domain': {'min': 0, 'max': 5, 'name': 'booker_country'}, 'embedding_sizes': {'cardinality': 6, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'hotel_country_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.hotel_country.parquet', 'domain': {'min': 0, 'max': 194, 'name': 'hotel_country'}, 'embedding_sizes': {'cardinality': 195, 'dimension': 31}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'weekday_checkin_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.weekday_checkin.parquet', 'domain': {'min': 0, 'max': 7, 'name': 'weekday_checkin'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'weekday_checkout_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.weekday_checkout.parquet', 'domain': {'min': 0, 'max': 7, 'name': 'weekday_checkout'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_schema = Workflow.load(os.path.join(OUTPUT_DATA_DIR, 'workflow')).output_schema.select_by_tag(Tags.SEQUENCE)\n",
    "seq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d422833",
   "metadata": {},
   "source": [
    "Let's also identify the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b90424a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city_id_list'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Workflow.load(os.path.join(OUTPUT_DATA_DIR, 'workflow')).output_schema.select_by_tag(Tags.SEQUENCE).column_names[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8adad",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb17fe",
   "metadata": {},
   "source": [
    "Let's construct our model.\n",
    "\n",
    "We can specify various hyperparameters, such as the number of heads and number of layers to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a460e4c",
   "metadata": {},
   "source": [
    "For the transformer portion of our model, we will use the `XLNet` architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf02dc",
   "metadata": {},
   "source": [
    "Later, when we run the `fit` method on our model, we will specify the `masking_probability` of `0.3` and link it to the transformer block defined in out model. Through the combination of these parameters, our model will train on sequences where any given timestep will be masked with a probability of 0.3 and it will be our model's training task to infer the target value for that step!\n",
    "\n",
    "To summarize, Masked Language Modeling is implemented by:\n",
    "\n",
    "* `SequenceMaskRandom()` - Used as a pre for model.fit(), it randomly selects items from the sequence to be masked for prediction as targets, by using Keras masking. This block also adds the necessary configuration to the specified `transformer` block so as it\n",
    "is pre-configured with the necessary layers needed to prepare the inputs to the HuggingFace transformer layer and to post-process its outputs. For example, one pre-processing operation is to replace the input embeddings at masked positions for prediction by a dummy trainable embedding, to avoid leakage of the targets.\n",
    "\n",
    "\n",
    "**Read more about the apis used to construct models** \n",
    "- [blocks](https://github.com/NVIDIA-Merlin/models/tree/stable/merlin/models/tf/blocks)\n",
    "- [MLPBlock](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/blocks/mlp.py)\n",
    "- [InputBlockV2](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/inputs/base.py)\n",
    "- [Embeddings](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/inputs/embedding.py)\n",
    "- [XLNetBlock](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/transformers/block.py)\n",
    "- [CategoricalOutput](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/outputs/classification.py)\n",
    "- [.schema.select_by_name](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/schema.py)\n",
    "- [.schema.select_by_tag](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/schema.py)\n",
    "- [model.compile()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/models/base.py)\n",
    "- [model.fit()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/models/base.py)\n",
    "- [model.evaluate()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/models/base.py)\n",
    "- [mm.SequenceMaskRandom](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/transforms/sequence.py)\n",
    "- [mm.SequenceMaskLast](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/models/tf/transforms/sequence.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cddfd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel=48\n",
    "mlp_block = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "            )\n",
    "transformer_block = mm.XLNetBlock(d_model=dmodel, n_head=4, n_layer=2)\n",
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "        seq_schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            Workflow.load(os.path.join(OUTPUT_DATA_DIR, 'workflow')).output_schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "    ),\n",
    "    mlp_block,\n",
    "    transformer_block,\n",
    "    mm.CategoricalOutput(\n",
    "        Workflow.load(os.path.join(OUTPUT_DATA_DIR, 'workflow')).output_schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac975cd",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65d28c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 06:06:44.034041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 06:06:54.541024: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/xl_net_block/sequential_block_5/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720/2720 [==============================] - 81s 25ms/step - loss: 7.3315 - recall_at_10: 0.1973 - mrr_at_10: 0.0863 - ndcg_at_10: 0.1123 - map_at_10: 0.0863 - precision_at_10: 0.0197 - regularization_loss: 0.0000e+00 - loss_batch: 7.3306\n",
      "Epoch 2/5\n",
      "2720/2720 [==============================] - 70s 25ms/step - loss: 6.0979 - recall_at_10: 0.3633 - mrr_at_10: 0.1707 - ndcg_at_10: 0.2161 - map_at_10: 0.1707 - precision_at_10: 0.0363 - regularization_loss: 0.0000e+00 - loss_batch: 6.0950\n",
      "Epoch 3/5\n",
      "2720/2720 [==============================] - 71s 26ms/step - loss: 5.5827 - recall_at_10: 0.4306 - mrr_at_10: 0.2056 - ndcg_at_10: 0.2588 - map_at_10: 0.2056 - precision_at_10: 0.0431 - regularization_loss: 0.0000e+00 - loss_batch: 5.5806\n",
      "Epoch 4/5\n",
      "2720/2720 [==============================] - 72s 26ms/step - loss: 5.3211 - recall_at_10: 0.4627 - mrr_at_10: 0.2213 - ndcg_at_10: 0.2784 - map_at_10: 0.2213 - precision_at_10: 0.0463 - regularization_loss: 0.0000e+00 - loss_batch: 5.3194\n",
      "Epoch 5/5\n",
      "2720/2720 [==============================] - 71s 26ms/step - loss: 5.1920 - recall_at_10: 0.4787 - mrr_at_10: 0.2306 - ndcg_at_10: 0.2892 - map_at_10: 0.2306 - precision_at_10: 0.0479 - regularization_loss: 0.0000e+00 - loss_batch: 5.1903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe67105a7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(run_eagerly=False, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.fit(\n",
    "    Dataset(os.path.join(OUTPUT_DATA_DIR, 'train_processed.parquet')),\n",
    "    batch_size=64,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    pre=mm.SequenceMaskRandom(schema=seq_schema, target=target, masking_prob=0.3, transformer=transformer_block)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24699106",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d87d27",
   "metadata": {},
   "source": [
    "We have trained our model.\n",
    "\n",
    "But in training the metrics come from a masked language modelling task. A portion of steps in the sequence was masked for each example. The metrics were calculated on this task.\n",
    "\n",
    "In reality, we probably care how well our model does on the next item prediction task (as it mimics the scenario in which the model would be likely to be used).\n",
    "\n",
    "Let's measure the performance of the model on a task where it attempts to predict the last item in a sequence.\n",
    "\n",
    "We will mask the last item using `SequenceMaskLast` and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3c6358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 06:12:51.968982: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/xl_net_block/sequential_block_5/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 11s 20ms/step - loss: 4.7151 - recall_at_10: 0.5533 - mrr_at_10: 0.3083 - ndcg_at_10: 0.3665 - map_at_10: 0.3083 - precision_at_10: 0.0553 - regularization_loss: 0.0000e+00 - loss_batch: 4.7149\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(\n",
    "    Dataset(os.path.join(OUTPUT_DATA_DIR, 'validation_processed.parquet')),\n",
    "    batch_size=128,\n",
    "    pre=mm.SequenceMaskLast(schema=seq_schema, target=target, transformer=transformer_block),\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ca276f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 4.715089797973633,\n",
       " 'recall_at_10': 0.5533444881439209,\n",
       " 'mrr_at_10': 0.30831339955329895,\n",
       " 'ndcg_at_10': 0.36654922366142273,\n",
       " 'map_at_10': 0.30831339955329895,\n",
       " 'precision_at_10': 0.055334459990262985,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 4.635858535766602}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5cc29",
   "metadata": {},
   "source": [
    "## Serving predictions using the Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6ee5f",
   "metadata": {},
   "source": [
    "Now, we will serve our trained models on [NVIDIA Triton Inference Server (TIS)](https://github.com/triton-inference-server/server). TIS is an open-source inference serving software that helps standardize model deployment and execution and delivers fast and scalable AI in production. To serve recommender models on TIS easily, NVIDIA Merlin team designed and developed [the Merlin Systems library](https://github.com/NVIDIA-Merlin/systems). Merlin Systems provides tools and operators to be able to serve end-to-end recommender systems pipelines on TIS easily\n",
    "\n",
    "In order to perform inference on the Triton Inference Server, we need to output the inference operators to disk.\n",
    "\n",
    "The inference operators form an `Ensemble`, which is a pipeline that takes in raw data, processes it using NVTabular, and finally outputs predictions from the model that we trained.\n",
    "\n",
    "Let's write the `Ensemble` to disk (we will later load it on Triton to perform inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae33813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, sequence_mask_random_layer_call_fn, sequence_mask_random_layer_call_and_return_conditional_losses, sequence_mask_last_layer_call_fn while saving (showing 5 of 108). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1sakw940/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1sakw940/model.savedmodel/assets\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:101: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[key] = tf.keras.utils.serialize_keras_object(maybe_value)\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:288: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[i] = tf.keras.utils.serialize_keras_object(layer)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/saving/legacy/saved_model/layer_serialization.py:134: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return serialization.serialize_keras_object(obj)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/systems/dag/node.py:100: UserWarning: Operator 'TransformWorkflow' is producing the output column 'city_id_count', which is not being used by any downstream operator in the ensemble graph.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (city_id_list): TensorShape([64, None, 1])\n",
      "    (booker_country_list): TensorShape([64, None, 1])\n",
      "    (hotel_country_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkin_list): TensorShape([64, None, 1])\n",
      "    (weekday_checkout_list): TensorShape([64, None, 1])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (city_id_list): tf.int64\n",
      "    (booker_country_list): tf.int64\n",
      "    (hotel_country_list): tf.int64\n",
      "    (weekday_checkin_list): tf.int64\n",
      "    (weekday_checkout_list): tf.int64\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, sequence_mask_random_layer_call_fn, sequence_mask_random_layer_call_and_return_conditional_losses, sequence_mask_last_layer_call_fn while saving (showing 5 of 108). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/data/ensemble/1_predicttensorflowtriton/1/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/data/ensemble/1_predicttensorflowtriton/1/model.savedmodel/assets\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:101: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[key] = tf.keras.utils.serialize_keras_object(maybe_value)\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:288: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[i] = tf.keras.utils.serialize_keras_object(layer)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/saving/legacy/saved_model/layer_serialization.py:134: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return serialization.serialize_keras_object(obj)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "inf_ops = wf.input_schema.column_names >> TransformWorkflow(wf) >> PredictTensorflow(model)\n",
    "\n",
    "ensemble = Ensemble(inf_ops, wf.input_schema)\n",
    "ensemble.export(os.path.join(OUTPUT_DATA_DIR, 'ensemble'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc6046",
   "metadata": {},
   "source": [
    "After we export the ensemble, we are ready to start the Triton Inference Server.\n",
    "\n",
    "The server is installed in Merlin Tensorflow and Merlin PyTorch containers. If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server [documentation](https://github.com/triton-inference-server/server/blob/r22.03/README.md#documentation).\n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "```tritonserver --model-repository={OUTPUT_DATA_DIR}/ensemble/```\n",
    "\n",
    "For the --model-repository argument, specify the same value as the `export_path` that you specified previously in the `ensemble.export` method.\n",
    "\n",
    "After you run the `tritonserver` command, wait until your terminal shows messages like the following example:\n",
    "\n",
    "I0414 18:29:50.741833 4067 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001<br>\n",
    "I0414 18:29:50.742197 4067 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000<br>\n",
    "I0414 18:29:50.783470 4067 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
    "\n",
    "Let us now package our data for inference. We will send the first 4 rows of our validation data, which corresponds to a single trip. The data will be first processed by the `NVTabular` workflow and subsequentally passed to our transformer model for predicting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a304d",
   "metadata": {},
   "source": [
    "Let us send the first 4 rows of our validation data to Triton. This will correspond to a single trip (all rows have the same `utrip_id`) with four stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cad9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "\n",
    "validation_data = validation_set_dataset.compute()\n",
    "inputs = convert_df_to_triton_input(wf.input_schema, validation_data.iloc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c508adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer('executor_model', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34eecf",
   "metadata": {},
   "source": [
    "The response consists of logits coming from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3284691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8206294 , -1.3849059 ,  1.9042726 , ...,  0.851537  ,\n",
       "        -2.4237087 , -0.73849726]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.as_numpy('city_id_list/categorical_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "824d2b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37203)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = response.as_numpy('city_id_list/categorical_output')\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d415b",
   "metadata": {},
   "source": [
    "The above values are logits output from the last layer of our model. They correspond in size to the cardinality of `city_id`, our target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29a8c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37203"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardinality = wf.output_schema['city_id_list'].properties['embedding_sizes']['cardinality']\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54c30f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c07fb",
   "metadata": {},
   "source": [
    "We have trained a transformer model for the next item prediction task using language model masking.\n",
    "\n",
    "For another session-based example that goes deeper into data preprocessing and that covers several advanced techniques (Weight Tying, Temperature Scaling) please see [Session-Based Next Item Prediction for Fashion E-Commerce](https://github.com/NVIDIA-Merlin/models/blob/t4rec_use_case/examples/usecases/ecommerce-session-based-next-item-prediction-for-fashion.ipynb). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
