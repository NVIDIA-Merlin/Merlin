{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d18943-381f-4096-b8c2-349cd12f57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73569d-99f1-4449-bfd1-8331fcde362d",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_getting-started-movielens-01-download-convert/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Training and Serving Merlin on AWS SageMaker\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container.\n",
    "Note that AWS libraries in this notebook require AWS credentials, and if you are running this notebook in a container, you might need to restart the container with the AWS credentials mounted, e.g., `-v $HOME/.aws:$HOME/.aws`.\n",
    "\n",
    "\n",
    "With AWS Sagemaker, you can package your own models that can then be trained and deployed in the SageMaker environment. This notebook shows you how to use Merlin for training and inference in the SageMaker environment.\n",
    "\n",
    "To run this notebook, you need to be able to run [AWS CLI](https://aws.amazon.com/cli/) and also have [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526ece09-f855-44af-8d82-5e2ff9a317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip -q install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6eadc-10ab-41c4-ba55-67ea59ac6fb5",
   "metadata": {},
   "source": [
    "## Part 1: Preparing your Merlin model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c864a8-5c09-44e7-9ae1-bc02c9b700e6",
   "metadata": {},
   "source": [
    "## Testing your algorithm on your local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f34b1-6bc2-4338-9d4e-09940c378966",
   "metadata": {},
   "source": [
    "In this notebook, we use the synthetic train and test datasets generated by mimicking the real [Ali-CCP](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1): Alibaba Click and Conversion Prediction dataset to build our recommender system ranking models. The Ali-CCP is a dataset gathered from real-world traffic logs of the recommender system in Taobao, the largest online retail platform in the world.\n",
    "\n",
    "If you would like to use real Ali-CCP dataset instead, you can download the training and test datasets on [tianchi.aliyun.com](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1). You can then use [get_aliccp()](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/datasets/ecommerce/aliccp/dataset.py#L43) function to curate the raw csv files and save them as parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96e35df-27b4-4979-a3f5-676593c87106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 1000000)\n",
    "SYNTHETIC_DATA = eval(os.environ.get(\"SYNTHETIC_DATA\", \"True\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 512))\n",
    "\n",
    "if SYNTHETIC_DATA:\n",
    "    train, valid = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))\n",
    "    # save the datasets as parquet files\n",
    "    train.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"train\"))\n",
    "    valid.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb0b2e-b865-4b48-8182-ce6f8c4f0789",
   "metadata": {},
   "source": [
    "Before you run your algorithm on SageMaker, you probably want to test and train your training algorithm locally first to make sure that it's working correctly.\n",
    "The training script [train.py](./train.py) in this example starts with the synthethic dataset we have created in the previous cell and produces a ranking model by performing the following tasks:\n",
    "- Perform feature engineering and preprocessing with [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular). NVTabular implements common feature engineering and preprocessing operators in easy-to-use, high-level APIs.\n",
    "- Use [Merlin Models](https://github.com/NVIDIA-Merlin/models/) to train [Facebook's DLRM model](https://arxiv.org/pdf/1906.00091.pdf) in Tensorflow.\n",
    "- Prepares [ensemble models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models) for serving on [Triton Inference Server](https://github.com/triton-inference-server/server).\n",
    "The training script outputs the final ensemble models to `model_dir`. You want to make sure that your script generates any artifacts within `model_dir`, since SageMaker packages any files in this directory into a compressed tar archive and made available at the S3 location. Ensemble models that are uploaded to S3 will be used later to handle predictions in Triton inference server later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60996955-1d6c-43e7-b29a-ad451b9aee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:49:52.902606: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-10-20 17:49:53.887953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:53.888286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:53.888376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:53.897977: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 17:49:53.899071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:53.899184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:53.899263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:55.086163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:55.086301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:55.086385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-20 17:49:55.086467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2867 MB memory:  -> device: 0, name: NVIDIA T600 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-10-20 17:49:55.089622: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.80G (3006267392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-10-20 17:49:55.089829: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.52G (2705640704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-10-20 17:49:55.090027: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.27G (2435076608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-10-20 17:49:55.090229: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.04G (2191568896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 193, in <module>\n",
      "    train()\n",
      "  File \"train.py\", line 128, in train\n",
      "    train_dataset = nvt.Dataset(train_path)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/io/dataset.py\", line 303, in __init__\n",
      "    self.engine = ParquetDatasetEngine(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/io/parquet.py\", line 311, in __init__\n",
      "    self._real_meta, rg_byte_size_0 = run_on_worker(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/core/utils.py\", line 486, in run_on_worker\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/io/parquet.py\", line 1210, in _sample_row_group\n",
      "    _df = cudf.io.read_parquet(path, row_groups=0, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py\", line 101, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/cudf/io/parquet.py\", line 470, in read_parquet\n",
      "    return _parquet_to_frame(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py\", line 101, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/cudf/io/parquet.py\", line 499, in _parquet_to_frame\n",
      "    return _read_parquet(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py\", line 101, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/cudf/io/parquet.py\", line 576, in _read_parquet\n",
      "    return libparquet.read_parquet(\n",
      "  File \"cudf/_lib/parquet.pyx\", line 113, in cudf._lib.parquet.read_parquet\n",
      "  File \"cudf/_lib/parquet.pyx\", line 173, in cudf._lib.parquet.read_parquet\n",
      "MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/rapids/rmm/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py \\\n",
    "    --train_dir={DATA_FOLDER}/train/ \\\n",
    "    --valid_dir={DATA_FOLDER}/valid/ \\\n",
    "    --model_dir=/tmp/ \\\n",
    "    --batch_size=512 \\\n",
    "    --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e6bb6-1d68-4797-a2b5-d87d164995cc",
   "metadata": {},
   "source": [
    "### The `Dockerfile`\n",
    "\n",
    "The `Dockerfile` describes the image that will be used on SageMaker for training and inference.\n",
    "We start from the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) docker image and install the [sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit) library, which makes the image compatible with Sagemaker for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ba6bc5-f6ea-4d35-88f9-23683826e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      "\n",
      "RUN pip3 install sagemaker-training\n"
     ]
    }
   ],
   "source": [
    "! cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9afdca-19f6-45e6-8a73-3878c87a3577",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is available as the shell script `build_and_push_image.sh`. If you are running this notebook inside the [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) docker container, you probably want to execute the script outside the container.\n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this is the region where the notebook instance was created). If the repository doesn't exist, the script will create it.\n",
    "\n",
    "Note that running the following script requires permissions to create new repositories on Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e40ffca-b651-413f-ac64-43f44a01e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "# The name of our algorithm\n",
      "ALGORITHM_NAME=sagemaker-merlin-tensorflow\n",
      "REGION=us-east-1\n",
      "\n",
      "cd container\n",
      "\n",
      "ACCOUNT=$(aws sts get-caller-identity --query Account --output text --region ${REGION})\n",
      "\n",
      "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
      "\n",
      "REPOSITORY=\"${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com\"\n",
      "IMAGE_URI=\"${REPOSITORY}/${ALGORITHM_NAME}:latest\"\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin ${REPOSITORY}\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "\n",
      "aws ecr describe-repositories --repository-names \"${ALGORITHM_NAME}\" --region ${REGION} > /dev/null 2>&1\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    aws ecr create-repository --repository-name \"${ALGORITHM_NAME}\" --region ${REGION} > /dev/null\n",
      "fi\n",
      "\n",
      "# Build the docker image locally with the image name and then push it to ECR\n",
      "# with the full name.\n",
      "\n",
      "docker build  -t ${ALGORITHM_NAME} .\n",
      "docker tag ${ALGORITHM_NAME} ${IMAGE_URI}\n",
      "\n",
      "docker push ${IMAGE_URI}\n"
     ]
    }
   ],
   "source": [
    "! cat ./build_and_push_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d525205-b00a-4177-ab14-8d3a218d4ac9",
   "metadata": {},
   "source": [
    "## Part 2: Training your Merlin model on Sagemaker\n",
    "\n",
    "Once you have tested your script that creates a Merlin ensemble graph, you can use it to train it on Sagemaker.\n",
    "\n",
    "Here, we create a Sagemaker session that we will use to perform our Sagemaker operations, specify the bucket to use, and the role for working with Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b62f39e-af41-4aec-857f-0541038d9c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIRECTORY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# S3 prefix\u001b[39;00m\n\u001b[1;32m      6\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEMO-merlin-tensorflow-aliccp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m data_location \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mupload_data(DATA_DIRECTORY, key_prefix\u001b[38;5;241m=\u001b[39mprefix)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_location)\n\u001b[1;32m     12\u001b[0m role \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mget_execution_role()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIRECTORY' is not defined"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 prefix\n",
    "prefix = \"DEMO-merlin-tensorflow-aliccp\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5d758-e4f3-4f44-ba4f-9823f74a04e0",
   "metadata": {},
   "source": [
    "We can use the Sagemaker Python SDK to upload the Ali-CCP synthetic data to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9109d1-73cf-4177-b896-09cff9289bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sess.upload_data(DATA_DIRECTORY, key_prefix=prefix)\n",
    "\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dbb12-76fb-41c0-ad90-32b7d1328e62",
   "metadata": {},
   "source": [
    "### Training on Sagemaker using the Python SDK\n",
    "\n",
    "Sagemaker provides the Python SDK for training a model on Sagemaker.\n",
    "\n",
    "Here, we start by using the ECR image URL of the image we pushed in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b9fc54-bafa-4487-8f34-2f9448c0b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843263297212.dkr.ecr.us-east-1.amazonaws.com/sagemaker-merlin-tensorflow:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "algorithm_name = \"sagemaker-merlin-tensorflow\"\n",
    "\n",
    "ecr_image = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, algorithm_name)\n",
    "\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51cafe-8685-4789-b348-e70b2f811035",
   "metadata": {},
   "source": [
    "We can call `Estimator.fit()` to start training on Sagemaker. Here, we use a `g4dn` GPU instance that are equipped with NVIDIA T4 GPUs.\n",
    "Our training script `train.py` is passed to the Estimator through the `entry_point` parameter, and we can adjust our hyperparameters in the `hyperparameters`.\n",
    "We have uploaded our training dataset to our S3 bucket in the previous code cell, and the S3 URLs to our training and validation sets are passed into the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f238c8c6-656e-4b3d-96c6-32654357fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 23:05:16 Starting - Starting the training job...\n",
      "2022-10-18 23:05:41 Starting - Preparing the instances for trainingProfilerReport-1666134315: InProgress\n",
      "......\n",
      "2022-10-18 23:06:52 Downloading - Downloading input data...\n",
      "2022-10-18 23:07:22 Training - Downloading the training image......................................\u001b[34m==================================\u001b[0m\n",
      "\u001b[34m== Triton Inference Server Base ==\u001b[0m\n",
      "\u001b[34m==================================\u001b[0m\n",
      "\u001b[34mNVIDIA Release 22.08 (build 42766143)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34mNOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.7 driver version 515.65.01 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\u001b[0m\n",
      "\u001b[34m2022-10-18 23:13:53,855 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 1024,\n",
      "        \"epoch\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":1024,\"epoch\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":1024,\"epoch\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"1024\",\"--epoch\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=1024\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/opt/tritonserver:/usr/local/lib/python3.8/dist-packages:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages/faiss-1.7.2-py3.8.egg:/usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg:/usr/local/lib/python3.8/dist-packages/merlin_hps-1.0.0-py3.8-linux-x86_64.egg:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --batch_size 1024 --epoch 10\u001b[0m\n",
      "\u001b[34m2022-10-18 23:13:53,856 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2022-10-18 23:13:59.437699: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[0m\n",
      "\n",
      "2022-10-18 23:14:03 Training - Training image download completed. Training in progress.\u001b[34m2022-10-18 23:14:04.196211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.197986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.198261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.236763: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.238319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.238639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:04.238863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:08.392792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:08.393079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:08.393286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 23:14:08.394189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10752 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\u001b[0m\n",
      "\u001b[34mWorkflow saved to /tmp/tmp1nv79anb/workflow.\u001b[0m\n",
      "\u001b[34mbatch_size = 1024, epochs = 10\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m684/684 - 16s - loss: 0.6932 - auc: 0.4996 - regularization_loss: 0.0000e+00 - val_loss: 0.6932 - val_auc: 0.4990 - val_regularization_loss: 0.0000e+00 - 16s/epoch - 24ms/step\u001b[0m\n",
      "\u001b[34mEpoch 2/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6931 - auc: 0.5024 - regularization_loss: 0.0000e+00 - val_loss: 0.6932 - val_auc: 0.4996 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 13ms/step\u001b[0m\n",
      "\u001b[34mEpoch 3/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6924 - auc: 0.5198 - regularization_loss: 0.0000e+00 - val_loss: 0.6937 - val_auc: 0.4989 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 13ms/step\u001b[0m\n",
      "\u001b[34mEpoch 4/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6868 - auc: 0.5470 - regularization_loss: 0.0000e+00 - val_loss: 0.6976 - val_auc: 0.4983 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 5/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6797 - auc: 0.5643 - regularization_loss: 0.0000e+00 - val_loss: 0.7043 - val_auc: 0.4980 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 6/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6754 - auc: 0.5715 - regularization_loss: 0.0000e+00 - val_loss: 0.7102 - val_auc: 0.4980 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 13ms/step\u001b[0m\n",
      "\u001b[34mEpoch 7/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6723 - auc: 0.5751 - regularization_loss: 0.0000e+00 - val_loss: 0.7219 - val_auc: 0.4981 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 8/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6701 - auc: 0.5770 - regularization_loss: 0.0000e+00 - val_loss: 0.7366 - val_auc: 0.4984 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 13ms/step\u001b[0m\n",
      "\u001b[34mEpoch 9/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6688 - auc: 0.5787 - regularization_loss: 0.0000e+00 - val_loss: 0.7509 - val_auc: 0.4983 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 13ms/step\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m684/684 - 9s - loss: 0.6680 - auc: 0.5797 - regularization_loss: 0.0000e+00 - val_loss: 0.7519 - val_auc: 0.4984 - val_regularization_loss: 0.0000e+00 - 9s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34m/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Model saved to /tmp/tmp1nv79anb/dlrm.\u001b[0m\n",
      "\u001b[34mModel saved to /tmp/tmp1nv79anb/dlrm.\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Ensemble graph saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34mEnsemble graph saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34m2022-10-18 23:16:37,392 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-18 23:16:57 Uploading - Uploading generated training model\n",
      "2022-10-18 23:16:57 Completed - Training job completed\n",
      "Training seconds: 604\n",
      "Billable seconds: 604\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "training_instance_type = \"ml.g4dn.xlarge\"  # GPU instance, T4\n",
    "\n",
    "estimator = Estimator(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    image_uri=ecr_image,\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters={\n",
    "        \"batch_size\": 1_024,\n",
    "        \"epoch\": 10, \n",
    "    },\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    {\n",
    "        \"train\": f\"{data_location}/train/\",\n",
    "        \"valid\": f\"{data_location}/valid/\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44a96bd-a6ab-4ff7-9232-9e0f0c330ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d6d979-1976-46dd-bf88-669bbad39ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-23-05-14-440/output/model.tar.gz to ../../../../tmp/ensemble/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp {estimator.model_data} /tmp/ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90341dc-8c5a-4500-acf8-1491664cd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_predicttensorflow/\n",
      "1_predicttensorflow/1/\n",
      "1_predicttensorflow/1/model.savedmodel/\n",
      "1_predicttensorflow/1/model.savedmodel/saved_model.pb\n",
      "1_predicttensorflow/1/model.savedmodel/variables/\n",
      "1_predicttensorflow/1/model.savedmodel/variables/variables.index\n",
      "1_predicttensorflow/1/model.savedmodel/variables/variables.data-00000-of-00001\n",
      "1_predicttensorflow/1/model.savedmodel/keras_metadata.pb\n",
      "1_predicttensorflow/1/model.savedmodel/assets/\n",
      "1_predicttensorflow/config.pbtxt\n",
      "ensemble_model/\n",
      "ensemble_model/1/\n",
      "ensemble_model/config.pbtxt\n",
      "0_transformworkflow/\n",
      "0_transformworkflow/1/\n",
      "0_transformworkflow/1/model.py\n",
      "0_transformworkflow/1/workflow/\n",
      "0_transformworkflow/1/workflow/metadata.json\n",
      "0_transformworkflow/1/workflow/workflow.pkl\n",
      "0_transformworkflow/1/workflow/categories/\n",
      "0_transformworkflow/1/workflow/categories/unique.user_consumption_2.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_category.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_profile.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_id.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_shop.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_categories.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_shops.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_gender.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_geography.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_id.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_brands.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_age.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_group.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_intentions.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_brand.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_is_occupied.parquet\n",
      "0_transformworkflow/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "! tar xvzf /tmp/ensemble/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0c0a7-0815-4aa9-8536-5a2085406f7a",
   "metadata": {},
   "source": [
    "## Part 3: Retrieving Recommendations from Triton Inference Server\n",
    "\n",
    "Although we use the Sagemaker Python SDK to train our model, here we will use `boto3` to launch our inference endpoint as it offers more low-level control than the Python SDK.\n",
    "\n",
    "The model artificat `model.tar.gz` uploaded to S3 from the Sagemaker training job contained three directories: `0_transformworkflow` for the NVTabular workflow, `1_predicttensorflow` for the Tensorflow model, and `ensemble_model` for the ensemble graph that we can use in Triton.\n",
    "\n",
    "```shell\n",
    "/tmp/ensemble/\n",
    "├── 0_transformworkflow\n",
    "│   ├── 1\n",
    "│   │   ├── model.py\n",
    "│   │   └── workflow\n",
    "│   │       ├── categories\n",
    "│   │       │   ├── unique.item_brand.parquet\n",
    "│   │       │   ├── unique.item_category.parquet\n",
    "│   │       │   ├── unique.item_id.parquet\n",
    "│   │       │   ├── unique.item_shop.parquet\n",
    "│   │       │   ├── unique.user_age.parquet\n",
    "│   │       │   ├── unique.user_brands.parquet\n",
    "│   │       │   ├── unique.user_categories.parquet\n",
    "│   │       │   ├── unique.user_consumption_2.parquet\n",
    "│   │       │   ├── unique.user_gender.parquet\n",
    "│   │       │   ├── unique.user_geography.parquet\n",
    "│   │       │   ├── unique.user_group.parquet\n",
    "│   │       │   ├── unique.user_id.parquet\n",
    "│   │       │   ├── unique.user_intentions.parquet\n",
    "│   │       │   ├── unique.user_is_occupied.parquet\n",
    "│   │       │   ├── unique.user_profile.parquet\n",
    "│   │       │   └── unique.user_shops.parquet\n",
    "│   │       ├── metadata.json\n",
    "│   │       └── workflow.pkl\n",
    "│   └── config.pbtxt\n",
    "├── 1_predicttensorflow\n",
    "│   ├── 1\n",
    "│   │   └── model.savedmodel\n",
    "│   │       ├── assets\n",
    "│   │       ├── keras_metadata.pb\n",
    "│   │       ├── saved_model.pb\n",
    "│   │       └── variables\n",
    "│   │           ├── variables.data-00000-of-00001\n",
    "│   │           └── variables.index\n",
    "│   └── config.pbtxt\n",
    "├── ensemble_model\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "└── model.tar.gz\n",
    "```\n",
    "\n",
    "We specify that we only want to use `ensemble_model` in Triton by passing the environment variable `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aaa86d7-093e-4bdc-a606-a730c1bf9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:843263297212:model/model-triton-merlin-ensemble-2022-10-18-23-17-19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "container = {\n",
    "    \"Image\": ecr_image,\n",
    "    \"ModelDataUrl\": estimator.model_data,\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_TRITON_TENSORFLOW_VERSION\": \"2\",\n",
    "        \"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble_model\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name = \"model-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Model Arn: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97661e2-e54b-435c-96e3-63507418afe1",
   "metadata": {},
   "source": [
    "We again use the `g4dn` GPU instance that are equipped with NVIDIA T4 GPUs for launching the Triton inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cade896-0e92-42ff-b507-96a82a248814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint-config/endpoint-config-triton-merlin-ensemble-2022-10-18-23-17-20\n"
     ]
    }
   ],
   "source": [
    "endpoint_instance_type = \"ml.g4dn.xlarge\"\n",
    "\n",
    "endpoint_config_name = \"endpoint-config-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": endpoint_instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "endpoint_config_arn = create_endpoint_config_response[\"EndpointConfigArn\"]\n",
    "\n",
    "print(f\"Endpoint Config Arn: {endpoint_config_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004eb730-0eaa-4945-b8be-6c820ed96485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint/endpoint-triton-merlin-ensemble-2022-10-18-23-17-21\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"endpoint-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "endpoint_arn = create_endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "print(f\"Endpoint Arn: {endpoint_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd230b47-6a95-46ad-9555-ed8f1ee29be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: InService\n",
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint/endpoint-triton-merlin-ensemble-2022-10-18-23-17-21\n",
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint Creation Status: {status}\")\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    rv = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = rv[\"EndpointStatus\"]\n",
    "    print(f\"Endpoint Creation Status: {status}\")\n",
    "\n",
    "endpoint_arn = rv[\"EndpointArn\"]\n",
    "\n",
    "print(f\"Endpoint Arn: {endpoint_arn}\")\n",
    "print(f\"Endpoint Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb458d8-3169-40cc-8cae-6b72a300394c",
   "metadata": {},
   "source": [
    "### Send a Request to Triton Inference Server to Transform a Raw Dataset\n",
    "\n",
    "Once we have an endpoint running, we can test it by sending requests.\n",
    "Here, we use the raw validation set and transform it using the saved VTabular workflow we have downloaded from S3 in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9524fc1d-e613-4c24-b733-f7dcf673f584",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to dlopen libcuda.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3553\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:424\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda.cuPythonInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to dlopen libcuda.so"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3553, in cuda._cuda.ccuda._cuInit\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 424, in cuda._cuda.ccuda.cuPythonInit\n",
      "RuntimeError: Failed to dlopen libcuda.so\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Function \"cuDeviceGetCount\" not found\n",
      "  warnings.warn(str(e))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function \"cuInit\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cuda/_cuda/ccuda.pyx:3556\u001b[0m, in \u001b[0;36mcuda._cuda.ccuda._cuInit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function \"cuInit\" not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'cuda._lib.ccudart.utils.cudaPythonGlobal.lazyInit'\n",
      "Traceback (most recent call last):\n",
      "  File \"cuda/_cuda/ccuda.pyx\", line 3556, in cuda._cuda.ccuda._cuInit\n",
      "RuntimeError: Function \"cuInit\" not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user_id  item_id  item_category  item_shop  item_brand  \\\n",
      "__null_dask_index__                                                           \n",
      "700000                    23       23             66       4590        1581   \n",
      "700001                    11       10             27       1878         647   \n",
      "700002                    30       25             72       5007        1725   \n",
      "\n",
      "                     user_shops  user_profile  user_group  user_gender  \\\n",
      "__null_dask_index__                                                      \n",
      "700000                     1024             1           1            1   \n",
      "700001                      466             1           1            1   \n",
      "700002                     1349             2           1            1   \n",
      "\n",
      "                     user_age  user_consumption_2  user_is_occupied  \\\n",
      "__null_dask_index__                                                   \n",
      "700000                      1                   1                 1   \n",
      "700001                      1                   1                 1   \n",
      "700002                      1                   1                 1   \n",
      "\n",
      "                     user_geography  user_intentions  user_brands  \\\n",
      "__null_dask_index__                                                 \n",
      "700000                            1              297          509   \n",
      "700001                            1              135          232   \n",
      "700002                            1              391          671   \n",
      "\n",
      "                     user_categories  \n",
      "__null_dask_index__                   \n",
      "700000                            54  \n",
      "700001                            25  \n",
      "700002                            71  \n"
     ]
    }
   ],
   "source": [
    "from merlin.schema.tags import Tags\n",
    "from merlin.core.dispatch import get_lib\n",
    "from nvtabular.workflow import Workflow\n",
    "\n",
    "df_lib = get_lib()\n",
    "\n",
    "original_data_path = DATA_DIRECTORY\n",
    "workflow = Workflow.load(\"/tmp/ensemble/0_transformworkflow/1/workflow/\")\n",
    "\n",
    "label_columns = workflow.output_schema.select_by_tag(Tags.TARGET).column_names\n",
    "workflow.remove_inputs(label_columns)\n",
    "\n",
    "# read in data for request\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(original_data_path, \"valid\", \"part.0.parquet\"),\n",
    "    columns=workflow.input_schema.column_names\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d86d6-a17a-47e8-9966-8738f5bdcbd4",
   "metadata": {},
   "source": [
    "In the following code cell, we use a utility function provided in [Merlin Systems](https://github.com/NVIDIA-Merlin/systems) to convert our dataframe to the payload format that can be used as inference request format for Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dcae463-6d80-4a7a-981a-a9d034d49fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"inputs\":[{\"name\":\"user_id\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_id\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_category\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_shop\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_brand\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_shops\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_profile\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_group\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_gender\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_age\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_consumption_2\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_is_occupied\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_geography\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_intentions\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_brands\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_categories\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}}],\"parameters\":{\"binary_data_output\":true}}\\x17\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x19\\x00\\x00\\x00B\\x00\\x00\\x00\\x1b\\x00\\x00\\x00H\\x00\\x00\\x00\\xee\\x11\\x00\\x00V\\x07\\x00\\x00\\x8f\\x13\\x00\\x00-\\x06\\x00\\x00\\x87\\x02\\x00\\x00\\xbd\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\xd2\\x01\\x00\\x00E\\x05\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00)\\x01\\x00\\x00\\x87\\x00\\x00\\x00\\x87\\x01\\x00\\x00\\xfd\\x01\\x00\\x00\\xe8\\x00\\x00\\x00\\x9f\\x02\\x00\\x006\\x00\\x00\\x00\\x19\\x00\\x00\\x00G\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "inputs = convert_df_to_triton_input(workflow.input_schema, batch, httpclient.InferInput)\n",
    "\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(inputs)\n",
    "\n",
    "print(request_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdb596-0cd8-415c-bab5-f750a213a938",
   "metadata": {},
   "source": [
    "Triton uses the [KServe community standard inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md).\n",
    "Here, we use the [binary+json format](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) for optimal performance in the inference request.\n",
    "\n",
    "In order for Triton to correctly parse the binary payload, we have to specify the length of the request metadata in the header `json-header-size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2484510b-4b50-410a-8b93-93ef8c546cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5257045]\n",
      " [0.5127169]\n",
      " [0.4523193]]\n"
     ]
    }
   ],
   "source": [
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "    Body=request_body,\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "output_data = result.as_numpy(\"click/binary_classification_task\")\n",
    "print(\"predicted sigmoid result:\\n\", output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b660ca-6376-4615-aef4-474c66fa2bec",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts\n",
    "\n",
    "Don't forget to clean up artifacts and terminate the endpoint, or the endpoint will continue to incur costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce1630c-96aa-49ee-ac79-cf44120adb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a3731da7-55d9-49be-886d-9f77d550a312',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a3731da7-55d9-49be-886d-9f77d550a312',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 18 Oct 2022 23:24:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_model(ModelName=model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
