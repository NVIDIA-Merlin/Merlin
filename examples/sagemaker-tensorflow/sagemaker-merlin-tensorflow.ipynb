{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e73569d-99f1-4449-bfd1-8331fcde362d",
   "metadata": {},
   "source": [
    "# Training and Serving Merlin on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c864a8-5c09-44e7-9ae1-bc02c9b700e6",
   "metadata": {},
   "source": [
    "## Testing your algorithm on your local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be9695-161e-4b53-abf3-728995fc8c79",
   "metadata": {},
   "source": [
    "We use the synthetic train and test datasets generated by mimicking the real Ali-CCP: Alibaba Click and Conversion Prediction dataset to build our recommender system ranking models.\n",
    "\n",
    "If you would like to use real Ali-CCP dataset instead, you can download the training and test datasets on tianchi.aliyun.com. You can then use get_aliccp() function to curate the raw csv files and save them as parquet files.\n",
    "\n",
    "\n",
    "```python\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/aliccp-raw-synthetic\")\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 1000000)\n",
    "SYNTHETIC_DATA = eval(os.environ.get(\"SYNTHETIC_DATA\", \"True\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 512))\n",
    "\n",
    "if SYNTHETIC_DATA:\n",
    "    train, valid = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))\n",
    "    # save the datasets as parquet files\n",
    "    train.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"train\"))\n",
    "    valid.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"valid\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683bdac5-5d26-4e07-b23d-72a2a6a55e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"/workspace/data/aliccp-raw-synthetic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2ba473-90fc-4644-aea4-87e3c84eb405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  valid\n"
     ]
    }
   ],
   "source": [
    "! ls {DATA_DIRECTORY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60996955-1d6c-43e7-b29a-ad451b9aee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'container/train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! python3 container/train.py \\\n",
    "    --train_dir=/workspace/data/aliccp_raw_synthetic/train/ \\\n",
    "    --valid_dir=/workspace/data/aliccp_raw_synthetic/valid/ \\\n",
    "    --model_dir=/tmp/local_training/ \\\n",
    "    --batch_size=512 \\\n",
    "    --epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e6bb6-1d68-4797-a2b5-d87d164995cc",
   "metadata": {},
   "source": [
    "### How Amazon SageMaker runs your Docker container\n",
    "\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument `train` or `serve`. How your container processes this argument depends on the container.\n",
    "\n",
    "* In this example, we don't define a `ENTRYPOINT` in the `Dockerfile`, so Docker runs the command [`train` at training time](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html) and [`serve` at serving time](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html). In this example, we define these as executable Python scripts, but they could be any program that we want to start in that environment.\n",
    "* If you specify a program as a `ENTRYPOINT` in the `Dockerfile`, that program will be run at startup and its first argument will be `train` or `serve`. The program can then look at that argument and decide what to do.\n",
    "* If you are building separate containers for training and hosting (or building only for one or the other), you can define a program as a `ENTRYPOINT` in the `Dockerfile` and ignore (or verify) the first argument passed in.\n",
    "\n",
    "#### Running your container during training\n",
    "\n",
    "When Amazon SageMaker runs training, your `train` script is run, as in a regular Python program. A number of files are laid out for your use, under the `/opt/ml` directory:\n",
    "\n",
    "```\n",
    "    /opt/ml\n",
    "    |-- input\n",
    "    |   |-- config\n",
    "    |   |   |-- hyperparameters.json\n",
    "    |   |    -- resourceConfig.json\n",
    "    |    -- data\n",
    "    |        -- <channel_name>\n",
    "    |            -- <input data>\n",
    "    |-- model\n",
    "    |   -- <model files>\n",
    "     -- output\n",
    "        -- failure\n",
    "```\n",
    "\n",
    "##### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values are always strings, so you may need to convert them. `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to `CreateTrainingJob`, but it's generally important that channels match algorithm expectations. The files for each channel are copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.\n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "##### The output\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker packages any files in this directory into a compressed tar archive file. This file is made available at the S3 location returned to the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file are returned to the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it is ignored.\n",
    "\n",
    "#### Running your container during hosting\n",
    "\n",
    "Hosting has a very different model than training because hosting is responding to inference requests that come in via HTTP. In this example, we use [TensorFlow Serving](https://www.tensorflow.org/serving/), however the hosting solution can be customized. One example is the [Python serving stack within the `scikit learn` example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb).\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` receives `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these are passed in as well. \n",
    "\n",
    "The container has the model files in the same place that they were written to during training:\n",
    "\n",
    "    /opt/ml\n",
    "    `-- model\n",
    "        `-- <model files>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ba6bc5-f6ea-4d35-88f9-23683826e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      "\n",
      "RUN pip3 install sagemaker-training\n",
      "\n",
      "ENV SAGEMAKER_TRITON_TENSORFLOW_VERSION 2\n",
      "\n",
      "#EXPOSE 8080\n"
     ]
    }
   ],
   "source": [
    "! cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9afdca-19f6-45e6-8a73-3878c87a3577",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `build_and_push_image.sh`.\n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this is the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e40ffca-b651-413f-ac64-43f44a01e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "# The name of our algorithm\n",
      "ALGORITHM_NAME=sagemaker-merlin-tensorflow\n",
      "REGION=us-east-1\n",
      "\n",
      "cd container\n",
      "\n",
      "ACCOUNT=$(aws sts get-caller-identity --query Account --output text --region ${REGION})\n",
      "\n",
      "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
      "\n",
      "REPOSITORY=\"${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com\"\n",
      "IMAGE_URI=\"${REPOSITORY}/${ALGORITHM_NAME}:lastest\"\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin ${REPOSITORY}\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "\n",
      "aws ecr describe-repositories --repository-names \"${ALGORITHM_NAME}\" --region ${REGION} > /dev/null 2>&1\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    aws ecr create-repository --repository-name \"${ALGORITHM_NAME}\" --region ${REGION} > /dev/null\n",
      "fi\n",
      "\n",
      "# Build the docker image locally with the image name and then push it to ECR\n",
      "# with the full name.\n",
      "\n",
      "docker build  -t ${ALGORITHM_NAME} .\n",
      "docker tag ${ALGORITHM_NAME} ${IMAGE_URI}\n",
      "\n",
      "docker push ${IMAGE_URI}\n"
     ]
    }
   ],
   "source": [
    "! cat build_and_push_image.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea36f4c-8fe3-44f4-a340-91923c7f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b96348-dd30-4691-8beb-9723d67afee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = \"DEMO-merlin-tensorflow-aliccp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40105894-6aa2-4346-a912-21cbd5881f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sess.upload_data(DATA_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f67ed27-a0ea-4a8f-b4f7-c0457c1d90a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-843263297212/DEMO-merlin-tensorflow-aliccp\n"
     ]
    }
   ],
   "source": [
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dbb12-76fb-41c0-ad90-32b7d1328e62",
   "metadata": {},
   "source": [
    "## Making predictions using Sagemaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0e7947-60d0-4130-8819-021118a8ade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AWSOS-AD-Engineer to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::843263297212:role/AWSOS-AD-Engineer\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b9fc54-bafa-4487-8f34-2f9448c0b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843263297212.dkr.ecr.us-east-1.amazonaws.com/sagemaker-merlin-tensorflow:testing\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "algorithm_name = \"sagemaker-merlin-tensorflow\"\n",
    "\n",
    "ecr_image = \"{}.dkr.ecr.{}.amazonaws.com/{}:testing\".format(account, region, algorithm_name)\n",
    "\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f238c8c6-656e-4b3d-96c6-32654357fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 08:15:09 Starting - Starting the training job...\n",
      "2022-10-18 08:15:34 Starting - Preparing the instances for trainingProfilerReport-1666080908: InProgress\n",
      ".........\n",
      "2022-10-18 08:17:14 Downloading - Downloading input data...\n",
      "2022-10-18 08:17:34 Training - Downloading the training image....................................\n",
      "2022-10-18 08:23:56 Training - Training image download completed. Training in progress.\u001b[34m==================================\u001b[0m\n",
      "\u001b[34m== Triton Inference Server Base ==\u001b[0m\n",
      "\u001b[34m==================================\u001b[0m\n",
      "\u001b[34mNVIDIA Release 22.08 (build 42766143)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34mNOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.7 driver version 515.65.01 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\u001b[0m\n",
      "\u001b[34m2022-10-18 08:23:55,037 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 1024,\n",
      "        \"epoch\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":1024,\"epoch\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":1024,\"epoch\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"1024\",\"--epoch\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=1024\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/opt/tritonserver:/usr/local/lib/python3.8/dist-packages:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages/faiss-1.7.2-py3.8.egg:/usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg:/usr/local/lib/python3.8/dist-packages/merlin_hps-1.0.0-py3.8-linux-x86_64.egg:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --batch_size 1024 --epoch 10\u001b[0m\n",
      "\u001b[34m2022-10-18 08:23:55,040 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:00.671181: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.831761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.834133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.834364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.871795: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.873283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.873588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:05.873804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:10.009269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:10.009512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:10.009744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2022-10-18 08:24:10.010855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10752 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\u001b[0m\n",
      "\u001b[34mWorkflow saved to /tmp/tmpg2ywr9hs/workflow.\u001b[0m\n",
      "\u001b[34mbatch_size = 1024, epochs = 10\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m684/684 - 17s - loss: 0.6932 - auc: 0.4995 - regularization_loss: 0.0000e+00 - val_loss: 0.6931 - val_auc: 0.5000 - val_regularization_loss: 0.0000e+00 - 17s/epoch - 24ms/step\u001b[0m\n",
      "\u001b[34mEpoch 2/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6932 - auc: 0.5014 - regularization_loss: 0.0000e+00 - val_loss: 0.6932 - val_auc: 0.5008 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 3/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6928 - auc: 0.5151 - regularization_loss: 0.0000e+00 - val_loss: 0.6939 - val_auc: 0.4994 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 4/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6877 - auc: 0.5450 - regularization_loss: 0.0000e+00 - val_loss: 0.6972 - val_auc: 0.4990 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 5/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6804 - auc: 0.5634 - regularization_loss: 0.0000e+00 - val_loss: 0.7037 - val_auc: 0.4988 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 6/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6759 - auc: 0.5716 - regularization_loss: 0.0000e+00 - val_loss: 0.7075 - val_auc: 0.4987 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 7/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6729 - auc: 0.5753 - regularization_loss: 0.0000e+00 - val_loss: 0.7195 - val_auc: 0.4986 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 8/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6705 - auc: 0.5777 - regularization_loss: 0.0000e+00 - val_loss: 0.7267 - val_auc: 0.4987 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 9/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6691 - auc: 0.5793 - regularization_loss: 0.0000e+00 - val_loss: 0.7468 - val_auc: 0.4990 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m684/684 - 10s - loss: 0.6682 - auc: 0.5802 - regularization_loss: 0.0000e+00 - val_loss: 0.7526 - val_auc: 0.4988 - val_regularization_loss: 0.0000e+00 - 10s/epoch - 14ms/step\u001b[0m\n",
      "\u001b[34m/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Model saved to /tmp/tmpg2ywr9hs/dlrm.\u001b[0m\n",
      "\u001b[34mModel saved to /tmp/tmpg2ywr9hs/dlrm.\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mWARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\u001b[0m\n",
      "\n",
      "2022-10-18 08:26:52 Uploading - Uploading generated training model\u001b[34mINFO:__main__:Ensemble graph saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34mEnsemble graph saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34m2022-10-18 08:26:43,226 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-18 08:27:16 Completed - Training job completed\n",
      "ProfilerReport-1666080908: IssuesFound\n",
      "Training seconds: 590\n",
      "Billable seconds: 590\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "training_instance_type = \"ml.g4dn.xlarge\"  # GPU instance, T4\n",
    "\n",
    "estimator = Estimator(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    image_uri=ecr_image,\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters={\n",
    "        \"batch_size\": 1_024,\n",
    "        \"epoch\": 10, \n",
    "    },\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    {\n",
    "        \"train\": f\"{data_location}/train/\",\n",
    "        \"valid\": f\"{data_location}/valid/\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44a96bd-a6ab-4ff7-9232-9e0f0c330ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5d6d979-1976-46dd-bf88-669bbad39ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-843263297212/sagemaker-merlin-tensorflow-2022-10-18-08-15-06-593/output/model.tar.gz to ../../../../tmp/ensemble/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp {estimator.model_data} /tmp/ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90341dc-8c5a-4500-acf8-1491664cd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble_model/\n",
      "ensemble_model/1/\n",
      "ensemble_model/config.pbtxt\n",
      "1_predicttensorflow/\n",
      "1_predicttensorflow/1/\n",
      "1_predicttensorflow/1/model.savedmodel/\n",
      "1_predicttensorflow/1/model.savedmodel/keras_metadata.pb\n",
      "1_predicttensorflow/1/model.savedmodel/assets/\n",
      "1_predicttensorflow/1/model.savedmodel/variables/\n",
      "1_predicttensorflow/1/model.savedmodel/variables/variables.data-00000-of-00001\n",
      "1_predicttensorflow/1/model.savedmodel/variables/variables.index\n",
      "1_predicttensorflow/1/model.savedmodel/saved_model.pb\n",
      "1_predicttensorflow/config.pbtxt\n",
      "0_transformworkflow/\n",
      "0_transformworkflow/1/\n",
      "0_transformworkflow/1/model.py\n",
      "0_transformworkflow/1/workflow/\n",
      "0_transformworkflow/1/workflow/categories/\n",
      "0_transformworkflow/1/workflow/categories/unique.item_shop.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_category.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_categories.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_brand.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_brands.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_geography.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_age.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_gender.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_intentions.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_id.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_is_occupied.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_shops.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_group.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.item_id.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_consumption_2.parquet\n",
      "0_transformworkflow/1/workflow/categories/unique.user_profile.parquet\n",
      "0_transformworkflow/1/workflow/metadata.json\n",
      "0_transformworkflow/1/workflow/workflow.pkl\n",
      "0_transformworkflow/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "! tar xvzf /tmp/ensemble/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0c0a7-0815-4aa9-8536-5a2085406f7a",
   "metadata": {},
   "source": [
    "## Retrieving Recommendations from Triton Inference Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aaa86d7-093e-4bdc-a606-a730c1bf9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:843263297212:model/model-triton-merlin-ensemble-2022-10-18-08-27-45\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "container = {\n",
    "    \"Image\": ecr_image,\n",
    "    \"ModelDataUrl\": estimator.model_data,\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble_model\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name = \"model-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Model Arn: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cade896-0e92-42ff-b507-96a82a248814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint-config/endpoint-config-triton-merlin-ensemble-2022-10-18-08-27-46\n"
     ]
    }
   ],
   "source": [
    "endpoint_instance_type = \"ml.g4dn.xlarge\"\n",
    "endpoint_config_name = \"endpoint-config-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": endpoint_instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "endpoint_config_arn = create_endpoint_config_response[\"EndpointConfigArn\"]\n",
    "\n",
    "print(f\"Endpoint Config Arn: {endpoint_config_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "004eb730-0eaa-4945-b8be-6c820ed96485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint/endpoint-triton-merlin-ensemble-2022-10-18-08-27-46\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"endpoint-triton-merlin-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "endpoint_arn = create_endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "print(f\"Endpoint Arn: {endpoint_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd230b47-6a95-46ad-9555-ed8f1ee29be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: Creating\n",
      "Endpoint Creation Status: InService\n",
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:843263297212:endpoint/endpoint-triton-merlin-ensemble-2022-10-18-08-27-46\n",
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint Creation Status: {status}\")\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    rv = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = rv[\"EndpointStatus\"]\n",
    "    print(f\"Endpoint Creation Status: {status}\")\n",
    "\n",
    "endpoint_arn = rv[\"EndpointArn\"]\n",
    "\n",
    "print(f\"Endpoint Arn: {endpoint_arn}\")\n",
    "print(f\"Endpoint Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9524fc1d-e613-4c24-b733-f7dcf673f584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user_id  item_id  item_category  item_shop  item_brand  \\\n",
      "__null_dask_index__                                                           \n",
      "700000                    23       23             66       4590        1581   \n",
      "700001                    11       10             27       1878         647   \n",
      "700002                    30       25             72       5007        1725   \n",
      "\n",
      "                     user_shops  user_profile  user_group  user_gender  \\\n",
      "__null_dask_index__                                                      \n",
      "700000                     1024             1           1            1   \n",
      "700001                      466             1           1            1   \n",
      "700002                     1349             2           1            1   \n",
      "\n",
      "                     user_age  user_consumption_2  user_is_occupied  \\\n",
      "__null_dask_index__                                                   \n",
      "700000                      1                   1                 1   \n",
      "700001                      1                   1                 1   \n",
      "700002                      1                   1                 1   \n",
      "\n",
      "                     user_geography  user_intentions  user_brands  \\\n",
      "__null_dask_index__                                                 \n",
      "700000                            1              297          509   \n",
      "700001                            1              135          232   \n",
      "700002                            1              391          671   \n",
      "\n",
      "                     user_categories  \n",
      "__null_dask_index__                   \n",
      "700000                            54  \n",
      "700001                            25  \n",
      "700002                            71  \n"
     ]
    }
   ],
   "source": [
    "from merlin.schema.tags import Tags\n",
    "from merlin.core.dispatch import get_lib\n",
    "from nvtabular.workflow import Workflow\n",
    "\n",
    "df_lib = get_lib()\n",
    "\n",
    "original_data_path = DATA_DIRECTORY\n",
    "workflow = Workflow.load(\"/tmp/ensemble/0_transformworkflow/1/workflow/\")\n",
    "\n",
    "label_columns = workflow.output_schema.select_by_tag(Tags.TARGET).column_names\n",
    "workflow.remove_inputs(label_columns)\n",
    "\n",
    "# read in data for request\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(original_data_path, \"valid\", \"part.0.parquet\"),\n",
    "    columns=workflow.input_schema.column_names\n",
    ")[:3]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dcae463-6d80-4a7a-981a-a9d034d49fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"inputs\":[{\"name\":\"user_id\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_id\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_category\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_shop\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"item_brand\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_shops\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_profile\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_group\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_gender\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_age\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_consumption_2\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_is_occupied\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_geography\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_intentions\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_brands\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"user_categories\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}}],\"parameters\":{\"binary_data_output\":true}}\\x17\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x19\\x00\\x00\\x00B\\x00\\x00\\x00\\x1b\\x00\\x00\\x00H\\x00\\x00\\x00\\xee\\x11\\x00\\x00V\\x07\\x00\\x00\\x8f\\x13\\x00\\x00-\\x06\\x00\\x00\\x87\\x02\\x00\\x00\\xbd\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\xd2\\x01\\x00\\x00E\\x05\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00)\\x01\\x00\\x00\\x87\\x00\\x00\\x00\\x87\\x01\\x00\\x00\\xfd\\x01\\x00\\x00\\xe8\\x00\\x00\\x00\\x9f\\x02\\x00\\x006\\x00\\x00\\x00\\x19\\x00\\x00\\x00G\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "\n",
    "inputs = convert_df_to_triton_input(workflow.input_schema, batch, httpclient.InferInput)\n",
    "\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(inputs)\n",
    "\n",
    "print(request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2484510b-4b50-410a-8b93-93ef8c546cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5311371 ]\n",
      " [0.51526797]\n",
      " [0.4596776 ]]\n"
     ]
    }
   ],
   "source": [
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "    Body=request_body,\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "output_data = result.as_numpy(\"click/binary_classification_task\")\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b660ca-6376-4615-aef4-474c66fa2bec",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ce1630c-96aa-49ee-ac79-cf44120adb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4c4d2158-9744-4eff-88ee-725e66bd12e9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4c4d2158-9744-4eff-88ee-725e66bd12e9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 18 Oct 2022 08:36:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_model(ModelName=model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583a6a9-4f23-4bc4-a52b-822a89809b0c",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Polish notebook.\n",
    "- Hyperparameter tuning: Sagemaker parases the model training logs using regular expressinos to extract the metrics. Write a regex for parsing logs.\n",
    "- Sagemaker feature store\n",
    "- kNN using sagemaker algorithms\n",
    "- Distributed/multi-GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4cfc5-eb00-46bc-a4ce-0a1b9ccd5a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
