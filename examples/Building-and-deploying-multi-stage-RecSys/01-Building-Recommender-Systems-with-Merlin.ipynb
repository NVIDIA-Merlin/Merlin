{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b5cc0-2110-464e-9773-003ffe7d216c",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Building Intelligent Recommender Systems with Merlin\n",
    "\n",
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9657308-2e08-49b4-8924-eace75a4634c",
   "metadata": {},
   "source": [
    "Recommender Systems (RecSys) are the engine of the modern internet and the catalyst for human decisions. Building a recommendation system is challenging because it requires multiple stages (data preprocessing, offline training, item retrieval, filtering, ranking, ordering, etc.) to work together seamlessly and efficiently. The biggest challenges for new practitioners are the lack of understanding around what RecSys look like in the real world, and the gap between examples of simple models and a production-ready end-to-end recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405280b0-3d48-43b6-ab95-d29be7a43e9e",
   "metadata": {},
   "source": [
    "The figure below represents a four-stage recommender systems. This is more complex process than only training a single model and deploying it, and it is much more realistic and closer to what's happening in the real-world recommender production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27220153",
   "metadata": {},
   "source": [
    "![fourstage](../images/fourstages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ffed1-4b4b-4b6f-b933-31e9f6c1b4e1",
   "metadata": {},
   "source": [
    "In these series of notebooks, we are going to showcase how we can deploy a four-stage recommender systems using Merlin Systems library easily on [Triton Inference Server](https://github.com/triton-inference-server/server). Let's go over the concepts in the figure briefly. \n",
    "- **Retrieval:** This is the step to narrow down millions of items into thounds of candidates. We are going to train a Two-Tower item retrieval model to retrive the relevant top-K candidate items.\n",
    "- **Filtering:** This step is to exclude the already interacted  or undesirable items from the candidate items set or to apply business logic rules. Although this is an important step, for this example we skip this step.\n",
    "- **Scoring:** This is also known as ranking. Here the retrieved and filtered candidate items are being scored. We are going to train a ranking model to be able to use at our scoring step. \n",
    "- **Ordering:** At this stage, we can order the final set of items that we want to recommend to the user. Here, we’re able to align the output of the model with business needs, constraints, or criteria.\n",
    "\n",
    "To learn more about the four-stage recommender systems, you can listen to Even Oldridge's [Moving Beyond Recommender Models talk](https://www.youtube.com/watch?v=5qjiY-kLwFY&list=PL65MqKWg6XcrdN4TJV0K1PdLhF_Uq-b43&index=7) at KDD'21 and read more [in this blog post](https://eugeneyan.com/writing/system-design-for-discovery/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f3194-9f17-4fa7-8baa-14333f2a122a",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "- Understanding four stages of recommender systems\n",
    "- Training retrieval and ranking models with Merlin Models\n",
    "- Setting up feature store and approximate nearest neighbours (ANN) search libraries\n",
    "- Deploying trained models to Triton Inference Server with Merlin Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8bd1f-fa29-4d4b-a320-c76538f2302f",
   "metadata": {},
   "source": [
    "In additon to NVIDIA Merlin libraries and `Triton` library, we are using two external libraries in these series of examples:\n",
    "\n",
    "- [Feast](https://docs.feast.dev/): an end-to-end open source feature store library for machine learning\n",
    "- [Faiss](https://github.com/facebookresearch/faiss): a library for efficient similarity search and clustering of dense vectors\n",
    "\n",
    "You can find more information about `Feast feature store` and `Faiss` libraries in the next notebook. Please follow the instructions in the README.md file to install these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7f3bd",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1586d8-e5a6-40c3-b6bb-61a3e62fa34c",
   "metadata": {},
   "source": [
    "**Compatibility:**\n",
    "\n",
    "These notebooks are developed and tested using our latest inference container on [NVIDIA's docker registry](https://catalog.ngc.nvidia.com/containers?filters=&orderBy=dateModifiedDESC&query=merlin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd8cc8d-5cc7-4a9f-91e5-3deec6f1fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow \"feast<0.20\" faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cdbfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 19:18:30.785739: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-26 19:18:31.885961: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-04-26 19:18:31.886097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import cudf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028a1398-76a8-4998-97d8-34a806e130d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad8ae3",
   "metadata": {},
   "source": [
    "In this example notebook, we will generate the synthetic train and test datasets mimicking the real [Ali-CCP: Alibaba Click and Conversion Prediction](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1) dataset to build our recommender system models.\n",
    "\n",
    "First, we define our input and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ddb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "output_path = os.path.join(DATA_FOLDER, 'processed/ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a746a3f-1845-4af3-8a37-1b34aa1bb81b",
   "metadata": {},
   "source": [
    "Then, we use `generate_data` utility function to generate synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44b3378-7297-4946-a271-742a9239bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "train = generate_data(\"aliccp-raw\", 10000000)\n",
    "valid = generate_data(\"aliccp-raw\", 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bae6de-0345-4963-8f73-3ae234e54040",
   "metadata": {},
   "source": [
    "If you would like to use the real ALI-CCP dataset, you can use [get_aliccp()](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/datasets/ecommerce/aliccp/dataset.py) function instead. This function takes the raw csv files, and generate parquet files that can be directly fed to NVTabular workflow above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bca46-a6b6-4a73-afd8-fe2869c60748",
   "metadata": {},
   "source": [
    "### Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b09cc-09fb-4814-a1cb-7e6168d9eb4b",
   "metadata": {},
   "source": [
    "We are going to process our raw categorical features by encoding them using `Categorify()` operator and tag the features with `user` or `item` tags in the schema file. To learn more about [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) and the schema object visit this example [notebook](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb) in the Merlin Models repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550d45c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 µs, sys: 29 µs, total: 201 µs\n",
      "Wall time: 204 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user_id = [\"user_id\"] >> Categorify(dtype='int32') >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(dtype='int32') >> TagAsItemID()\n",
    "\n",
    "item_features = [\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify(dtype='int32') >> TagAsItemFeatures() \n",
    "\n",
    "user_features = ['user_shops', 'user_profile', 'user_group', \n",
    "       'user_gender', 'user_age', 'user_consumption_2', 'user_is_occupied',\n",
    "       'user_geography', 'user_intentions', 'user_brands', 'user_categories'] \\\n",
    "    >> Categorify(dtype='int32') >> TagAsUserFeatures() \n",
    "\n",
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "\n",
    "outputs = user_id+item_id+item_features+user_features+targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19550f-49db-48a3-83c6-aad7d348673c",
   "metadata": {},
   "source": [
    "Let's call `transform_aliccp` utility function to be able to perform `fit` and `transform` steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e117e7b5-5007-424b-8d3f-9e1db245fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.datasets.ecommerce import transform_aliccp\n",
    "\n",
    "transform_aliccp((train, valid), output_path, nvt_workflow=outputs, workflow_name='workflow_ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16401d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training a Ranking Model with DLRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b234",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file, `schema.pbtxt` a protobuf text file, of our processed dataset. To learn more about the schema object and schema file you can explore [02-Merlin-Models-and-NVTabular-integration.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb) notebook.\n",
    "\n",
    "We use the `schema` object to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb870461-6ac2-49b2-ba6a-2da6ecb57f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(output_path, 'train', '*.parquet'), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, 'valid', '*.parquet'), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e4ebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68e26b",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture is a popular neural network model originally proposed by Facebook in 2019. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074). To learn more about DLRM architetcture please visit `Exploring-different-models` [notebook](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb) in the Merlin Models GH repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4325080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe2aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 19:18:47.296000: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/611 [============================>.] - ETA: 0s - auc: 0.4999 - loss: 0.6932 - regularization_loss: 0.0000e+00 - total_loss: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 19:19:38.592641: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/branch_executed/_13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 50s 38ms/step - auc: 0.4999 - loss: 0.6932 - regularization_loss: 0.0000e+00 - total_loss: 0.6932 - val_auc: 0.5000 - val_loss: 0.6932 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcefc04f730>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=16*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d91780-88d3-4dd8-9ed2-db33424d3e98",
   "metadata": {},
   "source": [
    "Let's save our DLRM model to be able to load back at the deployment stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd78a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dlrm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91205a3c-f46e-45a0-b668-1a9bdef0c51d",
   "metadata": {},
   "source": [
    "### Training a Retrieval Model with Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e5dbf-f648-4667-8dc3-47feef88d3f1",
   "metadata": {},
   "source": [
    "Now we move to the offline retrieval stage. We are going to train a Two-Tower model for item retrieval. To learn more about the Two-tower model you can visit [05-Retrieval-Model.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/05-Retrieval-Model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00de24e9-331a-486e-9843-6c554ad2ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_FOLDER, 'processed/retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaec18-84f2-42f6-bee5-d66cf28c03f4",
   "metadata": {},
   "source": [
    "We select only positive interaction rows where `click==1` in the dataset with `Filter()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a7d605-478f-40e6-a5dc-3e7a61e9b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_id = [\"user_id\"] >> Categorify(dtype='int32') >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(dtype='int32') >> TagAsItemID()\n",
    "\n",
    "item_features = [\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify(dtype='int32') >> TagAsItemFeatures()\n",
    "\n",
    "user_features = ['user_shops', 'user_profile', 'user_group', \n",
    "       'user_gender', 'user_age', 'user_consumption_2', 'user_is_occupied',\n",
    "       'user_geography', 'user_intentions', 'user_brands', 'user_categories'] \\\n",
    "        >> Categorify(dtype='int32') >> TagAsUserFeatures() \n",
    "\n",
    "inputs = user_id + item_id + item_features + user_features + ['click'] \n",
    "\n",
    "outputs = inputs >> Filter(f=lambda df: df[\"click\"] == 1)\n",
    "\n",
    "transform_aliccp((train, valid), output_path, nvt_workflow=outputs, workflow_name='workflow_retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc150549-6fa0-441f-939d-a358e56d5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tt = Dataset(os.path.join(output_path, 'train', '*.parquet'))\n",
    "valid_tt = Dataset(os.path.join(output_path, 'valid', '*.parquet'))\n",
    "\n",
    "schema = train_tt.schema\n",
    "schema = schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02471088-0ed8-42e7-968e-b7e68865d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),        \n",
    "    loss=\"categorical_crossentropy\",  \n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options = mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6703d7c-d38f-4d6d-a20a-9ee95ff1e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/611 [============================>.] - ETA: 0s - recall_at_10: 0.0344 - ndcg_10: 0.0334 - loss: 8.6966 - regularization_loss: 0.0000e+00 - total_loss: 8.6966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 19:20:33.534489: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/branch_executed/_24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 43s 61ms/step - recall_at_10: 0.0344 - ndcg_10: 0.0334 - loss: 8.6942 - regularization_loss: 0.0000e+00 - total_loss: 8.6942 - val_recall_at_10: 0.0345 - val_ndcg_10: 0.0342 - val_loss: 6.4634 - val_regularization_loss: 0.0000e+00 - val_total_loss: 6.4634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcea166eb50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', run_eagerly=False)\n",
    "model.fit(train_tt, validation_data=valid_tt, batch_size=1024*8, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a3f3f-81d8-489c-835f-c62f76df22d5",
   "metadata": {},
   "source": [
    "In the following cells we are going to export the required user and item features files, and save the query (user) tower model and item embeddings to disk. If you want to read more about exporting retrieval models, please visit [05-Retrieval-Model.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/05-Retrieval-Model.ipynb) notebook in Merlin Models library repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f434-f5a1-4478-b588-7e7ec17e6a88",
   "metadata": {},
   "source": [
    "### Set up a feature store with Feast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4e939-d3cf-44f0-9012-d2af3264ee25",
   "metadata": {},
   "source": [
    "Before we move onto the next step, we need to create a Feast feature repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa00071a-f840-47f9-8734-75ed2c99eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the base dir to for feature store\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/Merlin/examples/Deploying-multi-stage-RecSys/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301ef08-d5e0-4912-8e79-3cd03b83f7bf",
   "metadata": {},
   "source": [
    "This will create the feature repo in the current working directory, which is `BASE_DIR` for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7e96d2-9cd2-40d1-b356-8cd76b57bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!feast init feature_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e630e53-8336-487a-9ceb-133b1538acfb",
   "metadata": {},
   "source": [
    "You should be seeing a message like <i>Creating a new Feast repository in ... </i> printed out above. Now, navigate to the `feature_repo` folder and remove the demo parquet file created by default, and `examples.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ba2521-ed1b-4c2b-afdd-26b4a5a9c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(os.path.join(BASE_DIR, 'feature_repo', 'example.py'))\n",
    "os.remove(os.path.join(BASE_DIR, 'feature_repo/data', 'driver_stats.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc89d-170b-41a1-a29b-5f958ed31399",
   "metadata": {},
   "source": [
    "### Exporting query (user) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af24597-e89c-43a4-9a13-458d8bed7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tower = model.retrieval_block.query_block()\n",
    "query_tower.save('query_tower')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78315676-eb6c-405a-b1fd-3174ea328406",
   "metadata": {},
   "source": [
    "### Exporting user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea0b369c-2f01-42e3-9f3c-74c3ff4a6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "user_features = unique_rows_by_features(train, Tags.USER, Tags.USER_ID).compute().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b0949f9-e67a-414f-9d74-65f138e820a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_shops</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>user_group</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_consumption_2</th>\n",
       "      <th>user_is_occupied</th>\n",
       "      <th>user_geography</th>\n",
       "      <th>user_intentions</th>\n",
       "      <th>user_brands</th>\n",
       "      <th>user_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_shops  user_profile  user_group  user_gender  user_age  \\\n",
       "0        1           1             1           1            1         1   \n",
       "1        2           2             1           1            1         1   \n",
       "2        3           3             1           1            1         1   \n",
       "3        4           4             1           1            1         1   \n",
       "4        5           5             1           1            1         1   \n",
       "\n",
       "   user_consumption_2  user_is_occupied  user_geography  user_intentions  \\\n",
       "0                   1                 1               1                1   \n",
       "1                   1                 1               1                2   \n",
       "2                   1                 1               1                3   \n",
       "3                   1                 1               1                4   \n",
       "4                   1                 1               1                5   \n",
       "\n",
       "   user_brands  user_categories  \n",
       "0            1                1  \n",
       "1            2                2  \n",
       "2            3                3  \n",
       "3            4                4  \n",
       "4            5                5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46bd8c-1337-4c74-a85b-25348a897d90",
   "metadata": {},
   "source": [
    "We will artificially add `datetime` and `created` timestamp columns to our user_features dataframe. This required by Feast to track the user-item features and their creation time and to determine which version to use when we query Feast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30bd2f8-8a78-4df7-9bc4-42bd741c5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "user_features[\"datetime\"] = datetime.now()\n",
    "user_features[\"datetime\"] = user_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "user_features[\"created\"] = datetime.now()\n",
    "user_features[\"created\"] = user_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4998cd1-9dcd-4911-8f23-372e197b41e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_shops</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>user_group</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_consumption_2</th>\n",
       "      <th>user_is_occupied</th>\n",
       "      <th>user_geography</th>\n",
       "      <th>user_intentions</th>\n",
       "      <th>user_brands</th>\n",
       "      <th>user_categories</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-26 19:20:41.830940</td>\n",
       "      <td>2022-04-26 19:20:42.199991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-26 19:20:41.830940</td>\n",
       "      <td>2022-04-26 19:20:42.199991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-26 19:20:41.830940</td>\n",
       "      <td>2022-04-26 19:20:42.199991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-04-26 19:20:41.830940</td>\n",
       "      <td>2022-04-26 19:20:42.199991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-04-26 19:20:41.830940</td>\n",
       "      <td>2022-04-26 19:20:42.199991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_shops  user_profile  user_group  user_gender  user_age  \\\n",
       "0        1           1             1           1            1         1   \n",
       "1        2           2             1           1            1         1   \n",
       "2        3           3             1           1            1         1   \n",
       "3        4           4             1           1            1         1   \n",
       "4        5           5             1           1            1         1   \n",
       "\n",
       "   user_consumption_2  user_is_occupied  user_geography  user_intentions  \\\n",
       "0                   1                 1               1                1   \n",
       "1                   1                 1               1                2   \n",
       "2                   1                 1               1                3   \n",
       "3                   1                 1               1                4   \n",
       "4                   1                 1               1                5   \n",
       "\n",
       "   user_brands  user_categories                   datetime  \\\n",
       "0            1                1 2022-04-26 19:20:41.830940   \n",
       "1            2                2 2022-04-26 19:20:41.830940   \n",
       "2            3                3 2022-04-26 19:20:41.830940   \n",
       "3            4                4 2022-04-26 19:20:41.830940   \n",
       "4            5                5 2022-04-26 19:20:41.830940   \n",
       "\n",
       "                     created  \n",
       "0 2022-04-26 19:20:42.199991  \n",
       "1 2022-04-26 19:20:42.199991  \n",
       "2 2022-04-26 19:20:42.199991  \n",
       "3 2022-04-26 19:20:42.199991  \n",
       "4 2022-04-26 19:20:42.199991  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2981b3ed-6156-49f0-aa14-326a3853a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.to_parquet(os.path.join(BASE_DIR, 'feature_repo/data', 'user_features.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a33a668-8e2a-4546-8f54-0060d405ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = unique_rows_by_features(train, Tags.ITEM, Tags.ITEM_ID).compute().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97189581-473c-4928-8be7-ec31b86d69ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1331, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68a694d6-926f-4b0f-8edc-8cc7ac85ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features[\"datetime\"] = datetime.now()\n",
    "item_features[\"datetime\"] = item_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "item_features[\"created\"] = datetime.now()\n",
    "item_features[\"created\"] = item_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c03fa22-b112-4243-bbe1-1cd7260cb85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category</th>\n",
       "      <th>item_shop</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-26 19:20:42.415442</td>\n",
       "      <td>2022-04-26 19:20:42.417166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-26 19:20:42.415442</td>\n",
       "      <td>2022-04-26 19:20:42.417166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-26 19:20:42.415442</td>\n",
       "      <td>2022-04-26 19:20:42.417166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-04-26 19:20:42.415442</td>\n",
       "      <td>2022-04-26 19:20:42.417166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-04-26 19:20:42.415442</td>\n",
       "      <td>2022-04-26 19:20:42.417166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  item_category  item_shop  item_brand                   datetime  \\\n",
       "0        1              1          1           1 2022-04-26 19:20:42.415442   \n",
       "1        2              2          2           2 2022-04-26 19:20:42.415442   \n",
       "2        3              3          3           3 2022-04-26 19:20:42.415442   \n",
       "3        4              4          4           4 2022-04-26 19:20:42.415442   \n",
       "4        5              5          5           5 2022-04-26 19:20:42.415442   \n",
       "\n",
       "                     created  \n",
       "0 2022-04-26 19:20:42.417166  \n",
       "1 2022-04-26 19:20:42.417166  \n",
       "2 2022-04-26 19:20:42.417166  \n",
       "3 2022-04-26 19:20:42.417166  \n",
       "4 2022-04-26 19:20:42.417166  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c312884b-a1f8-4e08-8068-696e06a9bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(os.path.join(BASE_DIR, 'feature_repo/data', 'item_features.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30ceab-b264-4509-9c5b-5a10425e143b",
   "metadata": {},
   "source": [
    "### Extract and save Item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00f1fe65-882e-4962-bb16-19a130fda215",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embs = model.item_embeddings(Dataset(item_features, schema=schema), batch_size=1024)\n",
    "item_embs_df = item_embs.compute(scheduler=\"synchronous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf8b82ea-6cce-4dab-ad17-114b5e7eabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only item_id together with embedding columns \n",
    "item_embeddings = item_embs_df.drop(columns=['item_category', 'item_shop', 'item_brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e02f0957-6665-400a-80c0-60b307466caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.480469</td>\n",
       "      <td>-0.313522</td>\n",
       "      <td>0.448558</td>\n",
       "      <td>-0.265470</td>\n",
       "      <td>-0.448500</td>\n",
       "      <td>-0.408890</td>\n",
       "      <td>-0.519680</td>\n",
       "      <td>0.478366</td>\n",
       "      <td>-0.219287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274009</td>\n",
       "      <td>-0.514024</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.395779</td>\n",
       "      <td>-0.203225</td>\n",
       "      <td>0.453499</td>\n",
       "      <td>-0.461412</td>\n",
       "      <td>0.655305</td>\n",
       "      <td>-0.193951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.011039</td>\n",
       "      <td>-0.120592</td>\n",
       "      <td>-0.145036</td>\n",
       "      <td>0.028828</td>\n",
       "      <td>0.129078</td>\n",
       "      <td>0.045131</td>\n",
       "      <td>0.080241</td>\n",
       "      <td>-0.115238</td>\n",
       "      <td>0.071108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091944</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>-0.144494</td>\n",
       "      <td>-0.145458</td>\n",
       "      <td>-0.045634</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>-0.114459</td>\n",
       "      <td>0.092429</td>\n",
       "      <td>-0.158437</td>\n",
       "      <td>0.095762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>-0.073354</td>\n",
       "      <td>-0.172772</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.128028</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.051130</td>\n",
       "      <td>-0.116283</td>\n",
       "      <td>0.115144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067889</td>\n",
       "      <td>-0.007854</td>\n",
       "      <td>-0.136730</td>\n",
       "      <td>-0.078143</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.012736</td>\n",
       "      <td>-0.090365</td>\n",
       "      <td>0.101954</td>\n",
       "      <td>-0.207631</td>\n",
       "      <td>0.081567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.049146</td>\n",
       "      <td>-0.112506</td>\n",
       "      <td>-0.121235</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.111503</td>\n",
       "      <td>0.050727</td>\n",
       "      <td>0.101112</td>\n",
       "      <td>-0.127668</td>\n",
       "      <td>0.117911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095640</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>-0.136599</td>\n",
       "      <td>-0.035997</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>0.046752</td>\n",
       "      <td>-0.213177</td>\n",
       "      <td>0.110818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>-0.124910</td>\n",
       "      <td>-0.155769</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>0.138367</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>0.063956</td>\n",
       "      <td>-0.108430</td>\n",
       "      <td>0.063554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097628</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>-0.095431</td>\n",
       "      <td>-0.146156</td>\n",
       "      <td>-0.047555</td>\n",
       "      <td>0.056421</td>\n",
       "      <td>-0.067992</td>\n",
       "      <td>0.107304</td>\n",
       "      <td>-0.153172</td>\n",
       "      <td>0.040540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id         0         1         2         3         4         5  \\\n",
       "0        1  0.480469 -0.313522  0.448558 -0.265470 -0.448500 -0.408890   \n",
       "1        2 -0.011039 -0.120592 -0.145036  0.028828  0.129078  0.045131   \n",
       "2        3  0.031866 -0.073354 -0.172772  0.034140  0.128028  0.056722   \n",
       "3        4  0.049146 -0.112506 -0.121235  0.029425  0.111503  0.050727   \n",
       "4        5  0.002982 -0.124910 -0.155769 -0.006471  0.138367  0.054510   \n",
       "\n",
       "          6         7         8  ...        54        55        56        57  \\\n",
       "0 -0.519680  0.478366 -0.219287  ...  0.274009 -0.514024  0.632400  0.010554   \n",
       "1  0.080241 -0.115238  0.071108  ... -0.091944  0.024271 -0.144494 -0.145458   \n",
       "2  0.051130 -0.116283  0.115144  ... -0.067889 -0.007854 -0.136730 -0.078143   \n",
       "3  0.101112 -0.127668  0.117911  ... -0.095640  0.017186 -0.115709 -0.136599   \n",
       "4  0.063956 -0.108430  0.063554  ... -0.097628 -0.003393 -0.095431 -0.146156   \n",
       "\n",
       "         58        59        60        61        62        63  \n",
       "0  0.395779 -0.203225  0.453499 -0.461412  0.655305 -0.193951  \n",
       "1 -0.045634  0.020440 -0.114459  0.092429 -0.158437  0.095762  \n",
       "2  0.004284  0.012736 -0.090365  0.101954 -0.207631  0.081567  \n",
       "3 -0.035997  0.046666 -0.061223  0.046752 -0.213177  0.110818  \n",
       "4 -0.047555  0.056421 -0.067992  0.107304 -0.153172  0.040540  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66d7271e-0ea6-4568-ac5a-04089735f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embeddings.to_parquet(os.path.join(BASE_DIR,'item_embeddings.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadae279-913c-487b-ad55-4b4d6c110dc1",
   "metadata": {},
   "source": [
    "### Create feature definitions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70939f-8063-4422-b29b-6668acb1cfb7",
   "metadata": {},
   "source": [
    "Now we will create our user and item features definitions in the user_features.py and item_features.py files and save these files in the feature_repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4005d9b-1dba-40e0-ba69-c272edc8b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/user_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/user_features.py\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "import datetime \n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "user_features = FileSource(\n",
    "    path=\"/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/user_features.parquet\",\n",
    "    event_timestamp_column=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "user = Entity(name=\"user_id\", value_type=ValueType.INT32, description=\"user id\",)\n",
    "\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[\"user_id\"],\n",
    "    ttl=Duration(seconds=86400 * 7),\n",
    "    features=[\n",
    "        Feature(name=\"user_shops\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_profile\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_group\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_gender\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_age\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_consumption_2\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_is_occupied\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_geography\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_intentions\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_brands\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_categories\", dtype=ValueType.INT32),\n",
    "    ],\n",
    "    online=True,\n",
    "    input=user_features,\n",
    "    tags={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49c282ad-fcd2-448a-8a01-f6b0f0f325d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/item_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/item_features.py\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "import datetime \n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_features = FileSource(\n",
    "    path=\"/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/item_features.parquet\",\n",
    "    event_timestamp_column=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"item_id\", value_type=ValueType.INT32, description=\"item id\",)\n",
    "\n",
    "item_features_view = FeatureView(\n",
    "    name=\"item_features\",\n",
    "    entities=[\"item_id\"],\n",
    "    ttl=Duration(seconds=86400 * 7),\n",
    "    features=[\n",
    "        Feature(name=\"item_category\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"item_shop\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"item_brand\", dtype=ValueType.INT32),\n",
    "    ],\n",
    "    online=True,\n",
    "    input=item_features,\n",
    "    tags={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660333b2-4f99-49c7-8cd3-f0aad5dbd66f",
   "metadata": {},
   "source": [
    "Let's checkout our Feast feature repository structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57133c1e-18d9-4ccb-9704-cdebd271985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease   \n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [870 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1154 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1773 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2188 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1216 kB]\n",
      "Fetched 7535 kB in 2s (3099 kB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "tree is already the newest version (1.8.0-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# install tree\n",
    "!apt-get update\n",
    "!apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "986d53ea-c946-4046-a390-6d3b8801d280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── item_features.parquet\n",
      "│   └── user_features.parquet\n",
      "├── feature_store.yaml\n",
      "├── item_features.py\n",
      "└── user_features.py\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "feature_repo_path = os.path.join(BASE_DIR, 'feature_repo')\n",
    "!tree $feature_repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80678ea1-a7fb-4016-9e6f-c905497f4142",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "We trained and exported our ranking and retrieval models and NVTabular workflows. In the next step, we will learn how to deploy our trained models into [Triton Inference Server (TIS)](https://github.com/triton-inference-server/server) with Merlin Sytems library.\n",
    "\n",
    "For the next step, move on to the `02-Deploying-multi-stage-Recsys-with-Merlin-Systems.ipynb` notebook to deploy our saved models as an ensemble to TIS and obtain prediction results for a qiven request."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
