{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331f6cfb-a6c8-4d9a-9df4-2e91a46f97ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c069a59-b63c-4f25-ae1e-18f2e81015bb",
   "metadata": {},
   "source": [
    "# Deploying Ranking Models with Merlin Systems\n",
    "\n",
    "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines. The Merlin framework is broken up into several sub components, these include: Merlin-Core, Merlin-Models, NVTabular and Merlin-Systems. Merlin Systems will be the focus of this example.\n",
    "\n",
    "The purpose of the Merlin Systems library is to make it easy for Merlin users to quickly deploy their recommender systems from development to Triton Inference Server. We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying recommender system components to Triton.\n",
    "\n",
    "There are some points we need ensure before we continue with this Notebook. Please ensure you have a working NVTabular workflow and model stored in an accessible location. Merlin Systems take the data preprocessing workflow defined in NVTabular and load that into Triton Inference Server as a model. Subsequently it does the same for the trained model. Lets take a closer look at how Merlin Systems makes deploying to Triton simple and effortless, in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b90f7-26a5-4386-a591-c6d85f7afcb0",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "This Jupyter notebook example demonstrates \n",
    "- how to deploy an NVTabular model and a ranking model to Triton Inference Server as an ensemble\n",
    "- send a request to Triton \n",
    "- generate prediction results for a given query (a batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66befbf-b6c9-4d6a-b935-6ac80bac6e26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Starting Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efe32f-0a01-49d5-b276-29a4424c166f",
   "metadata": {},
   "source": [
    "After we export the ensemble, we are ready to start the Triton Inference Server. The server is installed in all the Merlin inference containers. If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server documentation.\n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "`tritonserver --model-repository = <path to the saved ensemble folder>`\n",
    "\n",
    "For the `--model-repository` argument, specify the same value as the `ensemble_export_path` that you specified previously when executing the `inference.py` script.\n",
    "\n",
    "After you run the tritonserver command, wait until your terminal shows messages like the following example:\n",
    "\n",
    "I0414 18:29:50.741833 4067 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001 <br>\n",
    "I0414 18:29:50.742197 4067 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000 <br>\n",
    "I0414 18:29:50.783470 4067 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002 ,br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1fbac-3fe7-40b7-96f2-bf4aee552e30",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2907046d-4ea4-4835-b23c-b93c26dc80d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 22:43:16.509753: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nvtabular.workflow import Workflow\n",
    "import tritonclient.grpc as grpcclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd7619-3639-4ba1-8977-de81e87118a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the saved NVTabular workflow. We will use workflow's input schema as an input below when sending request to Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d56801d-e3a9-4f28-88fc-e6506b8f222e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_path = os.environ.get(\"INPUT_FOLDER\", \"/workspace/Tenrec/dataset/workflow/\")\n",
    "workflow_stored_path = os.path.join(input_path)\n",
    "workflow = Workflow.load(workflow_stored_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0cb07e-ad12-48e5-88f7-aeb4f8b548ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_category</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int8', element_type=&lt;ElementType.I...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gender</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int8', element_type=&lt;ElementType.I...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int8', element_type=&lt;ElementType.I...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'user_id', 'tags': set(), 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'item_id', 'tags': set(), 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'video_category', 'tags': set(), 'properties': {}, 'dtype': DType(name='int8', element_type=<ElementType.Int: 'int'>, element_size=8, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'gender', 'tags': set(), 'properties': {}, 'dtype': DType(name='int8', element_type=<ElementType.Int: 'int'>, element_size=8, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'age', 'tags': set(), 'properties': {}, 'dtype': DType(name='int8', element_type=<ElementType.Int: 'int'>, element_size=8, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.input_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e449de-3a15-4206-9f89-2b1d14033849",
   "metadata": {},
   "source": [
    "Load the saved output names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e4f7b1-f9d4-48c7-9e84-7890204d4a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['click/binary_output', 'follow/binary_output']\n"
     ]
    }
   ],
   "source": [
    "outputs = np.load(open('outputs.npy', 'rb'), allow_pickle=True).tolist()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a949a0-077c-4259-bae3-3540f0b2c20b",
   "metadata": {},
   "source": [
    "We prepare a batch request to send to Triton and then Triton is going to send us a response, basically probability values for each target column. Since we are serving NVTabular model and our ranking model as a pipeline together, we can send a raw data as a request and served NVTabular model should be able to transform it in the same way it transformed the training set during preprocessing step.\n",
    "\n",
    "One thing to note that in this example, we are not creating the raw data from raw `.csv` file since, we did some data preparations and removed some user and items from the dataset based on the min frequencies we set during preprocessing file. So we use the raw validation data that were generated after train and eval set split step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d90575e-4fa9-4db4-b3bc-f4f10e9cadf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data_path = os.environ.get(\"INPUT_FOLDER\", \"/workspace/Tenrec/dataset/_cache/02/\")\n",
    "batch = pd.read_parquet(os.path.join(original_data_path, \"eval/\", \"part.0.parquet\"), columns=workflow.input_schema.column_names).reset_index(drop=True)\n",
    "batch = batch.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ebeb6c-b96c-42ac-8715-25fd50d716e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>video_category</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14402</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276438</td>\n",
       "      <td>8176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215817</td>\n",
       "      <td>62881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117574</td>\n",
       "      <td>74582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195714</td>\n",
       "      <td>34982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>117050</td>\n",
       "      <td>9769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>219293</td>\n",
       "      <td>7750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>252272</td>\n",
       "      <td>59629</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>232320</td>\n",
       "      <td>12737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>168813</td>\n",
       "      <td>148443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  video_category  gender  age\n",
       "0    14402      412               0       0    0\n",
       "1   276438     8176               1       0    0\n",
       "2   215817    62881               0       0    0\n",
       "3   117574    74582               1       0    0\n",
       "4   195714    34982               0       0    0\n",
       "5   117050     9769               0       0    0\n",
       "6   219293     7750               0       0    0\n",
       "7   252272    59629               0       0    0\n",
       "8   232320    12737               0       0    0\n",
       "9   168813   148443               0       0    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a0a3c-893d-4dcf-9c66-f01e8a0a6336",
   "metadata": {},
   "source": [
    "## Deploy models on Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a17ff0-3d33-41c3-a767-4167366333e6",
   "metadata": {},
   "source": [
    "First we need to ensure that we have a client connected to the server that we started. To do this, we use the Triton HTTP client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e36c3c-71e8-486b-977e-b8883c5f23b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as client\n",
    "\n",
    "# Create a triton client\n",
    "try:\n",
    "    triton_client = client.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e62dbd1-ab9d-42f9-a5e9-7c0fac7b0b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '191'}>\n",
      "bytearray(b'[{\"name\":\"0_transformworkflowtriton\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"1_predicttensorflowtriton\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"executor_model\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': '0_transformworkflowtriton', 'version': '1', 'state': 'READY'},\n",
       " {'name': '1_predicttensorflowtriton', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'executor_model', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure triton is in a good state\n",
    "triton_client.is_server_live()\n",
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc393a-83ea-4d5f-9994-206717e4cfec",
   "metadata": {},
   "source": [
    "Now that our server is running, we can send requests to it. In the code below we create a request to send to triton and send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2a80fb-b565-40a6-ae95-a7b482795b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'click/binary_output': array([[0.5073921 ],\n",
      "       [0.50442815],\n",
      "       [0.5070812 ],\n",
      "       [0.5021189 ],\n",
      "       [0.5028127 ],\n",
      "       [0.5052893 ],\n",
      "       [0.5060355 ],\n",
      "       [0.51079565],\n",
      "       [0.50457317],\n",
      "       [0.50388616]], dtype=float32), 'follow/binary_output': array([[0.49480468],\n",
      "       [0.49666217],\n",
      "       [0.49884534],\n",
      "       [0.4969691 ],\n",
      "       [0.4962534 ],\n",
      "       [0.49507955],\n",
      "       [0.49622113],\n",
      "       [0.49824178],\n",
      "       [0.49707925],\n",
      "       [0.49556646]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.triton.utils import send_triton_request\n",
    "response = send_triton_request(workflow.input_schema, batch, outputs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd36d25-20a5-41f5-b7c4-8118bfc3efb3",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8c181-a3ec-405a-a4ae-eb396db20d86",
   "metadata": {},
   "source": [
    "Congratulations on completing this quick start guide example series!\n",
    "\n",
    "In this quick start example series, you have preprocessed and transformed the data with NVTabular, trained a single-task or multi-task model with Merlin Models, and then finally deployed these models on Triton Inference Server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
