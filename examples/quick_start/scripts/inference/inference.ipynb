{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331f6cfb-a6c8-4d9a-9df4-2e91a46f97ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c069a59-b63c-4f25-ae1e-18f2e81015bb",
   "metadata": {},
   "source": [
    "\n",
    "## Serving Ranking Models With Merlin Systems\n",
    "This notebook is created using the latest stable merlin-tensorflow container. This Jupyter notebook example demonstrates how to deploy a ranking model to Triton Inference Server (TIS) and generate prediction results for a given query. As a prerequisite, the ranking model must be trained and saved with Merlin Models. Please read the README for the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b90f7-26a5-4386-a591-c6d85f7afcb0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines. The Merlin framework is broken up into several sub components, these include: Merlin-Core, Merlin-Models, NVTabular and Merlin-Systems. Merlin Systems will be the focus of this example.\n",
    "\n",
    "The purpose of the Merlin Systems library is to make it easy for Merlin users to quickly deploy their recommender systems from development to Triton Inference Server. We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying recommender system components to TIS.\n",
    "\n",
    "There are some points we need ensure before we continue with this Notebook. Please ensure you have a working NVTabular workflow and model stored in an accessible location. Merlin Systems take the data preprocessing workflow defined in NVTabular and load that into Triton Inference Server as a model. Subsequently it does the same for the trained model. Lets take a closer look at how Merlin Systems makes deploying to TIS simple and effortless, in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66befbf-b6c9-4d6a-b935-6ac80bac6e26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efe32f-0a01-49d5-b276-29a4424c166f",
   "metadata": {},
   "source": [
    "After we export the ensemble, we are ready to start the Triton Inference Server. The server is installed in all the Merlin inference containers. If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server documentation.\n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "tritonserver --model-repository=/workspace/data/ensemble --backend-config=tensorflow,version=2\n",
    "For the --model-repository argument, specify the same value as the export_path that you specified previously in the ensemble.export method.\n",
    "\n",
    "After you run the tritonserver command, wait until your terminal shows messages like the following example:\n",
    "\n",
    "I0414 18:29:50.741833 4067 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001\n",
    "I0414 18:29:50.742197 4067 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000\n",
    "I0414 18:29:50.783470 4067 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
