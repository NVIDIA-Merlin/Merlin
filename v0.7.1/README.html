<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIDIA Merlin &mdash; Merlin  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NVIDIA Merlin Example Notebooks" href="examples/index.html" />
    <link rel="prev" title="Merlin" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Merlin
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#benefits">Benefits</a></li>
<li class="toctree-l2"><a class="reference internal" href="#components-of-nvidia-merlin">Components of NVIDIA Merlin</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="containers.html">Merlin Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="support_matrix/index.html">Merlin Support Matrix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Merlin</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>NVIDIA Merlin</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="nvidia-merlin">
<h1><a class="reference external" href="https://github.com/NVIDIA-Merlin">NVIDIA Merlin</a><a class="headerlink" href="#nvidia-merlin" title="Permalink to this headline"></a></h1>
<p>NVIDIA Merlin is an open source library designed to accelerate recommender systems on NVIDIA’s GPUs. It enables data scientists, machine learning engineers, and researchers to build high-performing recommenders at scale. Merlin includes tools to address common ETL, training, and inference challenges. Each stage of the Merlin pipeline is optimized to support hundreds of terabytes of data, which is all accessible through easy-to-use APIs. For more information, see <a class="reference external" href="https://developer.nvidia.com/nvidia-merlin">NVIDIA Merlin</a>.</p>
<div class="section" id="benefits">
<h2>Benefits<a class="headerlink" href="#benefits" title="Permalink to this headline"></a></h2>
<p>NVIDIA Merlin is a scalable and GPU-accelerated solution, making it easy to build recommender systems from end to end. With NVIDIA Merlin, you can:</p>
<ul class="simple">
<li><p>transform data (ETL) for preprocessing and engineering features.</p></li>
<li><p>accelerate existing training pipelines in TensorFlow, PyTorch, or FastAI by leveraging optimized, custom-built dataloaders.</p></li>
<li><p>scale large deep learning recommender models by distributing large embedding tables that exceed available GPU and CPU memory.</p></li>
<li><p>deploy data transformations and trained models to production with only a few lines of code.</p></li>
</ul>
</div>
<div class="section" id="components-of-nvidia-merlin">
<h2>Components of NVIDIA Merlin<a class="headerlink" href="#components-of-nvidia-merlin" title="Permalink to this headline"></a></h2>
<p>NVIDIA Merlin consists of the following open source libraries:</p>
<ul class="simple">
<li><p>NVTabular</p></li>
<li><p>HugeCTR</p></li>
<li><p>Triton Inference Server</p></li>
</ul>
<p align="center">
<img src='https://developer.nvidia.com/sites/default/files/akamai/merlin/recommender-systems-dev-web-850.svg' width="65%">
</p>
<p><strong><a class="reference external" href="https://github.com/NVIDIA/NVTabular">NVTabular</a></strong><br>
NVTabular is a feature engineering and preprocessing library for tabular data. NVTabular is essentially the ETL component of the Merlin ecosystem. It is designed to quickly and easily manipulate terabyte-size datasets that are used to train deep learning based recommender systems. NVTabular offers a high-level API that can be used to define complex data transformation workflows. NVTabular is also capable of transformation speedups that can be 100 times to 1,000 times faster than transformations taking place on optimized CPU clusters. With NVTabular, you can:</p>
<ul class="simple">
<li><p>prepare datasets quickly and easily for experimentation so that more models can be trained.</p></li>
<li><p>process datasets that exceed GPU and CPU memory without having to worry about scale.</p></li>
<li><p>focus on what to do with the data and not how to do it by using abstraction at the operation level.</p></li>
</ul>
<p><strong><a class="reference external" href="https://github.com/NVIDIA/NVTabular">NVTabular DataLoaders</a></strong><br>
NVTabular provides seamless integration with common deep learning frameworks, such as TensorFlow, PyTorch, and HugeCTR. When training deep learning recommender system models, dataloading can be a bottleneck. Therefore, we’ve developed custom, highly-optimized dataloaders to accelerate existing TensorFlow and PyTorch training pipelines. The NVTabular dataloaders can lead to a speedup that is nine times faster than the same training pipeline used with the GPU. With the NVTabular dataloaders, you can:</p>
<ul class="simple">
<li><p>remove bottlenecks from dataloading by processing large chunks of data at a time instead of item by item.</p></li>
<li><p>process datasets that don’t fit within the GPU or CPU memory by streaming from the disk.</p></li>
<li><p>prepare batches asynchronously into the GPU to avoid CPU-GPU communication.</p></li>
<li><p>integrate easily into existing TensorFlow or PyTorch training pipelines by using a similar API.</p></li>
</ul>
<p><strong><a class="reference external" href="https://github.com/NVIDIA/HugeCTR">HugeCTR</a></strong><br>
HugeCTR is a GPU-accelerated framework designed to estimate click-through rates and distribute training across multiple GPUs and nodes. HugeCTR contains optimized dataloaders that can be used to prepare batches with GPU-acceleration. In addition, HugeCTR is capable of scaling large deep learning recommendation models. The neural network architectures often contain large embedding tables that represent hundreds of millions of users and items. These embedding tables can easily exceed the CPU and GPU memory. HugeCTR provides strategies for scaling large embedding tables beyond available memory. With HugeCTR, you can:</p>
<ul class="simple">
<li><p>scale embedding tables over multiple GPUs or nodes.</p></li>
<li><p>load a subset of an embedding table into the GPU in a coarse grained, on-demand manner during the training stage.</p></li>
</ul>
<p><strong><a class="reference external" href="https://github.com/triton-inference-server/server">Triton</a></strong><br>
NVTabular and HugeCTR both support the Triton Inference Server to provide GPU-accelerated inference. The Triton Inference Server is open-source inference serving software that can be used to simplify the deployment of trained AI models from any framework to production. With Triton, you can:</p>
<ul class="simple">
<li><p>deploy NVTabular ETL workflows and trained deep learning models to production with a few lines of code.</p></li>
<li><p>deploy an ensemble of NVTabular ETL and trained deep learning models to ensure that the same data transformations are applied in production.</p></li>
<li><p>deploy models concurrently on GPUs to maximize utilization.</p></li>
<li><p>enable low latency inferencing in real time or batch inferencing to maximize GPU and CPU utilization.</p></li>
<li><p>scale the production environment with Kubernetes for orchestration, metrics, and auto-scaling using a Docker container.</p></li>
</ul>
</div>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline"></a></h2>
<p>A collection of <a class="reference internal" href="examples/index.html"><span class="xref myst">end-to-end examples</span></a> is available within this repository in the form of Jupyter notebooks. The example notebooks demonstrate how to:</p>
<ul class="simple">
<li><p>download and prepare the dataset.</p></li>
<li><p>use preprocessing and engineering features.</p></li>
<li><p>train deep learning recommendation models with TensorFlow, PyTorch, FastAI, or HugeCTR.</p></li>
<li><p>deploy the models to production.</p></li>
</ul>
<p>These examples are based on different datasets and provide a wide range of real-world use cases.</p>
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h2>
<p>For more information about NVIDIA Merlin and its components, see the following:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/NVTabular">NVTabular GitHub</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/NVTabular/main/training/index.html">NVTabular Accelerated Training Documentation</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/NVTabular/main/resources/support_matrix.html">NVTabular Support Matrix</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/NVTabular/main/Introduction.html">NVTabular API Documentation</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/HugeCTR">HugeCTR GitHub</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_user_guide.md">HugeCTR User Guide</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/HugeCTR/blob/master/tools/dockerfiles/support_matrix.md">HugeCTR Support Matrix</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md">HugeCTR Python API</a></p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Merlin" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="examples/index.html" class="btn btn-neutral float-right" title="NVIDIA Merlin Example Notebooks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.7.1
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="README.html">v0.7.1</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>