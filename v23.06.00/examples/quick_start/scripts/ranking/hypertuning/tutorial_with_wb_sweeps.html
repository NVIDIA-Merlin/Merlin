<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hyperparameter tuning with Quick-start and Weights&amp;Biases Sweeps &mdash; Merlin  documentation</title><link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/Merlin/stable/examples/quick_start/scripts/ranking/hypertuning/tutorial_with_wb_sweeps.html" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../../../../../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            Merlin
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../guide/recommender_system_guide.html">Recommender System Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../containers.html">Merlin Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../support_matrix/index.html">Merlin Support Matrix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Merlin</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Hyperparameter tuning with Quick-start and Weights&amp;Biases Sweeps</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="hyperparameter-tuning-with-quick-start-and-weights-biases-sweeps">
<h1>Hyperparameter tuning with Quick-start and Weights&amp;Biases Sweeps<a class="headerlink" href="#hyperparameter-tuning-with-quick-start-and-weights-biases-sweeps" title="Permalink to this headline"></a></h1>
<p>In machine learning, hyperparameter tuning is the process of adjusting the model or training hyperparameters to get better values for metrics of interest (e.g. accuracy).</p>
<p>The Command Line Interface (CLI) of the Quick-start scripts makes it easy to setup a hyperparameter tuning jobs in Cloud solutions like <a class="reference external" href="https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview">Google Vertex AI</a> and <a class="reference external" href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters?view=azureml-api-2">MS Azure ML</a>, which can launch distributed hypertuning jobs.</p>
<p>In this tutorial, we demonstrate how to setup a hypertuning process of the <code class="docutils literal notranslate"><span class="pre">ranking.py</span></code> script in a single server/cloud instance using <a class="reference external" href="https://docs.wandb.ai/guides/sweeps/">Weights&amp;Biases Sweeps</a>.</p>
<div class="section" id="creating-the-sweeps-config-file">
<h2>1. Creating the Sweeps config file<a class="headerlink" href="#creating-the-sweeps-config-file" title="Permalink to this headline"></a></h2>
<p>First, you need to create a sweep configuration file, which defines the hyperparameters to tune, their search space and the metric to be optimized.</p>
<p>Here is an example of the config file for the <a class="reference download internal" download="" href="../../../../../_downloads/1c928518abcb337bfb61461a825be6ee/ranking.py"><span class="xref download myst">ranking.py</span></a> script. You will notice that we specify in <code class="docutils literal notranslate"><span class="pre">program:</span> <span class="pre">ranking.py</span></code> the script we want to execute, the <code class="docutils literal notranslate"><span class="pre">method:</span> <span class="pre">bayes</span></code> for bayesian optimization of the hyperparameters, toward maximizing the <code class="docutils literal notranslate"><span class="pre">auc-final</span></code> metric, which is computed on the evaluation set by the ranking script.<br />
Then you specify the hyperparameters distribution, which can be <code class="docutils literal notranslate"><span class="pre">categorical</span></code>, <code class="docutils literal notranslate"><span class="pre">int_uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">log_uniform</span></code>, among others.</p>
<p>The constant parameters that will not vary in the hyperparameter tuning (e.g. <code class="docutils literal notranslate"><span class="pre">--epochs</span></code>, <code class="docutils literal notranslate"><span class="pre">--train_data_path</span></code>, <code class="docutils literal notranslate"><span class="pre">--eval_data_path</span></code>) can be provided using a <code class="docutils literal notranslate"><span class="pre">categorical</span></code> distribution with a single value.</p>
<p>You can learn about more about the <a class="reference external" href="https://docs.wandb.ai/guides/sweeps/define-sweep-configuration">W&amp;B sweeps configuration</a> or about the <span class="xref myst">ranking.py script</span> hyperparameters available in its CLI.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">program</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ranking.py</span>
<span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bayes</span>
<span class="nt">metric</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auc-final</span>
<span class="w">  </span><span class="nt">goal</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">maximize</span>
<span class="nt">parameters</span><span class="p">:</span>
<span class="w">  </span><span class="nt">tasks</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;click&quot;</span>
<span class="w">  </span><span class="nt">stl_positive_class_weight</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">int_uniform</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">  </span><span class="nt">activation</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="w">  </span><span class="nt">dropout</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">embeddings_l2_reg</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-05</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-09</span>
<span class="w">  </span><span class="nt">embedding_sizes_multiplier</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">int_uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">l2_reg</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-04</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-08</span>
<span class="w">  </span><span class="nt">lr</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-01</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-03</span>
<span class="w">  </span><span class="nt">lr_decay_rate</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.98</span>
<span class="w">  </span><span class="nt">lr_decay_steps</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">int_uniform</span>
<span class="w">    </span><span class="nt">max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>
<span class="w">    </span><span class="nt">min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">mlp_layers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;64&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;128&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;256&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;64,32&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;128,64&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;256,128&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;128,64,32&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;256,128,64&quot;</span>
<span class="w">  </span><span class="nt">model</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlp</span>
<span class="w">  </span><span class="nt">train_batch_size</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16384</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32768</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65536</span>
<span class="w">  </span><span class="nt">eval_batch_size</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65536</span>
<span class="w">  </span><span class="nt">epochs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">train_data_path</span><span class="p">:</span><span class="w"> </span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/preproc_data/train</span>
<span class="w">  </span><span class="nt">eval_data_path</span><span class="p">:</span><span class="w"> </span>
<span class="w">    </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">categorical</span>
<span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/preproc_data/eval</span>
</pre></div>
</div>
<blockquote>
<div><p>We provide the <span class="xref myst">sweep config files</span> from our <span class="xref myst">hypertuning of ranking models</span> for TenRec dataset. You can use their search space as a basis for setting up your config files for hypertuning ranking models on your own dataset.</p>
</div></blockquote>
</div>
<div class="section" id="environment-setup">
<h2>2. Environment setup<a class="headerlink" href="#environment-setup" title="Permalink to this headline"></a></h2>
<p>We need to prepare the environment configured for running the Quick-start scripts. The easiest way is to pull and run the Merlin Tensorflow image, as explained <a class="reference internal" href="../../../ranking.html"><span class="doc std std-doc">here</span></a>, mapping the folder with the TenRec dataset.</p>
<blockquote>
<div><p>It assumes that you have already preprocessed the TenRec dataset using <code class="docutils literal notranslate"><span class="pre">preprocessing.py</span></code>, as explained <a class="reference internal" href="../../../ranking.html"><span class="doc std std-doc">here</span></a>.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PREPROC_INPUT_DATA_PATH</span><span class="o">=</span>/path/to/input/preprocessed/dataset/
<span class="nv">OUTPUT_PATH</span><span class="o">=</span>/path/to/output/path/
docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>-v<span class="w"> </span><span class="nv">$PREPROC_INPUT_DATA_PATH</span>:/preproc_data<span class="w"> </span>-v<span class="w"> </span><span class="nv">$OUTPUT_PATH</span>:/outputs<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>nvcr.io/nvidia/merlin/merlin-tensorflow:latest<span class="w"> </span>/bin/bash
</pre></div>
</div>
<div class="section" id="setup-wandb-client">
<h3>2.1 Setup wandb client<a class="headerlink" href="#setup-wandb-client" title="Permalink to this headline"></a></h3>
<p>Inside the container, we need to reinstall <strong>wandb</strong> PyPy package with <code class="docutils literal notranslate"><span class="pre">pip</span></code> so that the <code class="docutils literal notranslate"><span class="pre">wandb</span></code> CLI is properly installed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>uninstall<span class="w"> </span>wandb<span class="w"> </span>-y
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>/Merlin/examples/quick_start/requirements.txt
</pre></div>
</div>
<p>Then run this command to configure wandb CLI with your Weights&amp;Biases account.</p>
<blockquote>
<div><p>You will nee a Weights&amp;Biases, that you can create for free at <a class="reference external" href="https://wandb.ai/">https://wandb.ai/</a>.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wandb<span class="w"> </span>init
</pre></div>
</div>
<p>You will be prompted to paste the <strong>W&amp;B API key</strong>, which you can obtain after logging with your account at <a class="reference external" href="https://wandb.ai/authorize">https://wandb.ai/authorize</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wandb</span><span class="p">:</span> <span class="n">Logging</span> <span class="n">into</span> <span class="n">wandb</span><span class="o">.</span><span class="n">ai</span><span class="o">.</span> <span class="p">(</span><span class="n">Learn</span> <span class="n">how</span> <span class="n">to</span> <span class="n">deploy</span> <span class="n">a</span> <span class="n">W</span><span class="o">&amp;</span><span class="n">B</span> <span class="n">server</span> <span class="n">locally</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">wandb</span><span class="o">.</span><span class="n">me</span><span class="o">/</span><span class="n">wandb</span><span class="o">-</span><span class="n">server</span><span class="p">)</span>
<span class="n">wandb</span><span class="p">:</span> <span class="n">You</span> <span class="n">can</span> <span class="n">find</span> <span class="n">your</span> <span class="n">API</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">your</span> <span class="n">browser</span> <span class="n">here</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">wandb</span><span class="o">.</span><span class="n">ai</span><span class="o">/</span><span class="n">authorize</span>
<span class="n">wandb</span><span class="p">:</span> <span class="n">Paste</span> <span class="n">an</span> <span class="n">API</span> <span class="n">key</span> <span class="kn">from</span> <span class="nn">your</span> <span class="n">profile</span> <span class="ow">and</span> <span class="n">hit</span> <span class="n">enter</span><span class="p">,</span> <span class="ow">or</span> <span class="n">press</span> <span class="n">ctrl</span><span class="o">+</span><span class="n">c</span> <span class="n">to</span> <span class="n">quit</span>
</pre></div>
</div>
<p>Then it will prompt you with a team name and project name that should be used by default.</p>
<p>That is it, you have setup Weights&amp;Biases inside the Merlin Tensorflow container.</p>
</div>
</div>
<div class="section" id="testing-the-environment">
<h2>3. Testing the environment<a class="headerlink" href="#testing-the-environment" title="Permalink to this headline"></a></h2>
<p>Go to the folder with the <code class="docutils literal notranslate"><span class="pre">ranking.py</span></code> and run it with some basic parameters, to test if all requirements are installed correctly and if it runs until the end properly.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/Merlin/examples/quick_start/scripts/ranking
<span class="nv">PREPROC_DATA_PATH</span><span class="o">=</span>/preproc_data
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">TF_GPU_ALLOCATOR</span><span class="o">=</span>cuda_malloc_async<span class="w"> </span>python<span class="w"> </span>ranking.py<span class="w"> </span>--train_data_path<span class="w"> </span><span class="nv">$PREPROC_DATA_PATH</span>/train<span class="w"> </span>--eval_data_path<span class="w"> </span><span class="nv">$PREPROC_DATA_PATH</span>/eval<span class="w"> </span>--output_path<span class="w"> </span>./outputs/<span class="w"> </span>--tasks<span class="o">=</span>click<span class="w"> </span>--stl_positive_class_weight<span class="w"> </span><span class="m">3</span><span class="w"> </span>--model<span class="w"> </span>dlrm<span class="w"> </span>--embeddings_dim<span class="w"> </span><span class="m">64</span><span class="w"> </span>--l2_reg<span class="w"> </span>1e-2<span class="w"> </span>--embeddings_l2_reg<span class="w"> </span>1e-6<span class="w"> </span>--dropout<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span>--mlp_layers<span class="w"> </span><span class="m">64</span>,32<span class="w">  </span>--lr<span class="w"> </span>1e-4<span class="w"> </span>--lr_decay_rate<span class="w"> </span><span class="m">0</span>.99<span class="w"> </span>--lr_decay_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span>--train_batch_size<span class="w"> </span><span class="m">65536</span><span class="w"> </span>--eval_batch_size<span class="w"> </span><span class="m">65536</span><span class="w"> </span>--epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span>--train_steps_per_epoch<span class="w"> </span><span class="m">10</span><span class="w"> </span>--log_to_wandb
</pre></div>
</div>
<p>The above command includes <code class="docutils literal notranslate"><span class="pre">--log_to_wandb</span></code>, that enables logging with Weights&amp;Biases. You can also configure the W&amp;B team (<code class="docutils literal notranslate"><span class="pre">--wandb_entity</span></code>) and project (<code class="docutils literal notranslate"><span class="pre">--wandb_project</span></code>) to create the run.</p>
<p>During the execution, W&amp;B will log your execution and provide a run URL, that allows tracking metrics over time.</p>
<p>After that, the <code class="docutils literal notranslate"><span class="pre">ranking.py</span></code> script should finish sucessfully until the end, to ensure if you have your environment ready for starting the hypertuning.</p>
</div>
<div class="section" id="creating-the-sweep-at-weights-biases">
<h2>4. Creating the Sweep at Weights&amp;Biases<a class="headerlink" href="#creating-the-sweep-at-weights-biases" title="Permalink to this headline"></a></h2>
<p>You need to create a Sweep at Weights&amp;Biases, that will manage the hypertuning process. You can either create it at <a class="reference external" href="https://wandb.ai/">https://wandb.ai/</a> website, or via <code class="docutils literal notranslate"><span class="pre">wandb</span></code> CLI like in this example, where we use one of our existing sweep config files.</p>
<blockquote>
<div><p>Remember to add to set in the sweep config file the <code class="docutils literal notranslate"><span class="pre">--train_data_path</span></code> and <code class="docutils literal notranslate"><span class="pre">--eval_data_path</span></code>, as in the example above.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wandb<span class="w"> </span>sweep<span class="w"> </span>--name<span class="w"> </span>&lt;SWEEP_NAME&gt;<span class="w"> </span>/Merlin/examples/quick_start/scripts/ranking/hypertuning/wandb_sweeps/stl_mlp/stl_click_mlp.yaml
</pre></div>
</div>
<p>That command will print the link to the Sweep interface at <a class="reference external" href="https://wandb.ai/">https://wandb.ai/</a> and an example of the command to start the hypertuning: <code class="docutils literal notranslate"><span class="pre">wandb</span> <span class="pre">agent</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wandb</span><span class="p">:</span> <span class="n">Created</span> <span class="n">sweep</span> <span class="k">with</span> <span class="n">ID</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">SWEEP_ID</span><span class="o">&gt;</span>
<span class="n">wandb</span><span class="p">:</span> <span class="n">View</span> <span class="n">sweep</span> <span class="n">at</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">wandb</span><span class="o">.</span><span class="n">ai</span><span class="o">/&lt;</span><span class="n">TEAM_NAME</span><span class="o">&gt;/&lt;</span><span class="n">PROJECT_NAME</span><span class="o">&gt;/</span><span class="n">sweeps</span><span class="o">/&lt;</span><span class="n">SWEEP_ID</span><span class="o">&gt;</span>
<span class="n">wandb</span><span class="p">:</span> <span class="n">Run</span> <span class="n">sweep</span> <span class="n">agent</span> <span class="k">with</span><span class="p">:</span> <span class="n">wandb</span> <span class="n">agent</span> <span class="o">&lt;</span><span class="n">TEAM_NAME</span><span class="o">&gt;/&lt;</span><span class="n">PROJECT_NAME</span><span class="o">&gt;/</span><span class="n">sweeps</span><span class="o">/&lt;</span><span class="n">SWEEP_ID</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="starting-the-hypertuning-process">
<h2>5. Starting the hypertuning process<a class="headerlink" href="#starting-the-hypertuning-process" title="Permalink to this headline"></a></h2>
<p>It is time to use <code class="docutils literal notranslate"><span class="pre">wandb</span> <span class="pre">agent</span></code> to start the hypertuning process using the full sweep identification obtained in the previous command: <code class="docutils literal notranslate"><span class="pre">&lt;TEAM_NAME&gt;/&lt;PROJECT_NAME&gt;/sweeps/&lt;SWEEP_ID&gt;</span></code>.</p>
<p>It allows configuring the number of trials for hypertuning with <code class="docutils literal notranslate"><span class="pre">--count</span></code>, which will be run sequentially. If you want to make the hypertuning faster, you might start multiple <code class="docutils literal notranslate"><span class="pre">wandb</span> <span class="pre">agent</span></code> process in parallel using the same sweep id (each with their own <code class="docutils literal notranslate"><span class="pre">--count</span></code>), so they share information during the hypertuning. In this case, you need to set a separate GPU for each process with <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> env variable.</p>
<p>You can make <code class="docutils literal notranslate"><span class="pre">wandb</span> <span class="pre">agent</span></code> to run in background with <code class="docutils literal notranslate"><span class="pre">nohup</span></code> and save its messages to a file with <code class="docutils literal notranslate"><span class="pre">&amp;&gt;</span></code>. Here is an example command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">TF_GPU_ALLOCATOR</span><span class="o">=</span>cuda_malloc_async<span class="w"> </span>nohup<span class="w"> </span>wandb<span class="w"> </span>agent<span class="w"> </span>--count<span class="w"> </span><span class="m">100</span><span class="w"> </span>&lt;TEAM_NAME&gt;/&lt;PROJECT_NAME&gt;/sweeps/&lt;SWEEP_ID&gt;<span class="w"> </span><span class="p">&amp;</span>&gt;<span class="w"> </span><span class="se">\t</span>mp<span class="se">\n</span>ohup0.out<span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>That is it! You can now let hypertuning work improving your model accuracy while you go for other activities!</p>
</div>
<div class="section" id="monitoring-the-hypertuning-process-analysing-results">
<h2>5. Monitoring the hypertuning process &amp; analysing results<a class="headerlink" href="#monitoring-the-hypertuning-process-analysing-results" title="Permalink to this headline"></a></h2>
<p>You can track the hypertuning process by checking the W&amp;B Sweep URL provided when you created it. As show in below image, Weights&amp;Biases provides a nice interface to explore the Sweep results. It lists all the runs, which you can sort by your optimization metric (<code class="docutils literal notranslate"><span class="pre">auc-final</span></code> in this case).</p>
<p>That interface also provides out-of-the-box charts showing how runs accuracy evolved over time, the most important hyperparameters with respect to the optimization metric and a parallel bars chart across the hparams plot colored by the accuracy, to better visualize the search space ranges that provide the better accuracy.</p>
<center>
<img src="../../../images/wandb_sweeps.png" alt="Example of W&B Sweep interface" >
</center>
<p>For more information on how to manage the sweep, please refer to <a class="reference external" href="https://docs.wandb.ai/guides/sweeps/">Weights&amp;Biases Sweeps docs</a>.</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v23.06.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../../../../v22.11.00/index.html">v22.11.00</a></dd>
      <dd><a href="../../../../../../v22.12.00/index.html">v22.12.00</a></dd>
      <dd><a href="../../../../../../v23.02.00/index.html">v23.02.00</a></dd>
      <dd><a href="../../../../../../v23.04.00/index.html">v23.04.00</a></dd>
      <dd><a href="../../../../../../v23.05.00/index.html">v23.05.00</a></dd>
      <dd><a href="tutorial_with_wb_sweeps.html">v23.06.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../../../../../main/index.html">main</a></dd>
      <dd><a href="../../../../../../stable/index.html">stable</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>