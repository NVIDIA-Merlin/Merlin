{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "## Deploying the Model into Production with Merlin Systems and Triton IS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9657308-2e08-49b4-8924-eace75a4634c",
   "metadata": {},
   "source": [
    "At this point, when you reach out to this notebook, we expect that you have already executed the first notebook `01-Building-Recommender-Systems-PoC.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![tritonensemble](../images/triton_ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "Merlin Systems library have the set of operators to be able to serve multi-stage recommender systems built with Tensorflow on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently. Below, we will go through these operators and demonstrate their usage in serving a multi-stage system on Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/graph.py:23: FutureWarning: The `nvtabular.graph` module has moved to `merlin.dag`. Support for importing from `nvtabular.graph` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.dag`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/io.py:23: FutureWarning: The `nvtabular.io` module has moved to `merlin.io`. Support for importing from `nvtabular.io` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.io`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/utils.py:23: FutureWarning: The `nvtabular.utils` module has moved to `merlin.core.utils`. Support for importing from `nvtabular.utils` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.core.utils`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/dispatch.py:23: FutureWarning: The `nvtabular.dispatch` module has moved to `merlin.core.dispatch`. Support for importing from `nvtabular.dispatch` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.core.dispatch`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cudf\n",
    "import feast\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "\n",
    "from merlin.systems.triton.utils import run_triton_server, run_ensemble_on_tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ead20e-c573-462e-9aa2-c3494bf0129f",
   "metadata": {},
   "source": [
    "### Feast Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac115e-4794-4a69-a962-8481f6e86df3",
   "metadata": {},
   "source": [
    "We have defined our user and item features definitions in the `user_features.py` and  `item_features.py` files. With FeatureView() users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling Feast where to find user and item features.\n",
    "\n",
    "\n",
    "Before we move on to the next steps, we need to perform `apply`command as directed below:\n",
    "\n",
    "```\n",
    "# open a terminal and navigate to the `feature_repo` folder\n",
    "\n",
    "cd /Merlin/examples/PoC/feature_repo\n",
    "\n",
    "# run the following command\n",
    "\n",
    "feast apply\n",
    "```\n",
    "\n",
    "With `feast apply` we register our features, we can apply the changes to create our feature registry and store all entity and feature view definitions in a local SQLite online store called `online_store.db`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641fcd2-bd11-4569-80d4-2ae5e01a5cad",
   "metadata": {},
   "source": [
    "### Feast Materialize\n",
    "\n",
    "After we execute `apply` and registered our features and created our online local store, now we need to perform [materialization](https://docs.feast.dev/how-to-guides/running-feast-in-production) operation. This is done to keep our online store up to date and get it ready for prediction. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store up to date by finding the latest feature values for each user. \n",
    "\n",
    "When you run the `feast materialize ..` command below, you will see a print out message <i>Materializing 2 feature views from 1995-01-01 01:01:01+00:00 to 2025-01-01 01:01:01+00:00 into the sqlite online store </i> on your terminal.\n",
    "\n",
    "```\n",
    "# open a terminal and navigate to the `feature_repo` folder\n",
    "\n",
    "cd /Merlin/examples/PoC/feature_repo\n",
    "\n",
    "# run the following commands\n",
    "\n",
    "feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01\n",
    "```\n",
    "\n",
    "Note that materialization step takes some time.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc26e6-f6f3-4e44-bf3c-3b8e66dc9fd6",
   "metadata": {},
   "source": [
    "Now, let's check our feature_repo structure again after we ran `apply` and `materialize` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caba4e3-e6e0-4e2f-b51d-cd3456fd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./feature_repo\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── item_features.parquet\n",
      "│   ├── online_store.db\n",
      "│   ├── registry.db\n",
      "│   └── user_features.parquet\n",
      "├── feature_store.yaml\n",
      "├── item_features.py\n",
      "└── user_features.py\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./feature_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6b18a-40e0-4e4a-823e-19020c068d89",
   "metadata": {},
   "source": [
    "We use `configure_tensorflow` function to prevent the Tensorflow to claim entire GPU memory. With this func, we let TF to allocate 50% of the available GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b267f2-1d27-476c-b0cc-5564b429b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efada1e1-2556-4a26-b0ba-9cb96b3b151f",
   "metadata": {},
   "source": [
    "Create a folder for faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b7adc1-623b-41df-b1f9-dd4086a15bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking model, retrieval model, feast feature repo and fais index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Merlin/examples/PoC/\"\n",
    "faiss_index_path = base_path + 'tmp' + \"/index.faiss\"\n",
    "feast_repo_path = base_path + \"feature_repo/\"\n",
    "retrieval_model_path = base_path + \"query_tower/\"\n",
    "ranking_model_path = base_path + \"dlrm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b530f15-81c9-4c81-8962-c86ee0247245",
   "metadata": {},
   "source": [
    "Create a request schema that we are going to use when sending a request to Triton Infrence Server (TIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdda540-8209-49f9-8b6a-4b330570fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd5a53-79f4-40e1-baef-80d710ac2cef",
   "metadata": {},
   "source": [
    "`QueryFaiss` operator creates an interface between a FAISS Approximate Nearest Neighbors (ANN) Index and Triton Infrence Server. For a given input query vector, we do an ANN search query to find the ids of top-k nearby nodes in the index. \n",
    "\n",
    "`QueryFeast` operator is responsible for ensuring that our feast feature store can communicate correctly with tritonserver for the ensemble feast feature look ups.\n",
    "\n",
    "`setup_faiss` is  a utiltiy function that will create a Faiss index from an embedding vector with using L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec702617-d2fd-4d5b-8a00-8069e0f31274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "item_embeddings = np.ascontiguousarray(\n",
    "    pd.read_parquet(base_path + \"item_embeddings.parquet\").to_numpy()\n",
    ")\n",
    "\n",
    "feature_store = feast.FeatureStore(feast_repo_path)\n",
    "setup_faiss(item_embeddings, faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45df06-0cbe-4b52-ac1f-786e763895d7",
   "metadata": {},
   "source": [
    "Fetch user features with `QueryFeast` operator from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decbe7b-03e3-4978-baac-03f6a0b078c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e25be7-3ff0-49c2-a3fc-03ec4d615e77",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 00:40:16.934151: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 00:40:18.123955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:20.257596: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034311152 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2022 12:40:21 AM WARNING:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4429c-1fe1-4304-bcdf-badebe3b5485",
   "metadata": {},
   "source": [
    "Fetch item features for the candidate items that are retrieved from the retrieval step above from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4d09-db05-4666-b520-75dbbbc7ab17",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the \"unroll\" columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ef434-03a5-4a36-afb9-e19a43243c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0ce66-6b6c-43be-885e-a5435c3bbd9e",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to `PredictTensorflow()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 00:40:25.126858: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n",
      "2022-03-30 00:40:25.127006: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86fa47-de61-4007-ab55-9076e12ce963",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65598b-e3e7-4238-a73e-19d00c3deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"output_1\"], topk=10, temperature=20.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2e389-d884-44a1-8e32-4916a0eb43cf",
   "metadata": {},
   "source": [
    "### Export Graph as Ensemble\n",
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class  represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph.  After we create the ensemble object, we export the graph, supplying an export path for the `ensemble.export()` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc2e4f-5e58-4ad4-8ae5-d79ad286978f",
   "metadata": {},
   "source": [
    "Create the folder to export the models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c452f-543c-45a4-9995-130ca6919669",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"poc_ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = '/Merlin/examples/PoC/poc_ensemble/'\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eedd8-5dc0-4ad0-8725-c8da60fea693",
   "metadata": {},
   "source": [
    "Let's check our export_path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c81b2a-4fca-497b-8edf-5403fe5a483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./poc_ensemble\u001b[00m\n",
      "├── \u001b[01;34m0_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   │   └── model.cpython-38.pyc\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m1_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m2_queryfaiss\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mindex.faiss\u001b[00m\n",
      "│   │   │   └── index.faiss\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m3_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m4_unrollfeatures\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m5_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m6_softmaxsampling\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "└── \u001b[01;34mensemble_model\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    └── config.pbtxt\n",
      "\n",
      "24 directories, 23 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./poc_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fe264-e4a4-4dab-9b04-f83fb696d7d1",
   "metadata": {},
   "source": [
    "It is time to deploy the all the models as an ensemble model to Triton Inference very easily using Merlin Systems library. Now we can launch our triton server and load our models, and get a response for our query with a utility function `run_ensemble_on_tritonserver()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7896ec0-db89-4642-bfb6-eebf9afe77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 00:40:28.996325 1892 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "I0330 00:40:28.996416 1892 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:28.996421 1892 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:28.996425 1892 tensorflow.cc:2216] backend configuration:\n",
      "{\"cmdline\":{\"version\":\"2\"}}\n",
      "I0330 00:40:29.150263 1892 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f96b4000000' with size 268435456\n",
      "I0330 00:40:29.150648 1892 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0330 00:40:29.156027 1892 model_repository_manager.cc:994] loading: 0_queryfeast:1\n",
      "I0330 00:40:29.256479 1892 model_repository_manager.cc:994] loading: 1_predicttensorflow:1\n",
      "I0330 00:40:29.263985 1892 backend.cc:46] TRITONBACKEND_Initialize: nvtabular\n",
      "I0330 00:40:29.264019 1892 backend.cc:53] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:29.264031 1892 backend.cc:56] 'nvtabular' TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:29.264435 1892 backend.cc:76] Loaded libpython successfully\n",
      "I0330 00:40:29.356963 1892 model_repository_manager.cc:994] loading: 2_queryfaiss:1\n",
      "I0330 00:40:29.457427 1892 model_repository_manager.cc:994] loading: 3_queryfeast:1\n",
      "I0330 00:40:29.498327 1892 backend.cc:89] Python interpreter is initialized\n",
      "I0330 00:40:29.500531 1892 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 1_predicttensorflow (version 1)\n",
      "I0330 00:40:29.502226 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/0_queryfeast/1'\n",
      "I0330 00:40:29.557917 1892 model_repository_manager.cc:994] loading: 4_unrollfeatures:1\n",
      "I0330 00:40:29.658398 1892 model_repository_manager.cc:994] loading: 5_predicttensorflow:1\n",
      "I0330 00:40:29.758632 1892 model_repository_manager.cc:994] loading: 6_softmaxsampling:1\n",
      "I0330 00:40:31.637752 1892 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflow (GPU device 0)\n",
      "I0330 00:40:31.637888 1892 model_repository_manager.cc:1149] successfully loaded '0_queryfeast' version 1\n",
      "2022-03-30 00:40:32.773008: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:32.777114: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 00:40:32.777150: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:32.780174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:32.833536: W tensorflow/core/common_runtime/colocation_graph.cc:1218] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n",
      "  /job:localhost/replica:0/task:0/device:CPU:0].\n",
      "See below for details of this colocation group:\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and supported devices: \n",
      "Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n",
      "ReadVariableOp: GPU CPU \n",
      "VarHandleOp: CPU \n",
      "\n",
      "Colocation members, user-requested devices, and framework assigned devices, if any:\n",
      "  retrieval_model/sequential_block_20/item_id (VarHandleOp) /gpu:0\n",
      "  retrieval_model/sequential_block_20/item_id/Read/ReadVariableOp (ReadVariableOp) /gpu:0\n",
      "\n",
      "2022-03-30 00:40:32.833643: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 00:40:33.842139: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:33.872860: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 1099867 microseconds.\n",
      "I0330 00:40:33.873051 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/2_queryfaiss/1'\n",
      "I0330 00:40:33.873177 1892 model_repository_manager.cc:1149] successfully loaded '1_predicttensorflow' version 1\n",
      "/systems/merlin/systems/dag/ops/feast.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n",
      "/usr/local/lib/python3.8/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/usr/local/lib/python3.8/dist-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "03/30/2022 12:40:33 AM INFO:Loading faiss with AVX2 support.\n",
      "03/30/2022 12:40:33 AM INFO:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "03/30/2022 12:40:33 AM INFO:Loading faiss.\n",
      "03/30/2022 12:40:33 AM INFO:Successfully loaded faiss.\n",
      "I0330 00:40:34.504436 1892 model_repository_manager.cc:1149] successfully loaded '2_queryfaiss' version 1\n",
      "I0330 00:40:34.510268 1892 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 5_predicttensorflow (version 1)\n",
      "I0330 00:40:34.512085 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/3_queryfeast/1'\n",
      "I0330 00:40:34.528076 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/4_unrollfeatures/1'\n",
      "I0330 00:40:34.528133 1892 model_repository_manager.cc:1149] successfully loaded '3_queryfeast' version 1\n",
      "I0330 00:40:34.532301 1892 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 5_predicttensorflow (GPU device 0)\n",
      "I0330 00:40:34.532432 1892 model_repository_manager.cc:1149] successfully loaded '4_unrollfeatures' version 1\n",
      "2022-03-30 00:40:34.532746: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:34.549321: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 00:40:34.549362: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:34.551203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:34.575881: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 00:40:35.632967: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:35.689019: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 1156284 microseconds.\n",
      "I0330 00:40:35.689217 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/6_softmaxsampling/1'\n",
      "I0330 00:40:35.689308 1892 model_repository_manager.cc:1149] successfully loaded '5_predicttensorflow' version 1\n",
      "I0330 00:40:35.690666 1892 model_repository_manager.cc:1149] successfully loaded '6_softmaxsampling' version 1\n",
      "I0330 00:40:35.695317 1892 model_repository_manager.cc:994] loading: ensemble_model:1\n",
      "I0330 00:40:35.796054 1892 model_repository_manager.cc:1149] successfully loaded 'ensemble_model' version 1\n",
      "I0330 00:40:35.796301 1892 server.cc:522] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0330 00:40:35.796415 1892 server.cc:549] \n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| Backend    | Path                                                            | Config                      |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/tensorflow2/libtriton_tensorflow2.so | {\"cmdline\":{\"version\":\"2\"}} |\n",
      "| nvtabular  | /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so     | {}                          |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "\n",
      "I0330 00:40:35.796560 1892 server.cc:592] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| 0_queryfeast        | 1       | READY  |\n",
      "| 1_predicttensorflow | 1       | READY  |\n",
      "| 2_queryfaiss        | 1       | READY  |\n",
      "| 3_queryfeast        | 1       | READY  |\n",
      "| 4_unrollfeatures    | 1       | READY  |\n",
      "| 5_predicttensorflow | 1       | READY  |\n",
      "| 6_softmaxsampling   | 1       | READY  |\n",
      "| ensemble_model      | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I0330 00:40:35.870830 1892 metrics.cc:623] Collecting metrics for GPU 0: Quadro GV100\n",
      "I0330 00:40:35.871672 1892 tritonserver.cc:1932] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.19.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | /Merlin/examples/PoC/poc_ensemble/                                                                                                                                                           |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0330 00:40:35.883657 1892 grpc_server.cc:4375] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0330 00:40:35.885572 1892 http_server.cc:3075] Started HTTPService at 0.0.0.0:8000\n",
      "I0330 00:40:35.928305 1892 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal (2) received.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 00:40:44.842471 1892 server.cc:252] Waiting for in-flight requests to complete.\n",
      "I0330 00:40:44.842502 1892 model_repository_manager.cc:1026] unloading: ensemble_model:1\n",
      "I0330 00:40:44.842628 1892 model_repository_manager.cc:1026] unloading: 6_softmaxsampling:1\n",
      "I0330 00:40:44.842775 1892 model_repository_manager.cc:1026] unloading: 5_predicttensorflow:1\n",
      "I0330 00:40:44.842846 1892 model_repository_manager.cc:1132] successfully unloaded 'ensemble_model' version 1\n",
      "I0330 00:40:44.842869 1892 model_repository_manager.cc:1026] unloading: 4_unrollfeatures:1\n",
      "I0330 00:40:44.843011 1892 model_repository_manager.cc:1026] unloading: 3_queryfeast:1\n",
      "I0330 00:40:44.843074 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0330 00:40:44.843092 1892 model_repository_manager.cc:1026] unloading: 2_queryfaiss:1\n",
      "Signal (I0330 00:40:44.843153 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 00:40:44.843172 1892 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 00:40:44.843189 1892 model_repository_manager.cc:1026] unloading: 1_predicttensorflow:111\n",
      ") received.\n",
      "\n",
      "I0330 00:40:44.843277 1892 model_repository_manager.cc:1026] unloading: 0_queryfeast:1\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843299 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.I0330 00:40:44.843368 1892 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:44.843487 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843547 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843600 1892 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model stateI0330 00:40:44.843606 1892 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "I0330 00:40:44.843731 1892 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0330 00:40:45.853598 1892 server.cc:267] Timeout 29: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:46.942861 1892 server.cc:267] Timeout 28: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:47.972121 1892 server.cc:267] Timeout 27: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:49.004065 1892 server.cc:267] Timeout 26: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:50.037075 1892 server.cc:267] Timeout 25: Found 7 live models and 0 in-flight non-inference requests\n",
      " 0# 0x0000564A8941E299 in /opt/tritonserver/bin/tritonserver\n",
      " 1# 0x00007F9748ED3210 in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      " 2# 0x00007F96F28BDF2E in /usr/lib/x86_64-linux-gnu/libpython3.8.so.1.0\n",
      " 3# TRITONBACKEND_ModelInstanceFinalize in /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so\n",
      " 4# 0x00007F9749A70FC4 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 5# 0x00007F9749A6A3B9 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 6# 0x00007F9749A6AB1D in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 7# 0x00007F97498EE0D7 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 8# 0x00007F97492C1DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
      " 9# 0x00007F974973F609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\n",
      "10# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a request to be sent to TIS\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "request = make_df({\"user_id\": [1]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "\n",
    "response = run_ensemble_on_tritonserver(\n",
    "    export_path, ensemble.graph.output_schema.column_names, request, \"ensemble_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840b471-8153-4d5f-82f8-f77614470ca4",
   "metadata": {},
   "source": [
    "Convert our response to a numpy array and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583e6354-183a-4dae-8533-bfc643d4452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2628383],\n",
       "       [1780891],\n",
       "       [ 397955],\n",
       "       [2573239],\n",
       "       [1255680],\n",
       "       [ 505277],\n",
       "       [2084603],\n",
       "       [ 365618],\n",
       "       [ 229051],\n",
       "       [1323574]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output= response.as_numpy('ordered_ids')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "Note that these item ids are encoded values, not the raw original values. We will eventually create the reverse dictionary lookup functionality to be able to map these encoded item ids to their original raw ids with one-line of code. But if you really want to do it now, you can easily map these ids to their original values using the `unique.item_id.parquet` file stored in the `categories` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6445f-7fc0-40e1-8d9a-21749c8acd1e",
   "metadata": {},
   "source": [
    "That's it! You finished deploying a multi-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
