{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Deploying a Multi-Stage RecSys into Production with Merlin Systems and Triton Inference Server\n",
    "\n",
    "At this point, when you reach out to this notebook, we expect that you have already executed the first notebook `01-Building-Recommender-Systems-with-Merlin.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![tritonensemble](../images/triton_ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "Merlin Systems library have the set of operators to be able to serve multi-stage recommender systems built with Tensorflow on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently. Below, we will go through these operators and demonstrate their usage in serving a multi-stage system on Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3756f8-a115-436a-b5d4-48f0641451b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow \"feast<0.20\" faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "import faiss\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import run_triton_server, run_ensemble_on_tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356b2ed-3d94-4165-b311-9560212e55d3",
   "metadata": {},
   "source": [
    "We use `configure_tensorflow` function to prevent the Tensorflow to claim entire GPU memory. With this func, we let TF to allocate 50% of the available GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d75d74-b513-451d-b72d-790fef388e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/05/2022 06:52:55 PM INFO:init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ead20e-c573-462e-9aa2-c3494bf0129f",
   "metadata": {},
   "source": [
    "### Register our features on feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac115e-4794-4a69-a962-8481f6e86df3",
   "metadata": {},
   "source": [
    "The Feast feature registry is a central catalog of all the feature definitions and their related metadata(read more [here](https://docs.feast.dev/getting-started/architecture-and-components/registry)). We have defined our user and item features definitions in the `user_features.py` and  `item_features.py` files. With FeatureView() users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling Feast where to find user and item features.\n",
    "\n",
    "Before we move on to the next steps, we need to perform `feast apply`command as directed below.  With that, we register our features, we can apply the changes to create our feature registry and store all entity and feature view definitions in a local SQLite online store called `online_store.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c02d67-df45-4869-8262-647cba77efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/Merlin/examples/Deploying-multi-stage-RecSys/\")\n",
    "\n",
    "# define feature repo path\n",
    "feast_repo_path = BASE_DIR + \"feature_repo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fa545b-a979-4216-b176-ffd70d66e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\n",
      "/usr/local/lib/python3.8/dist-packages/feast/feature_view.py:100: DeprecationWarning: The argument 'input' is being deprecated. Please use 'batch_source' instead. Feast 0.13 and onwards will not support the argument 'input'.\n",
      "  warnings.warn(\n",
      "Created data source \u001b[1m\u001b[32m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/user_features.parquet\u001b[0m\n",
      "Created data source \u001b[1m\u001b[32m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/item_features.parquet\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mitem_id\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32muser_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mitem_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32muser_features\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mfeature_repo_item_features\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mfeature_repo_user_features\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd $feast_repo_path\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641fcd2-bd11-4569-80d4-2ae5e01a5cad",
   "metadata": {},
   "source": [
    "### Loading features from offline store into an online store \n",
    "\n",
    "After we execute `apply` and registered our features and created our online local store, now we need to perform [materialization](https://docs.feast.dev/how-to-guides/running-feast-in-production) operation. This is done to keep our online store up to date and get it ready for prediction. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store up to date by finding the latest feature values for each user. \n",
    "\n",
    "When you run the `feast materialize ..` command below, you will see a message <i>Materializing 2 feature views from 1995-01-01 01:01:01+00:00 to 2025-01-01 01:01:01+00:00 into the sqlite online store </i>  will be printed out.\n",
    "\n",
    "Note that materialization step takes some time.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dacbbc-bdb6-4f7a-b202-3802050f0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m1995-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m:\n",
      "100%|█████████████████████████████████████████████████████████| 1298/1298 [00:00<00:00, 5214.52it/s]\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m:\n",
      "100%|█████████████████████████████████████████████████████████| 1322/1322 [00:00<00:00, 1621.24it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc26e6-f6f3-4e44-bf3c-3b8e66dc9fd6",
   "metadata": {},
   "source": [
    "Now, let's check our feature_repo structure again after we ran `apply` and `materialize` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9caba4e3-e6e0-4e2f-b51d-cd3456fd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── item_features.parquet\n",
      "│   ├── online_store.db\n",
      "│   ├── registry.db\n",
      "│   └── user_features.parquet\n",
      "├── feature_store.yaml\n",
      "├── item_features.py\n",
      "└── user_features.py\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "# set up the base dir to for feature store\n",
    "feature_repo_path = os.path.join(BASE_DIR, 'feature_repo')\n",
    "!tree $feature_repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "### Set up Faiss index, create feature store client and objects for the Triton ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efada1e1-2556-4a26-b0ba-9cb96b3b151f",
   "metadata": {},
   "source": [
    "Create a folder for faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b7adc1-623b-41df-b1f9-dd4086a15bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR + 'faiss_index')):\n",
    "    os.makedirs(os.path.join(BASE_DIR + 'faiss_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking model, retrieval model, and faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = BASE_DIR + 'faiss_index' + \"/index.faiss\"\n",
    "retrieval_model_path = BASE_DIR + \"query_tower/\"\n",
    "ranking_model_path = BASE_DIR + \"dlrm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b530f15-81c9-4c81-8962-c86ee0247245",
   "metadata": {},
   "source": [
    "Create a request schema that we are going to use when sending a request to Triton Infrence Server (TIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cdda540-8209-49f9-8b6a-4b330570fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b996019-bd2a-44e0-b004-4f412b300d63",
   "metadata": {},
   "source": [
    "`QueryFaiss` operator creates an interface between a FAISS Approximate Nearest Neighbors (ANN) Index and Triton Infrence Server. For a given input query vector, we do an ANN search query to find the ids of top-k nearby nodes in the index.\n",
    "\n",
    "`setup_faiss` is  a utility function that will create a Faiss index from an embedding vector with using L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6cc5bf-d07c-4963-a748-6e2b4827ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "\n",
    "item_embeddings = np.ascontiguousarray(\n",
    "    pd.read_parquet(BASE_DIR + \"item_embeddings.parquet\").to_numpy()\n",
    ")\n",
    "setup_faiss(item_embeddings, faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46697177-512a-473e-8cca-9fe51d3daa03",
   "metadata": {},
   "source": [
    "Create feature store client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc00e04-c70c-4882-9952-66f4dbb97bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(feast_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45df06-0cbe-4b52-ac1f-786e763895d7",
   "metadata": {},
   "source": [
    "Fetch user features with `QueryFeast` operator from the feature store. `QueryFeast` operator is responsible for ensuring that our feast feature store can communicate correctly with tritonserver for the ensemble feast feature look ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3decbe7b-03e3-4978-baac-03f6a0b078c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/systems/merlin/systems/dag/ops/feast.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "user_features = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e25be7-3ff0-49c2-a3fc-03ec4d615e77",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 18:53:03.680149: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-05 18:53:04.788233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "04/05/2022 06:53:06 PM WARNING:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4429c-1fe1-4304-bcdf-badebe3b5485",
   "metadata": {},
   "source": [
    "Fetch item features for the candidate items that are retrieved from the retrieval step above from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4d09-db05-4666-b520-75dbbbc7ab17",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the \"unroll\" columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0ef434-03a5-4a36-afb9-e19a43243c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0ce66-6b6c-43be-885e-a5435c3bbd9e",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to `PredictTensorflow()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86fa47-de61-4007-ab55-9076e12ce963",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f65598b-e3e7-4238-a73e-19d00c3deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"output_1\"], topk=10, temperature=20.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2e389-d884-44a1-8e32-4916a0eb43cf",
   "metadata": {},
   "source": [
    "### Export Graph as Ensemble\n",
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class  represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph.  After we create the ensemble object, we export the graph, supplying an export path for the `ensemble.export()` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc2e4f-5e58-4ad4-8ae5-d79ad286978f",
   "metadata": {},
   "source": [
    "Create the folder to export the models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b28c452f-543c-45a4-9995-130ca6919669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR + 'poc_ensemble')):\n",
    "    os.makedirs(os.path.join(BASE_DIR + 'poc_ensemble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join(BASE_DIR + 'poc_ensemble')\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eedd8-5dc0-4ad0-8725-c8da60fea693",
   "metadata": {},
   "source": [
    "Let's check our export_path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3c81b2a-4fca-497b-8edf-5403fe5a483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble\u001b[00m\n",
      "├── \u001b[01;34m0_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m1_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m2_queryfaiss\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mindex.faiss\u001b[00m\n",
      "│   │   │   └── index.faiss\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m3_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m4_unrollfeatures\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m5_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m6_softmaxsampling\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "└── \u001b[01;34mensemble_model\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    └── config.pbtxt\n",
      "\n",
      "23 directories, 22 files\n"
     ]
    }
   ],
   "source": [
    "!tree $export_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fe264-e4a4-4dab-9b04-f83fb696d7d1",
   "metadata": {},
   "source": [
    "It is time to deploy the all the models as an ensemble model to Triton Inference very easily using Merlin Systems library. Now we can launch our triton server and load our models, and get a response for our query with a utility function `run_ensemble_on_tritonserver()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7896ec0-db89-4642-bfb6-eebf9afe77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0405 18:53:12.285197 12809 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "I0405 18:53:12.285284 12809 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.8\n",
      "I0405 18:53:12.285289 12809 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.8\n",
      "I0405 18:53:12.285293 12809 tensorflow.cc:2216] backend configuration:\n",
      "{\"cmdline\":{\"version\":\"2\"}}\n",
      "I0405 18:53:12.434720 12809 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fd4f8000000' with size 268435456\n",
      "I0405 18:53:12.435114 12809 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0405 18:53:12.440615 12809 model_repository_manager.cc:994] loading: 0_queryfeast:1\n",
      "I0405 18:53:12.541023 12809 model_repository_manager.cc:994] loading: 1_predicttensorflow:1\n",
      "I0405 18:53:12.544542 12809 backend.cc:46] TRITONBACKEND_Initialize: nvtabular\n",
      "I0405 18:53:12.544572 12809 backend.cc:53] Triton TRITONBACKEND API version: 1.8\n",
      "I0405 18:53:12.544585 12809 backend.cc:56] 'nvtabular' TRITONBACKEND API version: 1.8\n",
      "I0405 18:53:12.544858 12809 backend.cc:76] Loaded libpython successfully\n",
      "I0405 18:53:12.641295 12809 model_repository_manager.cc:994] loading: 2_queryfaiss:1\n",
      "I0405 18:53:12.712613 12809 backend.cc:89] Python interpreter is initialized\n",
      "I0405 18:53:12.713774 12809 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 1_predicttensorflow (version 1)\n",
      "I0405 18:53:12.715248 12809 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/0_queryfeast/1'\n",
      "I0405 18:53:12.743586 12809 model_repository_manager.cc:994] loading: 3_queryfeast:1\n",
      "I0405 18:53:12.844174 12809 model_repository_manager.cc:994] loading: 4_unrollfeatures:1\n",
      "I0405 18:53:12.944631 12809 model_repository_manager.cc:994] loading: 5_predicttensorflow:1\n",
      "I0405 18:53:13.044995 12809 model_repository_manager.cc:994] loading: 6_softmaxsampling:1\n",
      "I0405 18:53:14.728975 12809 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflow (GPU device 0)\n",
      "I0405 18:53:14.729115 12809 model_repository_manager.cc:1149] successfully loaded '0_queryfeast' version 1\n",
      "2022-04-05 18:53:15.913993: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:15.919091: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-05 18:53:15.919132: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:15.925459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-04-05 18:53:15.972201: W tensorflow/core/common_runtime/colocation_graph.cc:1218] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n",
      "  /job:localhost/replica:0/task:0/device:CPU:0].\n",
      "See below for details of this colocation group:\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and supported devices: \n",
      "Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n",
      "ReadVariableOp: GPU CPU \n",
      "VarHandleOp: CPU \n",
      "\n",
      "Colocation members, user-requested devices, and framework assigned devices, if any:\n",
      "  retrieval_model/sequential_block_20/item_id (VarHandleOp) /gpu:0\n",
      "  retrieval_model/sequential_block_20/item_id/Read/ReadVariableOp (ReadVariableOp) /gpu:0\n",
      "\n",
      "2022-04-05 18:53:15.972306: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-04-05 18:53:16.051512: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:16.076336: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 162361 microseconds.\n",
      "I0405 18:53:16.076597 12809 model_repository_manager.cc:1149] successfully loaded '1_predicttensorflow' version 1\n",
      "I0405 18:53:16.079944 12809 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 5_predicttensorflow (version 1)\n",
      "I0405 18:53:16.082241 12809 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/2_queryfaiss/1'\n",
      "/systems/merlin/systems/dag/ops/feast.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n",
      "/usr/local/lib/python3.8/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/usr/local/lib/python3.8/dist-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "04/05/2022 06:53:16 PM INFO:Loading faiss with AVX2 support.\n",
      "04/05/2022 06:53:16 PM INFO:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "04/05/2022 06:53:16 PM INFO:Loading faiss.\n",
      "04/05/2022 06:53:16 PM INFO:Successfully loaded faiss.\n",
      "I0405 18:53:16.110800 12809 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/3_queryfeast/1'\n",
      "I0405 18:53:16.111056 12809 model_repository_manager.cc:1149] successfully loaded '2_queryfaiss' version 1\n",
      "I0405 18:53:16.119599 12809 model_repository_manager.cc:1149] successfully loaded '3_queryfeast' version 1\n",
      "I0405 18:53:16.124341 12809 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/4_unrollfeatures/1'\n",
      "I0405 18:53:16.126394 12809 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 5_predicttensorflow (GPU device 0)\n",
      "I0405 18:53:16.126561 12809 model_repository_manager.cc:1149] successfully loaded '4_unrollfeatures' version 1\n",
      "2022-04-05 18:53:16.127020: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:16.148400: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-05 18:53:16.148442: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:16.150520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-04-05 18:53:16.176360: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-04-05 18:53:16.330717: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-04-05 18:53:16.383740: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 256733 microseconds.\n",
      "I0405 18:53:16.383873 12809 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/6_softmaxsampling/1'\n",
      "I0405 18:53:16.383965 12809 model_repository_manager.cc:1149] successfully loaded '5_predicttensorflow' version 1\n",
      "I0405 18:53:16.385192 12809 model_repository_manager.cc:1149] successfully loaded '6_softmaxsampling' version 1\n",
      "I0405 18:53:16.389471 12809 model_repository_manager.cc:994] loading: ensemble_model:1\n",
      "I0405 18:53:16.490208 12809 model_repository_manager.cc:1149] successfully loaded 'ensemble_model' version 1\n",
      "I0405 18:53:16.490451 12809 server.cc:522] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0405 18:53:16.490576 12809 server.cc:549] \n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| Backend    | Path                                                            | Config                      |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/tensorflow2/libtriton_tensorflow2.so | {\"cmdline\":{\"version\":\"2\"}} |\n",
      "| nvtabular  | /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so     | {}                          |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "\n",
      "I0405 18:53:16.490758 12809 server.cc:592] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| 0_queryfeast        | 1       | READY  |\n",
      "| 1_predicttensorflow | 1       | READY  |\n",
      "| 2_queryfaiss        | 1       | READY  |\n",
      "| 3_queryfeast        | 1       | READY  |\n",
      "| 4_unrollfeatures    | 1       | READY  |\n",
      "| 5_predicttensorflow | 1       | READY  |\n",
      "| 6_softmaxsampling   | 1       | READY  |\n",
      "| ensemble_model      | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I0405 18:53:16.546009 12809 metrics.cc:623] Collecting metrics for GPU 0: Quadro GV100\n",
      "I0405 18:53:16.546474 12809 tritonserver.cc:1932] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.19.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble                                                                                                                                   |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0405 18:53:16.547652 12809 grpc_server.cc:4375] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0405 18:53:16.548288 12809 http_server.cc:3075] Started HTTPService at 0.0.0.0:8000\n",
      "I0405 18:53:16.590481 12809 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "I0405 18:53:22.789904 12809 server.cc:252] Waiting for in-flight requests to complete.\n",
      "I0405 18:53:22.789934 12809 model_repository_manager.cc:1026] unloading: ensemble_model:1\n",
      "I0405 18:53:22.790047 12809 model_repository_manager.cc:1026] unloading: 6_softmaxsampling:1\n",
      "I0405 18:53:22.790157 12809 model_repository_manager.cc:1026] unloading: 5_predicttensorflow:1\n",
      "I0405 18:53:22.790212 12809 model_repository_manager.cc:1132] successfully unloaded 'ensemble_model' version 1\n",
      "I0405 18:53:22.790326 12809 model_repository_manager.cc:1026] unloading: 4_unrollfeatures:1I0405 18:53:22.790324 12809 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.\n",
      "I0405 18:53:22.790397 12809 model_repository_manager.cc:1026] unloading: 3_queryfeast:1\n",
      "I0405 18:53:22.790436 12809 model_repository_manager.cc:1026] unloading: 2_queryfaiss:1\n",
      "I0405 18:53:22.790500 12809 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0405 18:53:22.790501 12809 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0405 18:53:22.790543 12809 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.\n",
      "Signal (11) received.\n",
      "I0405 18:53:22.790619 12809 model_repository_manager.cc:1026] unloading: 1_predicttensorflow:1\n",
      "I0405 18:53:22.790695 12809 model_repository_manager.cc:1026] unloading: 0_queryfeast:1\n",
      "I0405 18:53:22.790803 12809 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0405 18:53:22.790697 12809 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0405 18:53:22.791279 12809 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0405 18:53:22.791393 12809 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0405 18:53:22.818461 12809 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0405 18:53:22.847960 12809 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal (2) received.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0405 18:53:23.800413 12809 server.cc:267] Timeout 29: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0405 18:53:24.811206 12809 server.cc:267] Timeout 28: Found 7 live models and 0 in-flight non-inference requests\n",
      " 0# 0x0000555EA4C27299 in /opt/tritonserver/bin/tritonserver\n",
      " 1# 0x00007FD58C687210 in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      " 2# 0x00007FD531CF2F2E in /usr/lib/x86_64-linux-gnu/libpython3.8.so.1.0\n",
      " 3# TRITONBACKEND_ModelInstanceFinalize in /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so\n",
      " 4# 0x00007FD58D224FC4 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 5# 0x00007FD58D21E3B9 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 6# 0x00007FD58D21EB1D in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 7# 0x00007FD58D0A20D7 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 8# 0x00007FD58CA75DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
      " 9# 0x00007FD58CEF3609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\n",
      "10# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a request to be sent to TIS\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "request = make_df({\"user_id\": [1]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "\n",
    "response = run_ensemble_on_tritonserver(\n",
    "    export_path, ensemble.graph.output_schema.column_names, request, \"ensemble_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840b471-8153-4d5f-82f8-f77614470ca4",
   "metadata": {},
   "source": [
    "Convert our response to a numpy array and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "583e6354-183a-4dae-8533-bfc643d4452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 392],\n",
       "       [ 267],\n",
       "       [1107],\n",
       "       [ 968],\n",
       "       [ 457],\n",
       "       [ 750],\n",
       "       [ 669],\n",
       "       [ 789],\n",
       "       [1237],\n",
       "       [1164]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output= response.as_numpy('ordered_ids')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "Note that these item ids are encoded values, not the raw original values. We will eventually create the reverse dictionary lookup functionality to be able to map these encoded item ids to their original raw ids with one-line of code. But if you really want to do it now, you can easily map these ids to their original values using the `unique.item_id.parquet` file stored in the `categories` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6445f-7fc0-40e1-8d9a-21749c8acd1e",
   "metadata": {},
   "source": [
    "That's it! You finished deploying a multi-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
