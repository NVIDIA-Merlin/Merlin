# syntax=docker/dockerfile:1.2
ARG TRITON_VERSION=22.03
ARG IMAGE=nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-py3
FROM ${IMAGE}

# Args
ARG CUDF_VER=v21.12.02
ARG HWLOC_VER=2.4.1
ARG RMM_VER=v21.12.00
ARG CORE_VER=main
ARG HUGECTR_VER=master
ARG HUGECTR_BACKEND_VER=main
ARG MODELS_VER=main
ARG NVTAB_VER=main
ARG NVTAB_BACKEND_VER=main
ARG SYSTEMS_VER=main
ARG TF4REC_VER=main

# Envs
ENV CUDA_SHORT_VERSION=11.6
ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV CUDA_PATH=$CUDA_HOME
ENV CUDA_CUDA_LIBRARY=${CUDA_HOME}/lib64/stubs
ENV PATH=${CUDA_HOME}/lib64/:${PATH}:${CUDA_HOME}/bin
ENV PATH=$PATH:/usr/lib/x86_64-linux-gnu/
ENV PYTHONPATH=/usr/lib/python3.8/site-packages:$PYTHONPATH
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1

# Install packages
ENV DEBIAN_FRONTEND=noninteractive
RUN apt update -y --fix-missing && \
    apt install -y --no-install-recommends software-properties-common && \
    apt-get install -y --no-install-recommends \
        clang-format \
        curl \
        libaio-dev \
        libboost-serialization-dev \
        libcurl4-openssl-dev \
        libssl-dev \
        libtbb-dev \
        pandoc \
        protobuf-compiler \
        python3-dev \
        python3-pip \
        rapidjson-dev \
        zip &&\
    apt-get autoremove -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN ln -s /usr/bin/python3 /usr/bin/python

# Install multiple packages
RUN pip install cupy-cuda115 nvidia-pyindex pybind11 pytest protobuf transformers==4.12 tensorflow-metadata 
RUN pip install betterproto cachetools graphviz nvtx scipy sklearn matplotlib
RUN pip install pandas numba==0.55.1 numpy==1.21.5
RUN pip install tritonclient[all] grpcio-channelz
RUN pip install dask==2021.11.2 distributed==2021.11.2 dask[dataframe]==2021.11.2 dask-cuda
RUN pip install git+https://github.com/rapidsai/asvdb.git@main
RUN pip install pip install tensorflow-gpu mpi4py
RUN pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
RUN pip install torchmetrics
RUN pip install --no-deps fastai fastcore fastprogress fastdownload
RUN CC=/usr/bin/gcc CXX=/usr/bin/g++ HOROVOD_CUDA_HOME=/usr/local/cuda/ HOROVOD_BUILD_CUDA_CC_LIST=60,70,75,80 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_TENSORFLOW=1 HOROVOD_NCCL_LINK=SHARED pip install --no-cache-dir git+https://github.com/horovod/horovod.git@master


# Install cmake
RUN apt remove --purge cmake -y && wget http://www.cmake.org/files/v3.21/cmake-3.21.1.tar.gz && \
    tar xf cmake-3.21.1.tar.gz && cd cmake-3.21.1 && ./configure && make && make install

# Install spdlog
RUN git clone --branch v1.9.2 https://github.com/gabime/spdlog.git build-env && \
    pushd build-env && \
      mkdir build && cd build && cmake .. && make -j && make install && \
    popd && \
    rm -rf build-env
   
# Install arrow
ENV ARROW_HOME=/usr/local
RUN git clone --branch apache-arrow-5.0.0 --recurse-submodules https://github.com/apache/arrow.git build-env && \
    pushd build-env && \
      export PARQUET_TEST_DATA="${PWD}/cpp/submodules/parquet-testing/data" && \
      export ARROW_TEST_DATA="${PWD}/testing/data" && \
      pip install -r python/requirements-build.txt && \
      mkdir cpp/release && \
      pushd cpp/release && \
        cmake -DCMAKE_INSTALL_PREFIX=${ARROW_HOME} \
              -DCMAKE_INSTALL_LIBDIR=lib \
              -DCMAKE_LIBRARY_PATH=${CUDA_CUDA_LIBRARY} \
              -DARROW_FLIGHT=ON \
              -DARROW_GANDIVA=OFF \
              -DARROW_ORC=ON \
              -DARROW_WITH_BZ2=ON \
              -DARROW_WITH_ZLIB=ON \
              -DARROW_WITH_ZSTD=ON \
              -DARROW_WITH_LZ4=ON \
              -DARROW_WITH_SNAPPY=ON \
              -DARROW_WITH_BROTLI=ON \
              -DARROW_PARQUET=ON \
              -DARROW_PYTHON=ON \
              -DARROW_PLASMA=ON \
              -DARROW_BUILD_TESTS=ON \
              -DARROW_CUDA=ON \
              -DARROW_DATASET=ON \
              -DARROW_HDFS=ON \
              -DARROW_S3=ON \ 
              .. && \
        make -j$(nproc) && \
        make install && \
      popd && \
      pushd python && \
        export PYARROW_WITH_PARQUET=ON && \
        export PYARROW_WITH_CUDA=ON && \
        export PYARROW_WITH_ORC=ON && \
        export PYARROW_WITH_DATASET=ON && \
        export PYARROW_WITH_S3=ON && \
        export PYARROW_WITH_HDFS=ON && \
        python setup.py build_ext --build-type=release bdist_wheel && \
        pip install dist/*.whl --no-deps --force-reinstall && \
      popd && \
    popd && \
    rm -rf build-env

# Install rmm
ENV INSTALL_PREFIX=/usr
RUN git clone https://github.com/rapidsai/rmm.git build-env && cd build-env/ && \
    git checkout ${RMM_VER} && \
    cd ..; \
    pushd build-env && \
    ./build.sh librmm && \
    pip install python/. --no-deps && \
    popd && \
    rm -rf build-env

# Install CUDF
RUN git clone https://github.com/rapidsai/cudf.git build-env && cd build-env/ && \
    git checkout ${CUDF_VER} && \
    git submodule update --init --recursive && \
    cd .. && \
    pushd build-env && \
      export CUDF_HOME=${PWD} && \
      export CUDF_ROOT=${PWD}/cpp/build/ && \
      export CMAKE_LIBRARY_PATH=${CUDA_CUDA_LIBRARY} && \
      export CUDAFLAGS=-Wno-error=unknown-pragmas && \
      ./build.sh libcudf cudf dask_cudf --allgpuarch --cmake-args=\"-DCUDF_ENABLE_ARROW_S3=OFF\" && \
    popd && \
    rm -rf build-env

# Install CUDA-Aware hwloc
RUN cd /opt/hpcx/ompi/include/openmpi/opal/mca/hwloc/hwloc201 && rm -rfv hwloc201.h hwloc/include/hwloc.h
RUN mkdir -p /var/tmp && wget -q -nc --no-check-certificate -P /var/tmp https://download.open-mpi.org/release/hwloc/v2.4/hwloc-${HWLOC_VER}.tar.gz && \
    mkdir -p /var/tmp && tar -x -f /var/tmp/hwloc-${HWLOC_VER}.tar.gz -C /var/tmp && \
    cd /var/tmp/hwloc-${HWLOC_VER} && \
    ./configure CPPFLAGS="-I/usr/local/cuda/include/ -L/usr/local/cuda/lib64/" LDFLAGS="-L/usr/local/cuda/lib64" --enable-cuda && \
    make -j$(nproc) && make install && \
    rm -rf /var/tmp/hwloc-${HWLOC_VER} /var/tmp/hwloc-${HWLOC_VER}.tar.gz

# Install Hiredis
RUN mkdir -p /var/tmp && cd /var/tmp && git clone --depth=1 https://github.com/redis/hiredis.git && cd - && \
    cd /var/tmp/hiredis && \
    mkdir build && cd build && \
    cmake .. && \
    make -j$(nproc) && make install && \
    rm -rf /var/tmp/hiredis

# Install redis++
RUN mkdir -p /var/tmp && cd /var/tmp && git clone --depth=1 -b 1.3.2 https://github.com/sewenew/redis-plus-plus.git && cd - && \
    cd /var/tmp/redis-plus-plus && \
    mkdir build && cd build && \
    cmake -DREDIS_PLUS_PLUS_CXX_STANDARD=17 .. && \
    make -j$(nproc) && make install && \
    rm -rf /var/tmp/redis-plus-plus

# Install RocksDB
RUN mkdir -p /var/tmp && cd /var/tmp && git clone --depth=1 https://github.com/facebook/rocksdb.git && cd - && \
    cd /var/tmp/rocksdb && \
    PORTABLE=1 make -j$(nproc) shared_lib && \
    make install-shared && \
    rm -rf /var/tmp/rocksdb
    
# Install LibRdKafka
RUN apt-get update -y && \
    DEBIAN_FRONTEND=noninteractive && apt-get install -y --no-install-recommends libssl-dev libsasl2-dev liblz4-dev libzstd-dev && \
    mkdir -p /var/tmp && cd /var/tmp && git clone --depth 1 -b v1.8.2 https://github.com/edenhill/librdkafka.git && cd - && \
    cd /var/tmp/librdkafka && \
    ./configure && make -j$(nproc) && make install && \
    rm -rf /var/tmp/librdkafka

ARG INSTALL_HDFS=false
RUN if [ "$INSTALL_HDFS" == "true" ]; then \
    mkdir -p /var/tmp && cd /var/tmp && wget https://download.java.net/java/GA/jdk16.0.2/d4a915d82b4c4fbb9bde534da945d746/7/GPL/openjdk-16.0.2_linux-x64_bin.tar.gz && \
    mkdir -p /usr/java && tar -zxvf ./openjdk-16.0.2_linux-x64_bin.tar.gz -C /usr/java &&  \
    rm -rf ./openjdk-16.0.2_linux-x64_bin.tar.gz && \
    mkdir -p /var/tmp && cd /var/tmp && wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz && \
    tar -zxvf ./hadoop-3.3.1.tar.gz && rm -rf hadoop-3.3.1.tar.gz && \
    cp ./hadoop-3.3.1/lib/native/libhdfs.so.0.0.0 /usr/local/lib/ && cp hadoop-3.3.1/include/hdfs.h /usr/local/include/ && \
    mv ./hadoop-3.3.1 /usr/local/hadoop && cd /usr/local/lib/ && ln -s libhdfs.so.0.0.0 libhdfs.so; \
fi

ENV JAVA_HOME=/usr/java/jdk-16.0.2
ENV PATH=$JAVA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$JAVA_HOME/lib/server
ENV HADOOP_HOME=/usr/local/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Install HugeCTR - Training
ENV CPATH=/usr/local/include:$CPATH

ENV OMPI_MCA_plm_rsh_agent=sh
ENV OMPI_MCA_opal_cuda_support=true

ENV NCCL_LAUNCH_MODE=PARALLEL
ENV NCCL_COLLNET_ENABLE=0

ENV SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD=0
ENV SHARP_COLL_LOCK_ON_COMM_INIT=1
ENV SHARP_COLL_LOG_LEVEL=3
ENV HCOLL_ENABLE_MCAST=0

# link sub modules expected by hugectr cmake
RUN ln -s /usr/lib/libcudf.so /usr/lib/libcudf_base.so
RUN ln -s /usr/lib/libcudf.so /usr/lib/libcudf_io.so
RUN ln -s /usr/lib/x86_64-linux-gnu/libibverbs.so.1 /usr/lib/x86_64-linux-gnu/libibverbs.so

RUN rm -rf /usr/lib/x86_64-linux-gnu/libibverbs.so && \
    ln -s /usr/lib/x86_64-linux-gnu/libibverbs.so.1.14.36.0 /usr/lib/x86_64-linux-gnu/libibverbs.so

RUN git clone https://github.com/NVIDIA-Merlin/HugeCTR.git /hugectr && cd /hugectr && git checkout ${HUGECTR_VER} && \
    git submodule update --init --recursive && \
    mkdir build && cd build && \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs/:$LD_LIBRARY_PATH && \
    export PATH=$PATH:/usr/local/cuda-${CUDA_SHORT_VERSION}/compat/ && \
    cmake -DCMAKE_CXX_COMPILER=/usr/bin/g++ -DCMAKE_C_COMPILER=/usr/bin/gcc -DCMAKE_BUILD_TYPE=Release -DSM="60;61;70;75;80" \
        -DENABLE_MULTINODES=ON .. && \
    make -j && make install && \
    chmod +x /usr/local/hugectr/bin/* && \
    chmod +x /usr/local/hugectr/lib/* && \
    cd /hugectr/onnx_converter && \
    python setup.py install && \
    rm -rf /hugectr

ENV CPATH=/usr/local/hugectr/include:$CPATH
ENV PATH=/usr/local/hugectr/bin:$PATH
ENV LIBRARY_PATH=/usr/local/hugectr/lib:$LIBRARY_PATH
ENV LD_LIBRARY_PATH=/usr/local/hugectr/lib:$LD_LIBRARY_PATH
ENV PYTHONPATH=/usr/local/hugectr/lib:$PYTHONPATH
ENV PYTHONPATH=/hugectr/onnx_converter:$PYTHONPATH

# Install HugeCTR - TF Plugin
RUN git clone https://github.com/NVIDIA-Merlin/HugeCTR.git /hugectr && cd /hugectr && git checkout ${HUGECTR_VER} && \
    cd sparse_operation_kit && \
    python setup.py install && \
    rm -rf /hugectr

# Install HugeCTR - Inference
RUN git clone https://github.com/NVIDIA-Merlin/HugeCTR.git /hugectr && cd /hugectr && git checkout ${HUGECTR_VER} && \
    git submodule update --init --recursive && \
    mkdir -p build && cd build &&\
    cmake -DCMAKE_BUILD_TYPE=Release -DSM="60;61;70;75;80" -DENABLE_INFERENCE=ON .. && \
    make -j && make install && \
    chmod +x /usr/local/hugectr/bin/* && \
    rm -rf /hugectr

RUN git clone https://github.com/triton-inference-server/hugectr_backend /repos/hugectr_inference_backend && cd /repos/hugectr_inference_backend && \
    git checkout ${HUGECTR_BACKEND_VER} && \
    mkdir -p build && cd build && \
    cmake -DCMAKE_INSTALL_PREFIX:PATH=/usr/local/hugectr \
          -DTRITON_COMMON_REPO_TAG="r$TRITON_VERSION" \
          -DTRITON_CORE_REPO_TAG="r$TRITON_VERSION" \
          -DTRITON_BACKEND_REPO_TAG="r$TRITON_VERSION" .. && \
    make -j && make install && \
    rm -rf /repos/hugectr_inference_backend

RUN ln -s /usr/local/hugectr/backends/hugectr /opt/tritonserver/backends/

# Clean up
RUN rm -rf /repos

HEALTHCHECK NONE
CMD ["/bin/bash"]
