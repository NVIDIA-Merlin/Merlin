{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com//notebooks/dlsw-notebooks/merlin_merlin_scaling-criteo-03-training-with-hugectr/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Scaling Criteo: Training with HugeCTR\n",
    "\n",
    "This notebook is created using the latest stable [merlin-hugectr](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr/tags) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "HugeCTR is an open-source framework to accelerate the training of CTR estimation models on NVIDIA GPUs. It is written in CUDA C++ and highly exploits GPU-accelerated libraries such as cuBLAS, cuDNN, and NCCL.<br><br>\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written in C++. We experienced up to 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available.\n",
    "\n",
    "HugeCTR is able to train recommender system models with larger-than-memory embedding tables by leveraging a parameter server. \n",
    "\n",
    "You can find more information about HugeCTR from the [GitHub repository](https://github.com/NVIDIA-Merlin/HugeCTR).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to to use HugeCTR for training recommender system models\n",
    "\n",
    "- Use **HugeCTR** to define a recommender system model\n",
    "- Train Facebook's [Deep Learning Recommendation Model](https://arxiv.org/pdf/1906.00091.pdf) with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with HugeCTR's Python API and parameters, you can read more in its GitHub repository:\n",
    "- [HugeCTR User Guide](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_user_guide.html)\n",
    "- [HugeCTR Python API](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html)\n",
    "- [HugeCTR example architectures](https://github.com/NVIDIA/HugeCTR/tree/main/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", BASE_DIR + \"/test_dask/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we clean the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf \" + os.path.join(OUTPUT_DATA_DIR, \"criteo_hugectr/\"))\n",
    "os.system(\"mkdir -p \" + os.path.join(OUTPUT_DATA_DIR, \"criteo_hugectr/1/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the code to a `./model.py` file and execute it. The code creates snapshots, which we will use for inference in the next notebook. We use `graph_to_json` to convert the model to a JSON configuration, required for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = f\"\"\"\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "# HugeCTR\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"{os.path.join(OUTPUT_DATA_DIR, \"train/_file_list.txt\")}\"],\n",
    "    eval_source=\"{os.path.join(OUTPUT_DATA_DIR, \"valid/_file_list.txt\")}\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[\n",
    "        10000000,\n",
    "        10000000,\n",
    "        3014529,\n",
    "        400781,\n",
    "        11,\n",
    "        2209,\n",
    "        11869,\n",
    "        148,\n",
    "        4,\n",
    "        977,\n",
    "        15,\n",
    "        38713,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        584616,\n",
    "        12883,\n",
    "        109,\n",
    "        37,\n",
    "        17177,\n",
    "        7425,\n",
    "        20266,\n",
    "        4,\n",
    "        7085,\n",
    "        1535,\n",
    "        64,\n",
    "    ],\n",
    ")\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[hugectr.DataReaderSparseParam(\"data1\", 1, False, 26)],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=6000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"dense\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc1\"], top_names=[\"relu1\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc2\"], top_names=[\"relu2\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc3\"], top_names=[\"relu3\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"relu3\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"fc4\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc4\"], top_names=[\"relu4\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu4\"],\n",
    "        top_names=[\"fc5\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc5\"], top_names=[\"relu5\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu5\"],\n",
    "        top_names=[\"fc6\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc6\"], top_names=[\"relu6\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu6\"],\n",
    "        top_names=[\"fc7\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc7\"], top_names=[\"relu7\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu7\"],\n",
    "        top_names=[\"fc8\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc8\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "MAX_ITER = 10000\n",
    "EVAL_INTERVAL = 3200\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=MAX_ITER, eval_interval=EVAL_INTERVAL, display=1000, snapshot=3200)\n",
    "model.graph_to_json(graph_config_file=\"{os.path.join(OUTPUT_DATA_DIR, \"criteo_hugectr/1/\", \"criteo.json\")}\")\n",
    "\"\"\"\n",
    "with open('./model.py', 'w', encoding='utf-8') as fi:\n",
    "    fi.write(file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 4.1\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][13:56:03.374][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][13:56:03.374][INFO][RK0][main]: Global seed is 2831956451\n",
      "[HCTR][13:56:03.376][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][13:56:06.490][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][13:56:06.490][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][13:56:06.490][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][13:56:06.491][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][13:56:06.493][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB-LS\n",
      "[HCTR][13:56:06.493][INFO][RK0][main]: num of DataReader workers for train: 1\n",
      "[HCTR][13:56:06.493][INFO][RK0][main]: num of DataReader workers for eval: 1\n",
      "[HCTR][13:56:06.540][INFO][RK0][main]: Vocabulary size: 54120457\n",
      "[HCTR][13:56:06.541][INFO][RK0][main]: max_vocabulary_size_per_gpu_=12288000\n",
      "[HCTR][13:56:06.562][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][13:56:24.055][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][13:56:24.107][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][13:56:24.111][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][13:56:24.113][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "run_time: 125.9122965335846\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "os.system('python model.py')\n",
    "end = time.time() - start\n",
    "print(f\"run_time: {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the model and created snapshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-hugectr:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}