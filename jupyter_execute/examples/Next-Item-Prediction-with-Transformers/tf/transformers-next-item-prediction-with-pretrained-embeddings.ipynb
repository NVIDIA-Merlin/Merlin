{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b545747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#`\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d3b8",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw%20notebooks/merlin_models-transformers-next-item-prediction-with-pretrained-embeddings/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformer-based architecture for next-item prediction task with pretrained embeddings\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will train a Transformer-based architecture for next-item prediction task with pretrained embeddings.\n",
    "\n",
    "**You can chose to download the full dataset manually or use synthetic data.**\n",
    "\n",
    "We will use the [SIGIR eCOM 2021 Data Challenge Dataset](https://github.com/coveooss/SIGIR-ecom-data-challenge) to train a session-based model. The dataset contains 36M events of users browsing an online store.\n",
    "\n",
    "We will reshape the data to organize it into 'sessions'. Each session will be a full customer online journey in chronological order. The goal will be to predict the `url` of the next action taken.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training a Transformer-based architecture for next-item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b847f",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd7827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 22:58:36.667322: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "2023-06-20 22:58:38.026020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:38.026445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:38.026622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "from merlin.schema import ColumnSchema, Schema, Tags\n",
    "\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', '/workspace/data')\n",
    "NUM_EPOCHS = int(os.environ.get('NUM_EPOCHS', 5))\n",
    "NUM_EXAMPLES = int(os.environ.get('NUM_EXAMPLES', 100_000))\n",
    "MINIMUM_SESSION_LENGTH = int(os.environ.get('MINIMUM_SESSION_LENGTH', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf7c86",
   "metadata": {},
   "source": [
    "You can download the full dataset by registering [here](https://www.coveo.com/en/ailabs/sigir-ecom-data-challenge). If you chose to download the data, please place it alongside this notebook in the `sigir_dataset` directory and extract it.\n",
    "\n",
    "By default, in this notebook we will be using synthetically generated data based on the SIGIR dataset, but you can run on the full dataset by changing the value of the boolean flag below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3d1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ON_SYNTHETIC_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc6d6d",
   "metadata": {},
   "source": [
    "### Clean downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016a3e2",
   "metadata": {},
   "source": [
    "If you are training on the full SIGIR dataset, the following code will pre-process it.\n",
    "\n",
    "Here we deal with `nan` values, drop rows with missing information and parse strings containing lists to lists.\n",
    "\n",
    "The synthetically generated data is already clean -- it doesn't require this pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428ab049",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_ON_SYNTHETIC_DATA:\n",
    "    train = nvt.Dataset('/workspace/sigir_dataset/train/browsing_train.csv', part_size='500MB')\n",
    "    skus = nvt.Dataset('/workspace/sigir_dataset/train/sku_to_content.csv')\n",
    "\n",
    "    skus = pd.read_csv('/workspace/sigir_dataset/train/sku_to_content.csv')\n",
    "\n",
    "    skus['description_vector'] = skus['description_vector'].replace(np.nan, '')\n",
    "    skus['image_vector'] = skus['image_vector'].replace(np.nan, '')\n",
    "\n",
    "    skus['description_vector'] = skus['description_vector'].apply(lambda x: [] if len(x) == 0 else eval(x))\n",
    "    skus['image_vector'] = skus['image_vector'].apply(lambda x: [] if len(x) == 0 else eval(x))\n",
    "    skus = skus[skus.description_vector.apply(len) > 0]\n",
    "    skus = nvt.Dataset(skus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33fa32",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ba9b9",
   "metadata": {},
   "source": [
    "If you are not running on the full dataset, the following lines of code will generate its synthetic counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84789211",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SYNTHETIC_DATA:\n",
    "    from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "    train = generate_data('sigir-browsing', NUM_EXAMPLES)\n",
    "    skus = generate_data('sigir-sku', NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533f446",
   "metadata": {},
   "source": [
    "## Constructing a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47bc4e",
   "metadata": {},
   "source": [
    "We need to process our data further before we can use it to train our model.\n",
    "\n",
    "In particular, the `skus` dataset contains the mapping between the `product_sku_hash` (essentially an item id) to the `description_vector` -- an embedding obtained from the description.\n",
    "\n",
    "We would like to enable our model to make use of this piece of information. In order to feed this data to our model, we need to map the `product_sku_hash` to an id.\n",
    "\n",
    "But we need to make sure that the way we process `skus` and the `train` dataset (event information) is consistent, that the same `product_sku_hash` is mapped to the same id both when processing `skus` and `train`.\n",
    "\n",
    "We do so by defining and fitting a `Categorify` op once and using it to process both the `skus` and the `train` datasets.\n",
    "\n",
    "Additionally, we apply some further processing to the `train` dataset. We group rows by `session_id_hash` so that each training example will contain events from a single customer visit to the online store arranged in chronological order.\n",
    "\n",
    "If you would like to learn more about leveraging `NVTabular` to process tabular data on the GPU using a set of industry standard operators, please consult the examples available [here](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples).\n",
    "\n",
    "Let's first process the `train` dataset and retain the `Categorify` operator (`cat_op`) for processing of `skus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5feee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_op = nvt.ops.Categorify()\n",
    "out = ['product_sku_hash'] >> cat_op >> nvt.ops.TagAsItemID()\n",
    "out += ['event_type', 'product_action', 'session_id_hash', 'hashed_url'] >> nvt.ops.Categorify()\n",
    "out += ['server_timestamp_epoch_ms'] >> nvt.ops.NormalizeMinMax()\n",
    "\n",
    "groupby_features = out >> nvt.ops.Groupby(\n",
    "    groupby_cols=['session_id_hash'],\n",
    "    aggs={\n",
    "        'product_sku_hash': ['list'],\n",
    "        'event_type': ['list'],\n",
    "        'product_action': ['list'],\n",
    "        'hashed_url': ['list', 'count'],\n",
    "        'server_timestamp_epoch_ms': ['list']\n",
    "    },\n",
    "    sort_cols=\"server_timestamp_epoch_ms\"\n",
    ")\n",
    "\n",
    "filtered_sessions = groupby_features >> nvt.ops.Filter(f=lambda df: df[\"hashed_url_count\"] >= MINIMUM_SESSION_LENGTH)\n",
    "\n",
    "# We won't be needing the `session_id_hash` nor the `hashed_url_count` any longer\n",
    "wf = nvt.Workflow(\n",
    "    filtered_sessions[\n",
    "        'product_sku_hash_list',\n",
    "        'event_type_list',\n",
    "        'product_action_list',\n",
    "        'hashed_url_list',\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's save the output of our workflow -- transformed `train` for later use (training of our model).\n",
    "wf.fit_transform(train).to_parquet('train_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4828e",
   "metadata": {},
   "source": [
    "Here are a couple of example rows from `train_transformed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650fb0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_sku_hash_list</th>\n",
       "      <th>event_type_list</th>\n",
       "      <th>product_action_list</th>\n",
       "      <th>hashed_url_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[578, 972, 378, 420, 328, 126, 233, 925, 410, ...</td>\n",
       "      <td>[3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, ...</td>\n",
       "      <td>[3, 3, 5, 6, 4, 3, 3, 4, 4, 4, 6, 5, 3, 4, 3, ...</td>\n",
       "      <td>[766, 955, 745, 210, 940, 688, 986, 524, 425, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[298, 304, 393, 697, 706, 313, 834, 83, 502, 1...</td>\n",
       "      <td>[4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 4, 3, ...</td>\n",
       "      <td>[3, 5, 6, 4, 4, 3, 3, 3, 6, 6, 3, 3, 6, 6, 3, ...</td>\n",
       "      <td>[13, 221, 915, 658, 456, 378, 802, 180, 580, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[706, 221, 22, 702, 339, 645, 436, 358, 84, 35...</td>\n",
       "      <td>[4, 3, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, ...</td>\n",
       "      <td>[3, 6, 4, 6, 3, 3, 5, 5, 4, 6, 4, 6, 3, 5, 6, ...</td>\n",
       "      <td>[271, 940, 562, 498, 172, 239, 270, 215, 489, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[278, 153, 189, 717, 580, 540, 219, 79, 200, 9...</td>\n",
       "      <td>[3, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, ...</td>\n",
       "      <td>[6, 6, 6, 6, 3, 4, 4, 4, 4, 4, 3, 6, 5, 4, 3, ...</td>\n",
       "      <td>[169, 419, 875, 725, 926, 770, 160, 554, 763, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[156, 922, 914, 592, 842, 916, 137, 928, 615, ...</td>\n",
       "      <td>[3, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, ...</td>\n",
       "      <td>[6, 4, 5, 6, 5, 4, 3, 3, 6, 5, 6, 5, 3, 6, 3, ...</td>\n",
       "      <td>[318, 506, 281, 191, 506, 480, 965, 399, 761, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               product_sku_hash_list  \\\n",
       "0  [578, 972, 378, 420, 328, 126, 233, 925, 410, ...   \n",
       "1  [298, 304, 393, 697, 706, 313, 834, 83, 502, 1...   \n",
       "2  [706, 221, 22, 702, 339, 645, 436, 358, 84, 35...   \n",
       "3  [278, 153, 189, 717, 580, 540, 219, 79, 200, 9...   \n",
       "4  [156, 922, 914, 592, 842, 916, 137, 928, 615, ...   \n",
       "\n",
       "                                     event_type_list  \\\n",
       "0  [3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, ...   \n",
       "1  [4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 4, 3, ...   \n",
       "2  [4, 3, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, ...   \n",
       "3  [3, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, ...   \n",
       "4  [3, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, ...   \n",
       "\n",
       "                                 product_action_list  \\\n",
       "0  [3, 3, 5, 6, 4, 3, 3, 4, 4, 4, 6, 5, 3, 4, 3, ...   \n",
       "1  [3, 5, 6, 4, 4, 3, 3, 3, 6, 6, 3, 3, 6, 6, 3, ...   \n",
       "2  [3, 6, 4, 6, 3, 3, 5, 5, 4, 6, 4, 6, 3, 5, 6, ...   \n",
       "3  [6, 6, 6, 6, 3, 4, 4, 4, 4, 4, 3, 6, 5, 4, 3, ...   \n",
       "4  [6, 4, 5, 6, 5, 4, 3, 3, 6, 5, 6, 5, 3, 6, 3, ...   \n",
       "\n",
       "                                     hashed_url_list  \n",
       "0  [766, 955, 745, 210, 940, 688, 986, 524, 425, ...  \n",
       "1  [13, 221, 915, 658, 456, 378, 802, 180, 580, 4...  \n",
       "2  [271, 940, 562, 498, 172, 239, 270, 215, 489, ...  \n",
       "3  [169, 419, 875, 725, 926, 770, 160, 554, 763, ...  \n",
       "4  [318, 506, 281, 191, 506, 480, 965, 399, 761, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvt.Dataset('train_transformed', engine='parquet').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f12dbd",
   "metadata": {},
   "source": [
    "Now that we have processed the train set, we can use the mapping preserved in the `cat_op` to process the `skus` dataset containing the embeddings we are after.\n",
    "\n",
    "Let's now `Categorify` the `product_sku_hash` in `skus` and grab just the description embedding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313808d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_sku_hash</th>\n",
       "      <th>description_vector</th>\n",
       "      <th>category_hash</th>\n",
       "      <th>price_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>[0.07939800762120258, 0.3465797761609977, -0.3...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.186690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>[0.4275482879608162, -0.30569476366666, 0.1440...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.951997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>[-0.31035419787213536, 0.18070481533058008, 0....</td>\n",
       "      <td>22</td>\n",
       "      <td>0.973384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.31319783485940356, -0.11623980504981396, -...</td>\n",
       "      <td>138</td>\n",
       "      <td>0.146260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[0.25091279302969943, -0.33473442518442525, 0....</td>\n",
       "      <td>119</td>\n",
       "      <td>0.808252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_sku_hash                                 description_vector  \\\n",
       "0                13  [0.07939800762120258, 0.3465797761609977, -0.3...   \n",
       "1                25  [0.4275482879608162, -0.30569476366666, 0.1440...   \n",
       "2                18  [-0.31035419787213536, 0.18070481533058008, 0....   \n",
       "3                 1  [-0.31319783485940356, -0.11623980504981396, -...   \n",
       "4                11  [0.25091279302969943, -0.33473442518442525, 0....   \n",
       "\n",
       "   category_hash  price_bucket  \n",
       "0             16      0.186690  \n",
       "1             38      0.951997  \n",
       "2             22      0.973384  \n",
       "3            138      0.146260  \n",
       "4            119      0.808252  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfad1bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_sku_hash</th>\n",
       "      <th>description_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836</td>\n",
       "      <td>[0.07939800762120258, 0.3465797761609977, -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>979</td>\n",
       "      <td>[0.4275482879608162, -0.30569476366666, 0.1440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.31035419787213536, 0.18070481533058008, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469</td>\n",
       "      <td>[-0.31319783485940356, -0.11623980504981396, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>[0.25091279302969943, -0.33473442518442525, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_sku_hash                                 description_vector\n",
       "0               836  [0.07939800762120258, 0.3465797761609977, -0.3...\n",
       "1               979  [0.4275482879608162, -0.30569476366666, 0.1440...\n",
       "2                11  [-0.31035419787213536, 0.18070481533058008, 0....\n",
       "3               469  [-0.31319783485940356, -0.11623980504981396, -...\n",
       "4               118  [0.25091279302969943, -0.33473442518442525, 0...."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = ['product_sku_hash'] >> cat_op\n",
    "wf_skus = nvt.Workflow(out + 'description_vector')\n",
    "skus_ds = wf_skus.transform(skus)\n",
    "\n",
    "skus_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fe65d",
   "metadata": {},
   "source": [
    "Let us now export the embedding information to a `numpy` array and write it to disk.\n",
    "\n",
    "We will later pass this information to the `Loader` so that it will load the correct emebedding for the product corresponding to a given step of a customer journey.\n",
    "\n",
    "The embeddings are linked to the train set using the `product_sku_hash` information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99dfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skus_ds.to_npy('skus.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d80879",
   "metadata": {},
   "source": [
    "How will the `Loader` know which embedding to associate with a given row of the train set?\n",
    "\n",
    "The `product_sku_hash` ids have been exported along with the embeddings and are contained in the first column of the output `numpy` array.\n",
    "\n",
    "Here is the id of the first embedding stored in `skus.npy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60c6651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('skus.npy')[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cf669",
   "metadata": {},
   "source": [
    "and here is the embedding vector corresponding to `product_sku_hash` of id referenced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c111fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07939801,  0.34657978, -0.38269496,  0.56307004, -0.10142923,\n",
       "        0.03702352, -0.11606304,  0.10070879, -0.21879928,  0.06107687,\n",
       "       -0.20743195, -0.01330719,  0.60182867,  0.0920322 ,  0.2648726 ,\n",
       "        0.56061561,  0.48643498,  0.39045152, -0.40012162,  0.09153962,\n",
       "       -0.38351605,  0.57134731,  0.59986226, -0.40321368, -0.32984972,\n",
       "        0.37559494,  0.1554353 , -0.0413067 ,  0.33814398,  0.30678041,\n",
       "        0.24001132,  0.42737922,  0.41554601, -0.40451691,  0.50428902,\n",
       "       -0.2004803 , -0.38297056,  0.06580838,  0.48285745,  0.51406472,\n",
       "        0.02268894,  0.36343324,  0.32497967, -0.29736346, -0.00538915,\n",
       "        0.12329302, -0.04998194,  0.27843002,  0.20212714,  0.39019503])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('skus.npy')[0, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4a13",
   "metadata": {},
   "source": [
    "We are now ready to construct the `Loader` that will feed the data to our model.\n",
    "\n",
    "We begin by reading in the embeddings information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e1f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('skus.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1f18d",
   "metadata": {},
   "source": [
    "We are now ready to define the `Loader`.\n",
    "\n",
    "We are passing in an `EmbeddingOperator` that will ensure that correct `sku` information (correct `description_vector`) is associated with the correct step in the customer journey (with the lookup key being contained in the `product_sku_hash_list`)\n",
    "\n",
    "When specifying the dataset, we are creating a `Merlin Dataset` based on the `train_transformed` data we saved above.\n",
    "\n",
    "Depending on the hardware that you will be running this on and the size of the dataset that you will be using, should you run out of GPU memory, you can specify one of the several parameters that can ease the memory load (`npartitions`, `part_size`, or `part_mem_fraction`).\n",
    "\n",
    "The `BATCH_SIZE` of 16 should work on a broad set of hardware, but if you are training on a lot of data and your hardware permitting you might want to significantly increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7212fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 22:58:50.835162: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-20 22:58:50.836068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.836268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.836425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.836673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.836849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.837009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-20 22:58:50.837114: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-06-20 22:58:50.837130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1621] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "from merlin.dataloader.tensorflow import Loader\n",
    "from merlin.dataloader.ops.embeddings import EmbeddingOperator\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "embedding_operator = EmbeddingOperator(\n",
    "    embeddings[:, 1:].astype(np.float32),\n",
    "    id_lookup_table=embeddings[:, 0].astype(int),\n",
    "    lookup_key=\"product_sku_hash_list\",\n",
    "    embedding_name='product_embeddings'\n",
    ")\n",
    "\n",
    "loader = Loader(\n",
    "    dataset=nvt.Dataset('train_transformed', engine='parquet'),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    transforms=[\n",
    "        embedding_operator\n",
    "    ],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f037d5d",
   "metadata": {},
   "source": [
    "Using the `EmbeddingOperator` object we referenced our `product_embeddings` and insructed the model what to use as a key to look up the information.\n",
    "\n",
    "Below is an example batch of data that our model will consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7371e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mm.sample_batch(loader, batch_size=BATCH_SIZE, include_targets=False, prepare_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9a50d",
   "metadata": {},
   "source": [
    "`product_embeddings` are included in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cbf8ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['product_sku_hash_list', 'event_type_list', 'product_action_list', 'hashed_url_list', 'product_embeddings'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e61e71",
   "metadata": {},
   "source": [
    "## Creating and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461926e",
   "metadata": {},
   "source": [
    "We are now ready to construct our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6867c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf as mm\n",
    "\n",
    "input_block = mm.InputBlockV2(\n",
    "    loader.output_schema,\n",
    "    embeddings=mm.Embeddings(\n",
    "        loader.output_schema.select_by_tag(Tags.CATEGORICAL),\n",
    "        sequence_combiner=None,\n",
    "    ),\n",
    "    pretrained_embeddings=mm.PretrainedEmbeddings(\n",
    "        loader.output_schema.select_by_tag(Tags.EMBEDDING),\n",
    "        sequence_combiner=None,\n",
    "        normalizer=\"l2-norm\",\n",
    "        output_dims={\"product_embeddings\": 64},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb788f",
   "metadata": {},
   "source": [
    "We have now constructed an `input_block` that will take our batch and transform it in a fashion that will make it amenable for further processing by subsequent layers of our model.\n",
    "\n",
    "To test that everything has worked, we can pass our example `batch` through the `input_block`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8afa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_block(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a70fe",
   "metadata": {},
   "source": [
    "Let us now construct the remaining layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b21c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'hashed_url_list'\n",
    "\n",
    "# We do not need the `train_transformed` dataset here, but we do need\n",
    "# to access the schema.\n",
    "# It contains important information that will help our model construct itself.\n",
    "schema = nvt.Dataset('train_transformed', engine='parquet').schema\n",
    "\n",
    "dmodel=64\n",
    "mlp_block = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "            )\n",
    "transformer_block = mm.XLNetBlock(d_model=dmodel, n_head=4, n_layer=2)\n",
    "model = mm.Model(\n",
    "    input_block,\n",
    "    mlp_block,\n",
    "    transformer_block,\n",
    "    mm.CategoricalOutput(\n",
    "        schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b54d19",
   "metadata": {},
   "source": [
    "And let us train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbb03f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-06-20 22:58:58.950175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 22:59:11.285571: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/xl_net_block/sequential_block_7/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 42s 2s/step - loss: 6.9800 - recall_at_10: 0.0106 - mrr_at_10: 0.0033 - ndcg_at_10: 0.0050 - map_at_10: 0.0033 - precision_at_10: 0.0011 - regularization_loss: 0.0000e+00 - loss_batch: 6.9689\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 34s 2s/step - loss: 6.9591 - recall_at_10: 0.0106 - mrr_at_10: 0.0031 - ndcg_at_10: 0.0048 - map_at_10: 0.0031 - precision_at_10: 0.0011 - regularization_loss: 0.0000e+00 - loss_batch: 6.9363\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 39s 2s/step - loss: 6.9471 - recall_at_10: 0.0107 - mrr_at_10: 0.0028 - ndcg_at_10: 0.0046 - map_at_10: 0.0028 - precision_at_10: 0.0011 - regularization_loss: 0.0000e+00 - loss_batch: 6.9206\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 38s 2s/step - loss: 6.9398 - recall_at_10: 0.0103 - mrr_at_10: 0.0030 - ndcg_at_10: 0.0047 - map_at_10: 0.0030 - precision_at_10: 0.0010 - regularization_loss: 0.0000e+00 - loss_batch: 6.9015\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 38s 2s/step - loss: 6.9375 - recall_at_10: 0.0104 - mrr_at_10: 0.0030 - ndcg_at_10: 0.0047 - map_at_10: 0.0030 - precision_at_10: 0.0010 - regularization_loss: 0.0000e+00 - loss_batch: 6.9095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55081d17c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(run_eagerly=False, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, pre=mm.SequenceMaskRandom(schema=loader.output_schema, target=target, masking_prob=0.3, transformer=transformer_block))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ab17b",
   "metadata": {},
   "source": [
    "## Serving predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778420d",
   "metadata": {},
   "source": [
    "Now that we have prepared a workflow for processing our data (`wf`), defined the embedding operator (`embedding_operator`) and trained our model (`model`), we have all the components we need to serve our model using the Triton Inference Server (TIS).\n",
    "\n",
    "Let us define a set of inference operators (a pipeline for processing our data all the way to obtaining predictions) and export them as an ensemble that we will be able to serve using TIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18f19033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385aba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, sequence_mask_random_layer_call_fn, sequence_mask_random_layer_call_and_return_conditional_losses, prepare_list_features_1_layer_call_fn while saving (showing 5 of 110). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi3g8g7q7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi3g8g7q7/assets\n"
     ]
    }
   ],
   "source": [
    "inference_operators = wf.input_schema.column_names >> TransformWorkflow(wf) >> embedding_operator >> PredictTensorflow(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c14a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer TFSharedEmbeddings(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer Dropout(\n",
      "  (_feature_shapes): Dict(\n",
      "    (product_sku_hash_list): TensorShape([16, None, 1])\n",
      "    (event_type_list): TensorShape([16, None, 1])\n",
      "    (product_action_list): TensorShape([16, None, 1])\n",
      "    (hashed_url_list): TensorShape([16, None, 1])\n",
      "    (product_embeddings): TensorShape([16, None, 50])\n",
      "  )\n",
      "  (_feature_dtypes): Dict(\n",
      "    (product_sku_hash_list): tf.int64\n",
      "    (event_type_list): tf.int64\n",
      "    (product_action_list): tf.int64\n",
      "    (hashed_url_list): tf.int64\n",
      "    (product_embeddings): tf.float32\n",
      "  )\n",
      "), because it is not built.\n",
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, sequence_mask_random_layer_call_fn, sequence_mask_random_layer_call_and_return_conditional_losses, prepare_list_features_1_layer_call_fn while saving (showing 5 of 110). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/data/ensemble/1_predicttensorflowtriton/1/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/data/ensemble/1_predicttensorflowtriton/1/model.savedmodel/assets\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:101: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[key] = tf.keras.utils.serialize_keras_object(maybe_value)\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:288: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config[i] = tf.keras.utils.serialize_keras_object(layer)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/saving/legacy/saved_model/layer_serialization.py:134: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return serialization.serialize_keras_object(obj)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "ensemble = Ensemble(inference_operators, wf.input_schema)\n",
    "ensemble.export(os.path.join(OUTPUT_DATA_DIR, 'ensemble'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fd1ea",
   "metadata": {},
   "source": [
    "After we export the ensemble, we are ready to start the Triton Inference Server.\n",
    "\n",
    "The server is installed in Merlin Tensorflow and Merlin PyTorch containers. If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server [documentation](https://github.com/triton-inference-server/server/blob/r22.03/README.md#documentation).\n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "```tritonserver --model-repository={OUTPUT_DATA_DIR}/ensemble/```\n",
    "\n",
    "For the --model-repository argument, specify the same value as the `export_path` that you specified previously in the `ensemble.export` method.\n",
    "\n",
    "After you run the `tritonserver` command, wait until your terminal shows messages like the following example:\n",
    "\n",
    "I0414 18:29:50.741833 4067 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001<br>\n",
    "I0414 18:29:50.742197 4067 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000<br>\n",
    "I0414 18:29:50.783470 4067 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
    "\n",
    "Let us now package our data for inference. We will send 5 rows of data, which corresponds to a single customer journey (session) through the online store. The data will be first processed by the `NVTabular` workflow and subsequentally passed to our transformer model for predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90483210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining five rows of data\n",
    "df = train.head(5)\n",
    "# making sure all the rows correspond to the same online session (have the same `session_id_hash`)\n",
    "df['session_id_hash'] = df['session_id_hash'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf671e",
   "metadata": {},
   "source": [
    "Let us now send the data to the Triton Inference Server for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8453048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inputs = convert_df_to_triton_input(wf.input_schema, df)\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer('executor_model', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b80e8",
   "metadata": {},
   "source": [
    "Let's parse the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc4b046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2332087 , -2.1218574 , -2.390479  , ..., -0.7735352 ,\n",
       "         0.1954267 , -0.34523243]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = response.as_numpy(\"hashed_url_list/categorical_output\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c2ed9",
   "metadata": {},
   "source": [
    "The response contains logits predicting the id of the url the customer is most likely to arrive at as next step of their journey through the online store.\n",
    "\n",
    "Here is the predicted hashed url id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b9af2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_hashed_url_id = predictions.argmax()\n",
    "predicted_hashed_url_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef47efd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have trained a transformer model for the next item prediction task using language model masking.\n",
    "\n",
    "For another session-based example that goes deeper into data preprocessing and that covers several advanced techniques (Weight Tying, Temperature Scaling) please see [Session-Based Next Item Prediction for Fashion E-Commerce](https://github.com/NVIDIA-Merlin/models/blob/t4rec_use_case/examples/usecases/ecommerce-session-based-next-item-prediction-for-fashion.ipynb). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}